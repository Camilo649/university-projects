<!DOCTYPE html>
<html class="writer-html5" lang="en"><head class="at-element-marker">
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
<meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/">
<meta content="The programming guide to using the CUDA Toolkit to obtain the best performance from NVIDIA GPUs." name="description">
<meta content="CUDA Best Practices, CUDA APOD, CUDA optimization" name="keywords">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>1. Preface — CUDA C++ Best Practices Guide 12.9 documentation</title>
      <link rel="stylesheet" href="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/pygments.css" type="text/css">
      <link rel="stylesheet" href="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/theme.css" type="text/css">
      <link rel="stylesheet" href="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/copybutton.css" type="text/css">
      <link rel="stylesheet" href="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" type="text/css">
      <link rel="stylesheet" href="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/omni-style.css" type="text/css">
      <link rel="stylesheet" href="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/api-styles.css" type="text/css">
    <link rel="shortcut icon" href="https://docs.nvidia.com/cuda/_static/favicon.ico">
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/documentation_options.js"></script>
        <script src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/jquery.js"></script>
        <script src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/underscore.js"></script>
        <script src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/doctools.js"></script>
        <script src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/mermaid-init.js"></script>
        <script src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/clipboard.min.js"></script>
        <script src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/copybutton.js"></script>
        <script src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/design-tabs.js"></script>
        <script async="async" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/tex-mml-chtml.js"></script>
        <script src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/geoip.js"></script>
    <script src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/theme.js"></script>
    <link rel="index" title="Index" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/genindex.html">
    <link rel="search" title="Search" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/search.html">
    <link rel="prev" title="Contents" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/contents.html">
 
<script src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/launch-191c2462b890.min.js"></script><script src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/EX35fdf437c1874827ac1a27ab07aa4f8b-libraryCode_source.min.js" async=""></script><script>_satellite["_runScript1"](function(event, target, Promise) {
function generateToken(){
	//return state after it has any double quotations removed
	var d = new Date().getTime();//Timestamp
	var d2 = (performance && performance.now && (performance.now()*1000)) || 0;//Time in microseconds since page-load or 0 if unsupported
	return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function(c) {
		var r = Math.random() * 16;//random number between 0 and 16
		if(d > 0){//Use timestamp until depleted
			r = (d + r)%16 | 0;
			d = Math.floor(d/16);
		} else {//Use microseconds since page-load if supported
			r = (d2 + r)%16 | 0;
			d2 = Math.floor(d2/16);
		}
		return (c === 'x' ? r : (r & 0x3 | 0x8)).toString(16);
	});
}




/*assign user a Anonymous token on page load if not allready available*/

//jQuery(document).ready(function() {
document.addEventListener("DOMContentLoaded", function() {
  try{
	/*Setting Anonymous Cookie*/
	const cookieAToken = "nvweb_A";
	const cookieEToken = "nvweb_E";
	/*assign anonymous token*/
	if (typeof Cookies!== "undefined" && typeof Cookies.get("nvweb_A")=== "undefined") {
		const cookieValue = generateToken();
		const daysToExpire = new Date(3147483647 * 1000).toUTCString();
		document.cookie = cookieAToken + '=' + cookieValue + ';domain=.nvidia.com;path=/' + '; expires=' + daysToExpire; 
	}
	
	/*assign passed param  token*/
	var paramresults = new RegExp('[\?&]' + 'cookieNameEmail' + '=([^&#]*)').exec(window.location.href);
	if (paramresults!=null) {
		const cookieValue = NVIDIAGDC.Browser.getUrlParameter("nvweb_E");
		const daysToExpire = new Date(3147483647 * 1000).toUTCString();
		document.cookie = cookieEToken + '=' + paramresults[1] + ';domain=.nvidia.com;path=/' + '; expires=' + daysToExpire; 
    
  }
}catch(e){console.log("webtoken: "+e)}

});
});</script><script>_satellite["_runScript2"](function(event, target, Promise) {
try{
  var _6scProvider = { 
name: "6sc-api", 
version: "1.0.0", 
timeout: 2000, 
provider: function(callback) { 
  
  
  
var a = localStorage.getItem("_6senseCompanyDetails"),
n = JSON.parse(a); 
  //console.log('company detail populated');

var country = n.company.country;
if(country == undefined && "" == country){
    country="Not Identified"
}

var industry = n.company.industry;
if(industry == undefined && "" == industry){
    industry="Not Identified"
}

var company = n.company.name;
if(company == undefined && "" == company){
    company="Not Identified"
}
  
//var pf={};
var dl_bs='';
var dl_ps='';
 
var ps=n.scores;  
for (var key in ps){
  if(ps[key].product=="deep_learning"){
    //pf["dl_buying_stage"]=ps[key].buying_stage;
    //pf["dl_profile_score"]=ps[key].profile_score;
    dl_bs=ps[key].buying_stage;
    dl_ps=ps[key].profile_score;
  }
} 

  
var sg=n.segments;
  segs=sg.names.toString()
  
  
  
    callback(null, {company: company, industry:industry, segments:segs, dl_buying_stage:dl_bs,dl_profile_score:dl_ps}); 
  }
};  
var cpage=window.location.host+window.location.pathname;
if(cpage.endsWith('developer-qa.nvidia.com/')==true){
var _nvcdpProvider = {
   name: "nvcdp",
   version: "1.0.0",
   timeout: 2000, 
   provider: function(callback) {
    var tokens=[];
	var responsetext = "";
	var responseJson;
    var webToken='';
     if(_satellite.cookie.get('nvweb_A') != undefined)
    webToken=_satellite.cookie.get('nvweb_A'); else webToken='';
    var emailToken='';
     if(_satellite.cookie.get('nvweb_E') != undefined)
    emailToken = _satellite.cookie.get('nvweb_E');  else emailToken='';
    var sfIdToken='';
     if(_satellite.cookie.get('nvweb_S') != undefined)
    sfIdToken =_satellite.cookie.get('nvweb_S');  else sfIdToken='';
	var requestURL = 'https://api-stage.nvidia.com/services/personalize/v1/personlize/token?sfIdToken='+sfIdToken+'&webToken='+webToken+'&emailToken='+emailToken;
    var XHR = new XMLHttpRequest();
    
    XHR.onreadystatechange = function() {
        if (XHR.readyState == 4 && XHR.status == 200) {
			responsetext=XHR.responseText;
		    responseJson = JSON.parse(responsetext);
          setTimeout(function() {
            tokens.push(responseJson);
     }, 200);

		}
    }
    XHR.open("GET", requestURL, true);
    XHR.timeout = 2000;
    XHR.setRequestHeader("content-type","application/json");
    XHR.setRequestHeader("nvidia-partner", true);
    XHR.send(null);
     
     setTimeout(function() {
            if(tokens.length>0){
               callback(null, {webtoken:tokens[0].webToken, emailtoken: tokens[0].emailToken,sfidtoken: tokens[0].sfIdToken});
             }
     }, 1500);
     
      }
    };
}
if(cpage.endsWith('developer-qa.nvidia.com/')==true){
window.targetGlobalSettings = { 
dataProviders: [_6scProvider,_nvcdpProvider]
}
}else{
  window.targetGlobalSettings = { 
dataProviders: [_6scProvider]
}
}
}catch(e){}
});</script><script>_satellite["_runScript3"](function(event, target, Promise) {
document.addEventListener(adobe.target.event.REQUEST_SUCCEEDED, function(e) {
    if (e.detail.responseTokens) {
        var responseToken = e.detail.responseTokens;
        var y = responseToken[0];
        var profileCatAffinity = y['profile.categoryAffinity'];
        _satellite.setVar('favCategory', profileCatAffinity);
    }
});
});</script>
 

<style type="text/css">.CtxtMenu_InfoClose {  top:.2em; right:.2em;}
.CtxtMenu_InfoContent {  overflow:auto; text-align:left; font-size:80%;  padding:.4em .6em; border:1px inset; margin:1em 0px;  max-height:20em; max-width:30em; background-color:#EEEEEE;  white-space:normal;}
.CtxtMenu_Info.CtxtMenu_MousePost {outline:none;}
.CtxtMenu_Info {  position:fixed; left:50%; width:auto; text-align:center;  border:3px outset; padding:1em 2em; background-color:#DDDDDD;  color:black;  cursor:default; font-family:message-box; font-size:120%;  font-style:normal; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 15px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius:15px;               /* Safari and Chrome */  -moz-border-radius:15px;                  /* Firefox */  -khtml-border-radius:15px;                /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */  filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color="gray", Positive="true"); /* IE */}
</style><style type="text/css">.CtxtMenu_MenuClose {  position:absolute;  cursor:pointer;  display:inline-block;  border:2px solid #AAA;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  font-family: "Courier New", Courier;  font-size:24px;  color:#F0F0F0}
.CtxtMenu_MenuClose span {  display:block; background-color:#AAA; border:1.5px solid;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  line-height:0;  padding:8px 0 6px     /* may need to be browser-specific */}
.CtxtMenu_MenuClose:hover {  color:white!important;  border:2px solid #CCC!important}
.CtxtMenu_MenuClose:hover span {  background-color:#CCC!important}
.CtxtMenu_MenuClose:hover:focus {  outline:none}
</style><style type="text/css">.CtxtMenu_Menu {  position:absolute;  background-color:white;  color:black;  width:auto; padding:5px 0px;  border:1px solid #CCCCCC; margin:0; cursor:default;  font: menu; text-align:left; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 5px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius: 5px;             /* Safari and Chrome */  -moz-border-radius: 5px;                /* Firefox */  -khtml-border-radius: 5px;              /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */}
.CtxtMenu_MenuItem {  padding: 1px 2em;  background:transparent;}
.CtxtMenu_MenuArrow {  position:absolute; right:.5em; padding-top:.25em; color:#666666;  font-family: null; font-size: .75em}
.CtxtMenu_MenuActive .CtxtMenu_MenuArrow {color:white}
.CtxtMenu_MenuArrow.CtxtMenu_RTL {left:.5em; right:auto}
.CtxtMenu_MenuCheck {  position:absolute; left:.7em;  font-family: null}
.CtxtMenu_MenuCheck.CtxtMenu_RTL { right:.7em; left:auto }
.CtxtMenu_MenuRadioCheck {  position:absolute; left: .7em;}
.CtxtMenu_MenuRadioCheck.CtxtMenu_RTL {  right: .7em; left:auto}
.CtxtMenu_MenuInputBox {  padding-left: 1em; right:.5em; color:#666666;  font-family: null;}
.CtxtMenu_MenuInputBox.CtxtMenu_RTL {  left: .1em;}
.CtxtMenu_MenuComboBox {  left:.1em; padding-bottom:.5em;}
.CtxtMenu_MenuSlider {  left: .1em;}
.CtxtMenu_SliderValue {  position:absolute; right:.1em; padding-top:.25em; color:#333333;  font-size: .75em}
.CtxtMenu_SliderBar {  outline: none; background: #d3d3d3}
.CtxtMenu_MenuLabel {  padding: 1px 2em 3px 1.33em;  font-style:italic}
.CtxtMenu_MenuRule {  border-top: 1px solid #DDDDDD;  margin: 4px 3px;}
.CtxtMenu_MenuDisabled {  color:GrayText}
.CtxtMenu_MenuActive {  background-color: #606872;  color: white;}
.CtxtMenu_MenuDisabled:focus {  background-color: #E8E8E8}
.CtxtMenu_MenuLabel:focus {  background-color: #E8E8E8}
.CtxtMenu_ContextMenu:focus {  outline:none}
.CtxtMenu_ContextMenu .CtxtMenu_MenuItem:focus {  outline:none}
.CtxtMenu_SelectionMenu {  position:relative; float:left;  border-bottom: none; -webkit-box-shadow:none; -webkit-border-radius:0px; }
.CtxtMenu_SelectionItem {  padding-right: 1em;}
.CtxtMenu_Selection {  right: 40%; width:50%; }
.CtxtMenu_SelectionBox {  padding: 0em; max-height:20em; max-width: none;  background-color:#FFFFFF;}
.CtxtMenu_SelectionDivider {  clear: both; border-top: 2px solid #000000;}
.CtxtMenu_Menu .CtxtMenu_MenuClose {  top:-10px; left:-10px}
</style><style id="MJX-CHTML-styles">
mjx-container[jax="CHTML"] {
  line-height: 0;
}

mjx-container [space="1"] {
  margin-left: .111em;
}

mjx-container [space="2"] {
  margin-left: .167em;
}

mjx-container [space="3"] {
  margin-left: .222em;
}

mjx-container [space="4"] {
  margin-left: .278em;
}

mjx-container [space="5"] {
  margin-left: .333em;
}

mjx-container [rspace="1"] {
  margin-right: .111em;
}

mjx-container [rspace="2"] {
  margin-right: .167em;
}

mjx-container [rspace="3"] {
  margin-right: .222em;
}

mjx-container [rspace="4"] {
  margin-right: .278em;
}

mjx-container [rspace="5"] {
  margin-right: .333em;
}

mjx-container [size="s"] {
  font-size: 70.7%;
}

mjx-container [size="ss"] {
  font-size: 50%;
}

mjx-container [size="Tn"] {
  font-size: 60%;
}

mjx-container [size="sm"] {
  font-size: 85%;
}

mjx-container [size="lg"] {
  font-size: 120%;
}

mjx-container [size="Lg"] {
  font-size: 144%;
}

mjx-container [size="LG"] {
  font-size: 173%;
}

mjx-container [size="hg"] {
  font-size: 207%;
}

mjx-container [size="HG"] {
  font-size: 249%;
}

mjx-container [width="full"] {
  width: 100%;
}

mjx-box {
  display: inline-block;
}

mjx-block {
  display: block;
}

mjx-itable {
  display: inline-table;
}

mjx-row {
  display: table-row;
}

mjx-row > * {
  display: table-cell;
}

mjx-mtext {
  display: inline-block;
  text-align: left;
}

mjx-mstyle {
  display: inline-block;
}

mjx-merror {
  display: inline-block;
  color: red;
  background-color: yellow;
}

mjx-mphantom {
  visibility: hidden;
}

_::-webkit-full-page-media, _:future, :root mjx-container {
  will-change: opacity;
}

mjx-assistive-mml {
  position: absolute !important;
  top: 0px;
  left: 0px;
  clip: rect(1px, 1px, 1px, 1px);
  padding: 1px 0px 0px 0px !important;
  border: 0px !important;
  display: block !important;
  width: auto !important;
  overflow: hidden !important;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

mjx-assistive-mml[display="block"] {
  width: 100% !important;
}

mjx-math {
  display: inline-block;
  text-align: left;
  line-height: 0;
  text-indent: 0;
  font-style: normal;
  font-weight: normal;
  font-size: 100%;
  font-size-adjust: none;
  letter-spacing: normal;
  border-collapse: collapse;
  word-wrap: normal;
  word-spacing: normal;
  white-space: nowrap;
  direction: ltr;
  padding: 1px 0;
}

mjx-container[jax="CHTML"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="CHTML"][display="true"][width="full"] {
  display: flex;
}

mjx-container[jax="CHTML"][display="true"] mjx-math {
  padding: 0;
}

mjx-container[jax="CHTML"][justify="left"] {
  text-align: left;
}

mjx-container[jax="CHTML"][justify="right"] {
  text-align: right;
}

mjx-mi {
  display: inline-block;
  text-align: left;
}

mjx-c {
  display: inline-block;
}

mjx-utext {
  display: inline-block;
  padding: .75em 0 .2em 0;
}

mjx-mo {
  display: inline-block;
  text-align: left;
}

mjx-stretchy-h {
  display: inline-table;
  width: 100%;
}

mjx-stretchy-h > * {
  display: table-cell;
  width: 0;
}

mjx-stretchy-h > * > mjx-c {
  display: inline-block;
  transform: scalex(1.0000001);
}

mjx-stretchy-h > * > mjx-c::before {
  display: inline-block;
  width: initial;
}

mjx-stretchy-h > mjx-ext {
  /* IE */ overflow: hidden;
  /* others */ overflow: clip visible;
  width: 100%;
}

mjx-stretchy-h > mjx-ext > mjx-c::before {
  transform: scalex(500);
}

mjx-stretchy-h > mjx-ext > mjx-c {
  width: 0;
}

mjx-stretchy-h > mjx-beg > mjx-c {
  margin-right: -.1em;
}

mjx-stretchy-h > mjx-end > mjx-c {
  margin-left: -.1em;
}

mjx-stretchy-v {
  display: inline-block;
}

mjx-stretchy-v > * {
  display: block;
}

mjx-stretchy-v > mjx-beg {
  height: 0;
}

mjx-stretchy-v > mjx-end > mjx-c {
  display: block;
}

mjx-stretchy-v > * > mjx-c {
  transform: scaley(1.0000001);
  transform-origin: left center;
  overflow: hidden;
}

mjx-stretchy-v > mjx-ext {
  display: block;
  height: 100%;
  box-sizing: border-box;
  border: 0px solid transparent;
  /* IE */ overflow: hidden;
  /* others */ overflow: visible clip;
}

mjx-stretchy-v > mjx-ext > mjx-c::before {
  width: initial;
  box-sizing: border-box;
}

mjx-stretchy-v > mjx-ext > mjx-c {
  transform: scaleY(500) translateY(.075em);
  overflow: visible;
}

mjx-mark {
  display: inline-block;
  height: 0px;
}

mjx-mfrac {
  display: inline-block;
  text-align: left;
}

mjx-frac {
  display: inline-block;
  vertical-align: 0.17em;
  padding: 0 .22em;
}

mjx-frac[type="d"] {
  vertical-align: .04em;
}

mjx-frac[delims] {
  padding: 0 .1em;
}

mjx-frac[atop] {
  padding: 0 .12em;
}

mjx-frac[atop][delims] {
  padding: 0;
}

mjx-dtable {
  display: inline-table;
  width: 100%;
}

mjx-dtable > * {
  font-size: 2000%;
}

mjx-dbox {
  display: block;
  font-size: 5%;
}

mjx-num {
  display: block;
  text-align: center;
}

mjx-den {
  display: block;
  text-align: center;
}

mjx-mfrac[bevelled] > mjx-num {
  display: inline-block;
}

mjx-mfrac[bevelled] > mjx-den {
  display: inline-block;
}

mjx-den[align="right"], mjx-num[align="right"] {
  text-align: right;
}

mjx-den[align="left"], mjx-num[align="left"] {
  text-align: left;
}

mjx-nstrut {
  display: inline-block;
  height: .054em;
  width: 0;
  vertical-align: -.054em;
}

mjx-nstrut[type="d"] {
  height: .217em;
  vertical-align: -.217em;
}

mjx-dstrut {
  display: inline-block;
  height: .505em;
  width: 0;
}

mjx-dstrut[type="d"] {
  height: .726em;
}

mjx-line {
  display: block;
  box-sizing: border-box;
  min-height: 1px;
  height: .06em;
  border-top: .06em solid;
  margin: .06em -.1em;
  overflow: hidden;
}

mjx-line[type="d"] {
  margin: .18em -.1em;
}

mjx-mn {
  display: inline-block;
  text-align: left;
}

mjx-mrow {
  display: inline-block;
  text-align: left;
}

mjx-TeXAtom {
  display: inline-block;
  text-align: left;
}

mjx-msup {
  display: inline-block;
  text-align: left;
}

mjx-msub {
  display: inline-block;
  text-align: left;
}

mjx-c::before {
  display: block;
  width: 0;
}

.MJX-TEX {
  font-family: MJXZERO, MJXTEX;
}

.TEX-B {
  font-family: MJXZERO, MJXTEX-B;
}

.TEX-I {
  font-family: MJXZERO, MJXTEX-I;
}

.TEX-MI {
  font-family: MJXZERO, MJXTEX-MI;
}

.TEX-BI {
  font-family: MJXZERO, MJXTEX-BI;
}

.TEX-S1 {
  font-family: MJXZERO, MJXTEX-S1;
}

.TEX-S2 {
  font-family: MJXZERO, MJXTEX-S2;
}

.TEX-S3 {
  font-family: MJXZERO, MJXTEX-S3;
}

.TEX-S4 {
  font-family: MJXZERO, MJXTEX-S4;
}

.TEX-A {
  font-family: MJXZERO, MJXTEX-A;
}

.TEX-C {
  font-family: MJXZERO, MJXTEX-C;
}

.TEX-CB {
  font-family: MJXZERO, MJXTEX-CB;
}

.TEX-FR {
  font-family: MJXZERO, MJXTEX-FR;
}

.TEX-FRB {
  font-family: MJXZERO, MJXTEX-FRB;
}

.TEX-SS {
  font-family: MJXZERO, MJXTEX-SS;
}

.TEX-SSB {
  font-family: MJXZERO, MJXTEX-SSB;
}

.TEX-SSI {
  font-family: MJXZERO, MJXTEX-SSI;
}

.TEX-SC {
  font-family: MJXZERO, MJXTEX-SC;
}

.TEX-T {
  font-family: MJXZERO, MJXTEX-T;
}

.TEX-V {
  font-family: MJXZERO, MJXTEX-V;
}

.TEX-VB {
  font-family: MJXZERO, MJXTEX-VB;
}

mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c {
  font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A ! important;
}

@font-face /* 0 */ {
  font-family: MJXZERO;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Zero.woff") format("woff");
}

@font-face /* 1 */ {
  font-family: MJXTEX;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff") format("woff");
}

@font-face /* 2 */ {
  font-family: MJXTEX-B;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Bold.woff") format("woff");
}

@font-face /* 3 */ {
  font-family: MJXTEX-I;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff") format("woff");
}

@font-face /* 4 */ {
  font-family: MJXTEX-MI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Italic.woff") format("woff");
}

@font-face /* 5 */ {
  font-family: MJXTEX-BI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-BoldItalic.woff") format("woff");
}

@font-face /* 6 */ {
  font-family: MJXTEX-S1;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff") format("woff");
}

@font-face /* 7 */ {
  font-family: MJXTEX-S2;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size2-Regular.woff") format("woff");
}

@font-face /* 8 */ {
  font-family: MJXTEX-S3;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size3-Regular.woff") format("woff");
}

@font-face /* 9 */ {
  font-family: MJXTEX-S4;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size4-Regular.woff") format("woff");
}

@font-face /* 10 */ {
  font-family: MJXTEX-A;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_AMS-Regular.woff") format("woff");
}

@font-face /* 11 */ {
  font-family: MJXTEX-C;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Regular.woff") format("woff");
}

@font-face /* 12 */ {
  font-family: MJXTEX-CB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Bold.woff") format("woff");
}

@font-face /* 13 */ {
  font-family: MJXTEX-FR;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Regular.woff") format("woff");
}

@font-face /* 14 */ {
  font-family: MJXTEX-FRB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Bold.woff") format("woff");
}

@font-face /* 15 */ {
  font-family: MJXTEX-SS;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Regular.woff") format("woff");
}

@font-face /* 16 */ {
  font-family: MJXTEX-SSB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Bold.woff") format("woff");
}

@font-face /* 17 */ {
  font-family: MJXTEX-SSI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Italic.woff") format("woff");
}

@font-face /* 18 */ {
  font-family: MJXTEX-SC;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Script-Regular.woff") format("woff");
}

@font-face /* 19 */ {
  font-family: MJXTEX-T;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Typewriter-Regular.woff") format("woff");
}

@font-face /* 20 */ {
  font-family: MJXTEX-V;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Regular.woff") format("woff");
}

@font-face /* 21 */ {
  font-family: MJXTEX-VB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Bold.woff") format("woff");
}

mjx-c.mjx-c1D446.TEX-I::before {
  padding: 0.705em 0.645em 0.022em 0;
  content: "S";
}

mjx-c.mjx-c3D::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "=";
}

mjx-c.mjx-c31::before {
  padding: 0.666em 0.5em 0 0;
  content: "1";
}

mjx-c.mjx-c28::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: "(";
}

mjx-c.mjx-c2212::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "\2212";
}

mjx-c.mjx-c1D443.TEX-I::before {
  padding: 0.683em 0.751em 0 0;
  content: "P";
}

mjx-c.mjx-c29::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: ")";
}

mjx-c.mjx-c2B::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "+";
}

mjx-c.mjx-c1D441.TEX-I::before {
  padding: 0.683em 0.888em 0 0;
  content: "N";
}

mjx-c.mjx-c2F::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "/";
}

mjx-c.mjx-c28.TEX-S1::before {
  padding: 0.85em 0.458em 0.349em 0;
  content: "(";
}

mjx-c.mjx-c30::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "0";
}

mjx-c.mjx-c2E::before {
  padding: 0.12em 0.278em 0 0;
  content: ".";
}

mjx-c.mjx-c38::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "8";
}

mjx-c.mjx-c37::before {
  padding: 0.676em 0.5em 0.022em 0;
  content: "7";
}

mjx-c.mjx-cD7::before {
  padding: 0.491em 0.778em 0 0;
  content: "\D7";
}

mjx-c.mjx-c39::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "9";
}

mjx-c.mjx-c34::before {
  padding: 0.677em 0.5em 0 0;
  content: "4";
}

mjx-c.mjx-c36::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "6";
}

mjx-c.mjx-c32::before {
  padding: 0.666em 0.5em 0 0;
  content: "2";
}

mjx-c.mjx-c29.TEX-S1::before {
  padding: 0.85em 0.458em 0.349em 0;
  content: ")";
}

mjx-c.mjx-cF7::before {
  padding: 0.537em 0.778em 0.036em 0;
  content: "\F7";
}

mjx-c.mjx-c47::before {
  padding: 0.705em 0.785em 0.022em 0;
  content: "G";
}

mjx-c.mjx-c42::before {
  padding: 0.683em 0.708em 0 0;
  content: "B";
}

mjx-c.mjx-c73::before {
  padding: 0.448em 0.394em 0.011em 0;
  content: "s";
}

mjx-c.mjx-c45::before {
  padding: 0.68em 0.681em 0 0;
  content: "E";
}

mjx-c.mjx-c66::before {
  padding: 0.705em 0.372em 0 0;
  content: "f";
}

mjx-c.mjx-c65::before {
  padding: 0.448em 0.444em 0.011em 0;
  content: "e";
}

mjx-c.mjx-c63::before {
  padding: 0.448em 0.444em 0.011em 0;
  content: "c";
}

mjx-c.mjx-c74::before {
  padding: 0.615em 0.389em 0.01em 0;
  content: "t";
}

mjx-c.mjx-c69::before {
  padding: 0.669em 0.278em 0 0;
  content: "i";
}

mjx-c.mjx-c76::before {
  padding: 0.431em 0.528em 0.011em 0;
  content: "v";
}

mjx-c.mjx-c5C::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "\5C";
}

mjx-c.mjx-c20::before {
  padding: 0 0.25em 0 0;
  content: " ";
}

mjx-c.mjx-c62::before {
  padding: 0.694em 0.556em 0.011em 0;
  content: "b";
}

mjx-c.mjx-c61::before {
  padding: 0.448em 0.5em 0.011em 0;
  content: "a";
}

mjx-c.mjx-c6E::before {
  padding: 0.442em 0.556em 0 0;
  content: "n";
}

mjx-c.mjx-c64::before {
  padding: 0.694em 0.556em 0.011em 0;
  content: "d";
}

mjx-c.mjx-c77::before {
  padding: 0.431em 0.722em 0.011em 0;
  content: "w";
}

mjx-c.mjx-c68::before {
  padding: 0.694em 0.556em 0 0;
  content: "h";
}

mjx-c.mjx-c1D435.TEX-I::before {
  padding: 0.683em 0.759em 0 0;
  content: "B";
}

mjx-c.mjx-c1D45F.TEX-I::before {
  padding: 0.442em 0.451em 0.011em 0;
  content: "r";
}

mjx-c.mjx-c1D464.TEX-I::before {
  padding: 0.443em 0.716em 0.011em 0;
  content: "w";
}

mjx-c.mjx-c6D::before {
  padding: 0.442em 0.833em 0 0;
  content: "m";
}

mjx-c.mjx-c1D45B.TEX-I::before {
  padding: 0.442em 0.6em 0.011em 0;
  content: "n";
}

mjx-c.mjx-c1D456.TEX-I::before {
  padding: 0.661em 0.345em 0.011em 0;
  content: "i";
}

mjx-c.mjx-c226B::before {
  padding: 0.567em 1em 0.067em 0;
  content: "\226B";
}

mjx-c.mjx-c1D459.TEX-I::before {
  padding: 0.694em 0.298em 0.011em 0;
  content: "l";
}

mjx-c.mjx-c1D45C.TEX-I::before {
  padding: 0.441em 0.485em 0.011em 0;
  content: "o";
}

mjx-c.mjx-c1D454.TEX-I::before {
  padding: 0.442em 0.477em 0.205em 0;
  content: "g";
}

mjx-c.mjx-c25::before {
  padding: 0.75em 0.833em 0.056em 0;
  content: "%";
}

mjx-c.mjx-c26::before {
  padding: 0.716em 0.778em 0.022em 0;
  content: "&";
}
</style><script src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/RCde71e17d940247cd96c4c6b7435d0fdd-source.min.js" async=""></script><script src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/RCee1257a03b3249d08ecd7f8224565ef5-source.min.js" async=""></script><script type="text/javascript" async="" id="bizible-settings" data-account="nvidia.com" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/bizible.js"></script><script src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/RC151373b6896f49d2b2168a41b7edf51a-source.min.js" async=""></script><script src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/RCb239dc73bb784e559b639c5d018223ee-source.min.js" async=""></script><script src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/RCca8d79fb3fbe401fa19af031e12cab92-source.min.js" async=""></script><script src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/RC552e766f57c74b3d91815e039f3e3e99-source.min.js" async=""></script><script src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/RC3bedf6072a0349ab989530150d64d79a-source.min.js" async=""></script><script type="text/javascript" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/cdtm.js"></script><script type="text/javascript" async="" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/c6af8848c2687.js"></script><script async="" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/hotjar-3655182.js"></script><script type="text/javascript" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/js"></script><script type="text/javascript" async="" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/f_002.txt"></script><script type="text/javascript" async="" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/f.txt"></script><script async="" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/modules.f4e3c2911a3e11c9682d.js" charset="utf-8"></script><script src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/survey-v2.bacab71853455a2f7ea9.js"></script><style>._hj-Pbej5__styles__resetStyles *{line-height:normal;font-family:Arial, sans-serif, Tahoma !important;text-transform:initial !important;letter-spacing:normal !important}._hj-Pbej5__styles__resetStyles *::before,._hj-Pbej5__styles__resetStyles *::after{box-sizing:initial}._hj-Pbej5__styles__resetStyles div{height:auto}._hj-Pbej5__styles__resetStyles button{display:inline-block;height:auto;font-size:1rem}._hj-Pbej5__styles__resetStyles div,._hj-Pbej5__styles__resetStyles span,._hj-Pbej5__styles__resetStyles p,._hj-Pbej5__styles__resetStyles a,._hj-Pbej5__styles__resetStyles button{font-weight:normal !important}._hj-Pbej5__styles__resetStyles div,._hj-Pbej5__styles__resetStyles span,._hj-Pbej5__styles__resetStyles p,._hj-Pbej5__styles__resetStyles a,._hj-Pbej5__styles__resetStyles img,._hj-Pbej5__styles__resetStyles strong,._hj-Pbej5__styles__resetStyles form,._hj-Pbej5__styles__resetStyles label{border:0;font-size:100%;vertical-align:baseline;background:transparent;margin:0;padding:0;float:none !important}._hj-Pbej5__styles__resetStyles span{color:inherit}._hj-Pbej5__styles__resetStyles ol,._hj-Pbej5__styles__resetStyles ul,._hj-Pbej5__styles__resetStyles li{list-style:none !important;margin:0 !important;padding:0 !important}._hj-Pbej5__styles__resetStyles li:before,._hj-Pbej5__styles__resetStyles li:after{content:none !important}._hj-Pbej5__styles__resetStyles hr{display:block;height:1px;border:0;border-top:1px solid #ccc;margin:1em 0;padding:0}._hj-Pbej5__styles__resetStyles input[type='submit'],._hj-Pbej5__styles__resetStyles input[type='button'],._hj-Pbej5__styles__resetStyles button{margin:0;padding:0;float:none !important}._hj-Pbej5__styles__resetStyles input,._hj-Pbej5__styles__resetStyles select,._hj-Pbej5__styles__resetStyles a img{vertical-align:middle}._hj-s3UIi__styles__globalStyles *,._hj-s3UIi__styles__globalStyles *::before,._hj-s3UIi__styles__globalStyles *::after{box-sizing:border-box}@font-face{font-family:'hotjar';src:url(https://script.hotjar.com/font-hotjar_5.f4b154.eot);src:url(https://script.hotjar.com/font-hotjar_5.f4b154.eot#iefix) format("embedded-opentype"),url(https://script.hotjar.com/font-hotjar_5.65042d.woff2) format("woff2"),url(https://script.hotjar.com/font-hotjar_5.0ddfe2.ttf) format("truetype"),url(https://script.hotjar.com/font-hotjar_5.17b429.woff) format("woff"),url(https://script.hotjar.com/font-hotjar_5.2c7ab2.svg#hotjar) format("svg");font-weight:normal;font-style:normal}@keyframes _hj-eYRYp__styles__spin{0%{transform:rotate(0deg)}100%{transform:rotate(360deg)}}@keyframes _hj-5\+Z5O__styles__colors{0%{border-color:#f4364c;border-top-color:transparent}25%{border-color:#00a2f2;border-top-color:transparent}50%{border-color:#efb60c;border-top-color:transparent}75%{border-color:#42ca49;border-top-color:transparent}100%{border-color:#f4364c;border-top-color:transparent}}._hj-s3UIi__styles__globalStyles p{color:inherit !important}._hj-s3UIi__styles__globalStyles a,._hj-s3UIi__styles__globalStyles a:link,._hj-s3UIi__styles__globalStyles a:hover,._hj-s3UIi__styles__globalStyles a:active{color:inherit !important;text-decoration:underline}._hj-s3UIi__styles__globalStyles ._hj-L5SMl__styles__icon{speak:none !important;font-style:normal !important;font-weight:normal !important;font-variant:normal !important;text-transform:none !important;overflow-wrap:normal !important;word-break:normal !important;word-wrap:normal !important;white-space:nowrap !important;line-height:normal !important;-webkit-font-smoothing:antialiased !important;-moz-osx-font-smoothing:grayscale !important;vertical-align:middle !important}._hj-s3UIi__styles__globalStyles ._hj-L5SMl__styles__icon,._hj-s3UIi__styles__globalStyles ._hj-L5SMl__styles__icon:before,._hj-s3UIi__styles__globalStyles ._hj-L5SMl__styles__icon:after,._hj-s3UIi__styles__globalStyles ._hj-L5SMl__styles__icon *,._hj-s3UIi__styles__globalStyles ._hj-L5SMl__styles__icon *:before,._hj-s3UIi__styles__globalStyles ._hj-L5SMl__styles__icon *:after{font-family:'hotjar' !important;display:inline-block !important;direction:ltr !important}._hj-s3UIi__styles__globalStyles ._hj-L5SMl__styles__icon:before{color:inherit !important}._hj-s3UIi__styles__globalStyles ._hj-dk3Fb__styles__iconX:before{content:'\e803'}._hj-s3UIi__styles__globalStyles ._hj-9iDZB__styles__iconOk:before{content:'\e804'}._hj-s3UIi__styles__globalStyles ._hj-t13KX__styles__iconError:before{content:'\e90c'}._hj-s3UIi__styles__globalStyles ._hj-D\+oDX__styles__iconLogo:before{content:'\e806'}._hj-s3UIi__styles__globalStyles ._hj-Nbq9C__styles__iconSelectElement:before{content:'\e91a'}._hj-s3UIi__styles__globalStyles ._hj-mtJG6__styles__surveyIcons{background-repeat:no-repeat;width:16px;height:16px;display:inline-block !important;zoom:1;vertical-align:middle}._hj-widget-theme-light ._hj-s3UIi__styles__globalStyles ._hj-mtJG6__styles__surveyIcons{background-image:url(https://script.hotjar.com/widget_icons_light.766225.png)}._hj-widget-theme-dark ._hj-s3UIi__styles__globalStyles ._hj-mtJG6__styles__surveyIcons{background-image:url(https://script.hotjar.com/widget_icons_dark.ad934a.png)}._hj-s3UIi__styles__globalStyles ._hj-EZqbk__styles__inputField{font-family:Arial, sans-serif, Tahoma;font-size:14px;color:#333 !important;padding:6px !important;text-indent:0 !important;height:30px;width:100%;min-width:100%;background:white;border:1px solid !important;outline:none !important;max-width:none !important;float:none;border-radius:3px}._hj-s3UIi__styles__globalStyles ._hj-AwaE7__styles__textarea{resize:none;height:100px}._hj-s3UIi__styles__globalStyles ._hj-EIBGi__styles__basicButton,._hj-s3UIi__styles__globalStyles ._hj-SU8LU__styles__primaryButton{cursor:pointer;text-decoration:none;text-transform:capitalize;font-size:14px;font-weight:bold;padding:6px 16px !important;border:0;outline:0;display:inline-block;vertical-align:top;width:auto;zoom:1;transition:all 0.2s ease-in-out;box-shadow:0 2px 3px 0 rgba(0,0,0,0.15);border-radius:4px;color:white}._hj-s3UIi__styles__globalStyles ._hj-EIBGi__styles__basicButton:hover,._hj-s3UIi__styles__globalStyles ._hj-SU8LU__styles__primaryButton:hover,._hj-s3UIi__styles__globalStyles ._hj-EIBGi__styles__basicButton:focus,._hj-s3UIi__styles__globalStyles ._hj-SU8LU__styles__primaryButton:focus,._hj-s3UIi__styles__globalStyles ._hj-EIBGi__styles__basicButton:active,._hj-s3UIi__styles__globalStyles ._hj-SU8LU__styles__primaryButton:active{background:#00a251}._hj-s3UIi__styles__globalStyles ._hj-EIBGi__styles__basicButton[disabled],._hj-s3UIi__styles__globalStyles ._hj-SU8LU__styles__primaryButton[disabled]{cursor:default}._hj-s3UIi__styles__globalStyles ._hj-SU8LU__styles__primaryButton{font-size:14px !important;font-weight:500 !important;padding:6px 16px !important;border:0 !important;outline:0 !important;min-height:initial !important;width:auto !important;min-width:initial !important;background:var(--hjFeedbackAccentColor) !important;color:var(--hjFeedbackAccentTextColor) !important;box-shadow:none !important}._hj-s3UIi__styles__globalStyles ._hj-SU8LU__styles__primaryButton:hover,._hj-s3UIi__styles__globalStyles ._hj-SU8LU__styles__primaryButton:focus,._hj-s3UIi__styles__globalStyles ._hj-SU8LU__styles__primaryButton:active{background:var(--hjFeedbackAccentActiveColor) !important}._hj-s3UIi__styles__globalStyles ._hj-SU8LU__styles__primaryButton:focus{background:var(--hjFeedbackAccentColor) !important;box-shadow:0 0 0 1px var(--hjFeedbackPrimaryColor),0 0 0 3px var(--hjFeedbackAccentColor) !important}._hj-s3UIi__styles__globalStyles ._hj-SU8LU__styles__primaryButton:hover{background:var(--hjFeedbackAccentHoverColor) !important}._hj-s3UIi__styles__globalStyles ._hj-SU8LU__styles__primaryButton[disabled]{cursor:default;background:var(--hjFeedbackDisabledAccentColor) !important;color:var(--hjFeedbackDisabledAccentTextColor) !important}._hj-s3UIi__styles__globalStyles ._hj-F457\+__styles__clearButton{cursor:pointer;text-decoration:underline;font-size:13px !important;padding:0 10px !important;border:0 !important}._hj-s3UIi__styles__globalStyles ._hj-F457\+__styles__clearButton,._hj-s3UIi__styles__globalStyles ._hj-F457\+__styles__clearButton:hover,._hj-s3UIi__styles__globalStyles ._hj-F457\+__styles__clearButton:focus,._hj-s3UIi__styles__globalStyles ._hj-F457\+__styles__clearButton:active{background:transparent !important}._hj-s3UIi__styles__globalStyles ._hj-hTm4\+__styles__answersContentWrapper{padding:4px 12px 16px 12px}._hj-s3UIi__styles__globalStyles ._hj-ag9y\+__styles__spinner{border:1px solid rgba(0,0,0,0.6);border-top-color:transparent !important;border-radius:50%;transform:rotate(0deg);animation:_hj-eYRYp__styles__spin 0.4s linear infinite, _hj-5\+Z5O__styles__colors 5.6s ease-in-out infinite}._hj-s3UIi__styles__globalStyles ._hj-H1LCt__styles__widget{font-size:13px !important;position:fixed;z-index:2147483640;bottom:-400px;right:100px;width:300px;-webkit-border-radius:5px 5px 0 0;-moz-border-radius:5px 5px 0 0;border-radius:5px 5px 0 0;-webkit-transform:translateZ(0) !important;transform:translateZ(0) !important}._hj-AwaE7__styles__textarea{}._hj-dk3Fb__styles__iconX,._hj-9iDZB__styles__iconOk,._hj-t13KX__styles__iconError,._hj-D\+oDX__styles__iconLogo,._hj-Nbq9C__styles__iconSelectElement{}._hj-eJm8p__styles__rtl,._hj-eJm8p__styles__rtl *{direction:rtl !important}._hj-hc6BA__styles__roundedCorners{border-radius:3px}@media screen and (max-width: 480px){._hj-A4W17__styles__inlineSurvey{max-width:100%;overflow-x:auto}}
</style><style data-emotion="css-global" data-s=""></style><style data-emotion="css" data-s=""></style></head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
            <a href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/contents.html">
            <img src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/Logo_and_CUDA.png" class="logo" alt="Logo">
          </a>

<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get" __bizdiag="-592731329" __biza="WJ__">
    <input type="text" name="q" placeholder="Search docs">
    <input type="hidden" name="check_keywords" value="yes">
    <input type="hidden" name="area" value="default">
  </form>
</div>
        </div>
<div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current" aria-expanded="true">
<li class="toctree-l1 current" aria-expanded="true">
<a class="reference internal current" href="#" aria-expanded="true"><button class="toctree-expand" title="Open/close menu"></button>1. Preface</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#who-should-read-this-guide">1.1. Who Should Read This Guide?</a></li>
<li class="toctree-l2">
<a class="reference internal" href="#assess-parallelize-optimize-deploy"><button class="toctree-expand" title="Open/close menu"></button>1.2. Assess, Parallelize, Optimize, Deploy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#assess">1.2.1. Assess</a></li>
<li class="toctree-l3"><a class="reference internal" href="#parallelize">1.2.2. Parallelize</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optimize">1.2.3. Optimize</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deploy">1.2.4. Deploy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#recommendations-and-best-practices">1.3. Recommendations and Best Practices</a></li>
<li class="toctree-l2"><a class="reference internal" href="#assessing-your-application">1.4. Assessing Your Application</a></li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="#heterogeneous-computing"><button class="toctree-expand" title="Open/close menu"></button>2. Heterogeneous Computing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#differences-between-host-and-device">2.1. Differences between Host and Device</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-runs-on-a-cuda-enabled-device">2.2. What Runs on a CUDA-Enabled Device?</a></li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="#application-profiling"><button class="toctree-expand" title="Open/close menu"></button>3. Application Profiling</a><ul>
<li class="toctree-l2">
<a class="reference internal" href="#profile"><button class="toctree-expand" title="Open/close menu"></button>3.1. Profile</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#creating-the-profile">3.1.1. Creating the Profile</a></li>
<li class="toctree-l3"><a class="reference internal" href="#identifying-hotspots">3.1.2. Identifying Hotspots</a></li>
<li class="toctree-l3">
<a class="reference internal" href="#understanding-scaling"><button class="toctree-expand" title="Open/close menu"></button>3.1.3. Understanding Scaling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#strong-scaling-and-amdahl-s-law">3.1.3.1. Strong Scaling and Amdahl’s Law</a></li>
<li class="toctree-l4"><a class="reference internal" href="#weak-scaling-and-gustafson-s-law">3.1.3.2. Weak Scaling and Gustafson’s Law</a></li>
<li class="toctree-l4"><a class="reference internal" href="#applying-strong-and-weak-scaling">3.1.3.3. Applying Strong and Weak Scaling</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#parallelizing-your-application">4. Parallelizing Your Application</a></li>
<li class="toctree-l1">
<a class="reference internal" href="#getting-started"><button class="toctree-expand" title="Open/close menu"></button>5. Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#parallel-libraries">5.1. Parallel Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="#parallelizing-compilers">5.2. Parallelizing Compilers</a></li>
<li class="toctree-l2"><a class="reference internal" href="#coding-to-expose-parallelism">5.3. Coding to Expose Parallelism</a></li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="#getting-the-right-answer"><button class="toctree-expand" title="Open/close menu"></button>6. Getting the Right Answer</a><ul>
<li class="toctree-l2">
<a class="reference internal" href="#verification"><button class="toctree-expand" title="Open/close menu"></button>6.1. Verification</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#reference-comparison">6.1.1. Reference Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="#unit-testing">6.1.2. Unit Testing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#debugging">6.2. Debugging</a></li>
<li class="toctree-l2">
<a class="reference internal" href="#numerical-accuracy-and-precision"><button class="toctree-expand" title="Open/close menu"></button>6.3. Numerical Accuracy and Precision</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#single-vs-double-precision">6.3.1. Single vs. Double Precision</a></li>
<li class="toctree-l3"><a class="reference internal" href="#floating-point-math-is-not-associative">6.3.2. Floating Point Math Is Not Associative</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ieee-754-compliance">6.3.3. IEEE 754 Compliance</a></li>
<li class="toctree-l3"><a class="reference internal" href="#x86-80-bit-computations">6.3.4. x86 80-bit Computations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#optimizing-cuda-applications">7. Optimizing CUDA Applications</a></li>
<li class="toctree-l1">
<a class="reference internal" href="#performance-metrics"><button class="toctree-expand" title="Open/close menu"></button>8. Performance Metrics</a><ul>
<li class="toctree-l2">
<a class="reference internal" href="#timing"><button class="toctree-expand" title="Open/close menu"></button>8.1. Timing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#using-cpu-timers">8.1.1. Using CPU Timers</a></li>
<li class="toctree-l3"><a class="reference internal" href="#using-cuda-gpu-timers">8.1.2. Using CUDA GPU Timers</a></li>
</ul>
</li>
<li class="toctree-l2">
<a class="reference internal" href="#bandwidth"><button class="toctree-expand" title="Open/close menu"></button>8.2. Bandwidth</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#theoretical-bandwidth-calculation">8.2.1. Theoretical Bandwidth Calculation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#effective-bandwidth-calculation">8.2.2. Effective Bandwidth Calculation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#throughput-reported-by-visual-profiler">8.2.3. Throughput Reported by Visual Profiler</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="#memory-optimizations"><button class="toctree-expand" title="Open/close menu"></button>9. Memory Optimizations</a><ul>
<li class="toctree-l2">
<a class="reference internal" href="#data-transfer-between-host-and-device"><button class="toctree-expand" title="Open/close menu"></button>9.1. Data Transfer Between Host and Device</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pinned-memory">9.1.1. Pinned Memory</a></li>
<li class="toctree-l3"><a class="reference internal" href="#asynchronous-and-overlapping-transfers-with-computation">9.1.2. Asynchronous and Overlapping Transfers with Computation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#zero-copy">9.1.3. Zero Copy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#unified-virtual-addressing">9.1.4. Unified Virtual Addressing</a></li>
</ul>
</li>
<li class="toctree-l2">
<a class="reference internal" href="#device-memory-spaces"><button class="toctree-expand" title="Open/close menu"></button>9.2. Device Memory Spaces</a><ul>
<li class="toctree-l3">
<a class="reference internal" href="#coalesced-access-to-global-memory"><button class="toctree-expand" title="Open/close menu"></button>9.2.1. Coalesced Access to Global Memory</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#a-simple-access-pattern">9.2.1.1. A Simple Access Pattern</a></li>
<li class="toctree-l4"><a class="reference internal" href="#a-sequential-but-misaligned-access-pattern">9.2.1.2. A Sequential but Misaligned Access Pattern</a></li>
<li class="toctree-l4"><a class="reference internal" href="#effects-of-misaligned-accesses">9.2.1.3. Effects of Misaligned Accesses</a></li>
<li class="toctree-l4"><a class="reference internal" href="#strided-accesses">9.2.1.4. Strided Accesses</a></li>
</ul>
</li>
<li class="toctree-l3">
<a class="reference internal" href="#l2-cache"><button class="toctree-expand" title="Open/close menu"></button>9.2.2. L2 Cache</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#l2-cache-access-window">9.2.2.1. L2 Cache Access Window</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tuning-the-access-window-hit-ratio">9.2.2.2. Tuning the Access Window Hit-Ratio</a></li>
</ul>
</li>
<li class="toctree-l3">
<a class="reference internal" href="#shared-memory"><button class="toctree-expand" title="Open/close menu"></button>9.2.3. Shared Memory</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#shared-memory-and-memory-banks">9.2.3.1. Shared Memory and Memory Banks</a></li>
<li class="toctree-l4"><a class="reference internal" href="#shared-memory-in-matrix-multiplication-c-ab">9.2.3.2. Shared Memory in Matrix Multiplication (C=AB)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#shared-memory-in-matrix-multiplication-c-aat">9.2.3.3. Shared Memory in Matrix Multiplication (C=AAT)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#asynchronous-copy-from-global-memory-to-shared-memory">9.2.3.4. Asynchronous Copy from Global Memory to Shared Memory</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#local-memory">9.2.4. Local Memory</a></li>
<li class="toctree-l3">
<a class="reference internal" href="#texture-memory"><button class="toctree-expand" title="Open/close menu"></button>9.2.5. Texture Memory</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#additional-texture-capabilities">9.2.5.1. Additional Texture Capabilities</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#constant-memory">9.2.6. Constant Memory</a></li>
<li class="toctree-l3">
<a class="reference internal" href="#registers"><button class="toctree-expand" title="Open/close menu"></button>9.2.7. Registers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#register-pressure">9.2.7.1. Register Pressure</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#allocation">9.3. Allocation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#numa-best-practices">9.4. NUMA Best Practices</a></li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="#execution-configuration-optimizations"><button class="toctree-expand" title="Open/close menu"></button>10. Execution Configuration Optimizations</a><ul>
<li class="toctree-l2">
<a class="reference internal" href="#occupancy"><button class="toctree-expand" title="Open/close menu"></button>10.1. Occupancy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#calculating-occupancy">10.1.1. Calculating Occupancy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#hiding-register-dependencies">10.2. Hiding Register Dependencies</a></li>
<li class="toctree-l2"><a class="reference internal" href="#thread-and-block-heuristics">10.3. Thread and Block Heuristics</a></li>
<li class="toctree-l2"><a class="reference internal" href="#effects-of-shared-memory">10.4. Effects of Shared Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="#concurrent-kernel-execution">10.5. Concurrent Kernel Execution</a></li>
<li class="toctree-l2"><a class="reference internal" href="#multiple-contexts">10.6. Multiple contexts</a></li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="#instruction-optimization"><button class="toctree-expand" title="Open/close menu"></button>11. Instruction Optimization</a><ul>
<li class="toctree-l2">
<a class="reference internal" href="#arithmetic-instructions"><button class="toctree-expand" title="Open/close menu"></button>11.1. Arithmetic Instructions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#division-modulo-operations">11.1.1. Division Modulo Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loop-counters-signed-vs-unsigned">11.1.2. Loop Counters Signed vs. Unsigned</a></li>
<li class="toctree-l3"><a class="reference internal" href="#reciprocal-square-root">11.1.3. Reciprocal Square Root</a></li>
<li class="toctree-l3"><a class="reference internal" href="#other-arithmetic-instructions">11.1.4. Other Arithmetic Instructions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#exponentiation-with-small-fractional-arguments">11.1.5. Exponentiation With Small Fractional Arguments</a></li>
<li class="toctree-l3"><a class="reference internal" href="#math-libraries">11.1.6. Math Libraries</a></li>
<li class="toctree-l3"><a class="reference internal" href="#precision-related-compiler-flags">11.1.7. Precision-related Compiler Flags</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#memory-instructions">11.2. Memory Instructions</a></li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="#control-flow"><button class="toctree-expand" title="Open/close menu"></button>12. Control Flow</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#branching-and-divergence">12.1. Branching and Divergence</a></li>
<li class="toctree-l2"><a class="reference internal" href="#branch-predication">12.2. Branch Predication</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#deploying-cuda-applications">13. Deploying CUDA Applications</a></li>
<li class="toctree-l1">
<a class="reference internal" href="#understanding-the-programming-environment"><button class="toctree-expand" title="Open/close menu"></button>14. Understanding the Programming Environment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#cuda-compute-capability">14.1. CUDA Compute Capability</a></li>
<li class="toctree-l2"><a class="reference internal" href="#additional-hardware-data">14.2. Additional Hardware Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#which-compute-capability-target">14.3. Which Compute Capability Target</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cuda-runtime">14.4. CUDA Runtime</a></li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="#cuda-compatibility-developer-s-guide"><button class="toctree-expand" title="Open/close menu"></button>15. CUDA Compatibility Developer’s Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#cuda-toolkit-versioning">15.1. CUDA Toolkit Versioning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#source-compatibility">15.2. Source Compatibility</a></li>
<li class="toctree-l2">
<a class="reference internal" href="#binary-compatibility"><button class="toctree-expand" title="Open/close menu"></button>15.3. Binary Compatibility</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#cuda-binary-cubin-compatibility">15.3.1. CUDA Binary (cubin) Compatibility</a></li>
</ul>
</li>
<li class="toctree-l2">
<a class="reference internal" href="#cuda-compatibility-across-minor-releases"><button class="toctree-expand" title="Open/close menu"></button>15.4. CUDA Compatibility Across Minor Releases</a><ul>
<li class="toctree-l3">
<a class="reference internal" href="#existing-cuda-applications-within-minor-versions-of-cuda"><button class="toctree-expand" title="Open/close menu"></button>15.4.1. Existing CUDA Applications within Minor Versions of CUDA</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#handling-new-cuda-features-and-driver-apis">15.4.1.1. Handling New CUDA Features and Driver APIs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#using-ptx">15.4.1.2. Using PTX</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dynamic-code-generation">15.4.1.3. Dynamic Code Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#recommendations-for-building-a-minor-version-compatible-library">15.4.1.4. Recommendations for building a minor-version compatible library</a></li>
<li class="toctree-l4"><a class="reference internal" href="#recommendations-for-taking-advantage-of-minor-version-compatibility-in-your-application">15.4.1.5. Recommendations for taking advantage of minor version compatibility in your application</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="#preparing-for-deployment"><button class="toctree-expand" title="Open/close menu"></button>16. Preparing for Deployment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#testing-for-cuda-availability">16.1. Testing for CUDA Availability</a></li>
<li class="toctree-l2"><a class="reference internal" href="#error-handling">16.2. Error Handling</a></li>
<li class="toctree-l2"><a class="reference internal" href="#building-for-maximum-compatibility">16.3. Building for Maximum Compatibility</a></li>
<li class="toctree-l2">
<a class="reference internal" href="#distributing-the-cuda-runtime-and-libraries"><button class="toctree-expand" title="Open/close menu"></button>16.4. Distributing the CUDA Runtime and Libraries</a><ul>
<li class="toctree-l3">
<a class="reference internal" href="#cuda-toolkit-library-redistribution"><button class="toctree-expand" title="Open/close menu"></button>16.4.1. CUDA Toolkit Library Redistribution</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#which-files-to-redistribute">16.4.1.1. Which Files to Redistribute</a></li>
<li class="toctree-l4"><a class="reference internal" href="#where-to-install-redistributed-cuda-libraries">16.4.1.2. Where to Install Redistributed CUDA Libraries</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="#deployment-infrastructure-tools"><button class="toctree-expand" title="Open/close menu"></button>17. Deployment Infrastructure Tools</a><ul>
<li class="toctree-l2">
<a class="reference internal" href="#nvidia-smi"><button class="toctree-expand" title="Open/close menu"></button>17.1. Nvidia-SMI</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#queryable-state">17.1.1. Queryable state</a></li>
<li class="toctree-l3"><a class="reference internal" href="#modifiable-state">17.1.2. Modifiable state</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#nvml">17.2. NVML</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cluster-management-tools">17.3. Cluster Management Tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="#compiler-jit-cache-management-tools">17.4. Compiler JIT Cache Management Tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cuda-visible-devices">17.5. CUDA_VISIBLE_DEVICES</a></li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="#id84"><button class="toctree-expand" title="Open/close menu"></button>18. Recommendations and Best Practices</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overall-performance-optimization-strategies">18.1. Overall Performance Optimization Strategies</a></li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="#nvcc-compiler-switches"><button class="toctree-expand" title="Open/close menu"></button>19. nvcc Compiler Switches</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#nvcc">19.1. nvcc</a></li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="#notices"><button class="toctree-expand" title="Open/close menu"></button>20. Notices</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#notice">20.1. Notice</a></li>
<li class="toctree-l2"><a class="reference internal" href="#opencl">20.2. OpenCL</a></li>
<li class="toctree-l2"><a class="reference internal" href="#trademarks">20.3. Trademarks</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/contents.html">CUDA C++ Best Practices Guide</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">


  

<li>
<a href="https://docs.nvidia.com/cuda/index.html" class="icon icon-home"></a> »</li>
  
<li>
<span class="section-number">1. </span>Preface</li>

      <li class="wy-breadcrumbs-aside">
      </li>
<li class="wy-breadcrumbs-aside">


  <span>v12.9 |</span>



  <a href="https://docs.nvidia.com/cuda/pdf/CUDA_C_Best_Practices_Guide.pdf" class="reference external">PDF</a>



  <span>|</span>



  <a href="https://developer.nvidia.com/cuda-toolkit-archive" class="reference external">Archive</a>


  <span>&nbsp;</span>
</li>

  </ul>
  <hr>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p class="rubric-h1 rubric">CUDA C++ Best Practices Guide</p>
<p>The programming guide to using the CUDA Toolkit to obtain the best performance from NVIDIA GPUs.</p>
<section id="preface">
<h1>
<span class="section-number">1. </span>Preface<a class="headerlink" href="#preface" title="Permalink to this headline"></a>
</h1>
<p>This Best Practices Guide is a manual to help developers obtain the best performance from NVIDIA<sup>®</sup> CUDA<sup>®</sup>
 GPUs. It presents established parallelization and optimization 
techniques and explains coding metaphors and idioms that can greatly 
simplify programming for CUDA-capable GPU architectures.</p>
<p>While the contents can be used as a reference manual, you should be 
aware that some topics are revisited in different contexts as various 
programming and configuration topics are explored. As a result, it is 
recommended that first-time readers proceed through the guide 
sequentially. This approach will greatly improve your understanding of 
effective programming practices and enable you to better use the guide 
for reference later.</p>
<section id="who-should-read-this-guide">
<h2>
<span class="section-number">1.1. </span>Who Should Read This Guide?<a class="headerlink" href="#who-should-read-this-guide" title="Permalink to this headline"></a>
</h2>
<p>The discussions in this guide all use the C++ programming language, so you should be comfortable reading C++ code.</p>
<p>This guide refers to and relies on several other documents that you 
should have at your disposal for reference, all of which are available 
at no cost from the CUDA website <a class="reference external" href="https://docs.nvidia.com/cuda/">https://docs.nvidia.com/cuda/</a>. The following documents are especially important resources:</p>
<ul class="simple">
<li><p>CUDA Installation Guide</p></li>
<li><p>CUDA C++ Programming Guide</p></li>
<li><p>CUDA Toolkit Reference Manual</p></li>
</ul>
<p>In particular, the optimization section of this guide assumes that 
you have already successfully downloaded and installed the CUDA Toolkit 
(if not, please refer to the relevant CUDA Installation Guide for your 
platform) and that you have a basic familiarity with the CUDA C++ 
programming language and environment (if not, please refer to the CUDA 
C++ Programming Guide).</p>
</section>
<section id="assess-parallelize-optimize-deploy">
<h2>
<span class="section-number">1.2. </span>Assess, Parallelize, Optimize, Deploy<a class="headerlink" href="#assess-parallelize-optimize-deploy" title="Permalink to this headline"></a>
</h2>
<p>This guide introduces the <em>Assess, Parallelize, Optimize, Deploy(APOD)</em>
 design cycle for applications with the goal of helping application 
developers to rapidly identify the portions of their code that would 
most readily benefit from GPU acceleration, rapidly realize that 
benefit, and begin leveraging the resulting speedups in production as 
early as possible.</p>
<p>APOD is a cyclical process: initial speedups can be achieved, tested,
 and deployed with only minimal initial investment of time, at which 
point the cycle can begin again by identifying further optimization 
opportunities, seeing additional speedups, and then deploying the even 
faster versions of the application into production.</p>
<figure class="align-center">
<img alt="_images/apod-cycle.png" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/apod-cycle.png">
</figure>
<section id="assess">
<h3>
<span class="section-number">1.2.1. </span>Assess<a class="headerlink" href="#assess" title="Permalink to this headline"></a>
</h3>
<p>For an existing project, the first step is to assess the application 
to locate the parts of the code that are responsible for the bulk of the
 execution time. Armed with this knowledge, the developer can evaluate 
these bottlenecks for parallelization and start to investigate GPU 
acceleration.</p>
<p>By understanding the end-user’s requirements and constraints and by 
applying Amdahl’s and Gustafson’s laws, the developer can determine the 
upper bound of performance improvement from acceleration of the 
identified portions of the application.</p>
</section>
<section id="parallelize">
<h3>
<span class="section-number">1.2.2. </span>Parallelize<a class="headerlink" href="#parallelize" title="Permalink to this headline"></a>
</h3>
<p>Having identified the hotspots and having done the basic exercises to
 set goals and expectations, the developer needs to parallelize the 
code. Depending on the original code, this can be as simple as calling 
into an existing GPU-optimized library such as <code class="docutils literal notranslate"><span class="pre">cuBLAS</span></code>, <code class="docutils literal notranslate"><span class="pre">cuFFT</span></code>, or <code class="docutils literal notranslate"><span class="pre">Thrust</span></code>, or it could be as simple as adding a few preprocessor directives as hints to a parallelizing compiler.</p>
<p>On the other hand, some applications’ designs will require some 
amount of refactoring to expose their inherent parallelism. As even CPU 
architectures will require exposing parallelism in order to improve or 
simply maintain the performance of sequential applications, the CUDA 
family of parallel programming languages (CUDA C++, CUDA Fortran, etc.) 
aims to make the expression of this parallelism as simple as possible, 
while simultaneously enabling operation on CUDA-capable GPUs designed 
for maximum parallel throughput.</p>
</section>
<section id="optimize">
<h3>
<span class="section-number">1.2.3. </span>Optimize<a class="headerlink" href="#optimize" title="Permalink to this headline"></a>
</h3>
<p>After each round of application parallelization is complete, the 
developer can move to optimizing the implementation to improve 
performance. Since there are many possible optimizations that can be 
considered, having a good understanding of the needs of the application 
can help to make the process as smooth as possible. However, as with 
APOD as a whole, program optimization is an iterative process (identify 
an opportunity for optimization, apply and test the optimization, verify
 the speedup achieved, and repeat), meaning that it is not necessary for
 a programmer to spend large amounts of time memorizing the bulk of all 
possible optimization strategies prior to seeing good speedups. Instead,
 strategies can be applied incrementally as they are learned.</p>
<p>Optimizations can be applied at various levels, from overlapping data
 transfers with computation all the way down to fine-tuning 
floating-point operation sequences. The available profiling tools are 
invaluable for guiding this process, as they can help suggest a 
next-best course of action for the developer’s optimization efforts and 
provide references into the relevant portions of the optimization 
section of this guide.</p>
</section>
<section id="deploy">
<h3>
<span class="section-number">1.2.4. </span>Deploy<a class="headerlink" href="#deploy" title="Permalink to this headline"></a>
</h3>
<p>Having completed the GPU acceleration of one or more components of 
the application it is possible to compare the outcome with the original 
expectation. Recall that the initial <em>assess</em> step allowed the developer to determine an upper bound for the potential speedup attainable by accelerating given hotspots.</p>
<p>Before tackling other hotspots to improve the total speedup, the 
developer should consider taking the partially parallelized 
implementation and carry it through to production. This is important for
 a number of reasons; for example, it allows the user to profit from 
their investment as early as possible (the speedup may be partial but is
 still valuable), and it minimizes risk for the developer and the user 
by providing an evolutionary rather than revolutionary set of changes to
 the application.</p>
</section>
</section>
<section id="recommendations-and-best-practices">
<h2>
<span class="section-number">1.3. </span>Recommendations and Best Practices<a class="headerlink" href="#recommendations-and-best-practices" title="Permalink to this headline"></a>
</h2>
<p>Throughout this guide, specific recommendations are made regarding 
the design and implementation of CUDA C++ code. These recommendations 
are categorized by priority, which is a blend of the effect of the 
recommendation and its scope. Actions that present substantial 
improvements for most CUDA applications have the highest priority, while
 small optimizations that affect only very specific situations are given
 a lower priority.</p>
<p>Before implementing lower priority recommendations, it is good 
practice to make sure all higher priority recommendations that are 
relevant have already been applied. This approach will tend to provide 
the best results for the time invested and will avoid the trap of 
premature optimization.</p>
<p>The criteria of benefit and scope for establishing priority will vary
 depending on the nature of the program. In this guide, they represent a
 typical case. Your code might reflect different priority factors. 
Regardless of this possibility, it is good practice to verify that no 
higher-priority recommendations have been overlooked before undertaking 
lower-priority items.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Code samples throughout the guide omit error checking for 
conciseness. Production code should, however, systematically check the 
error code returned by each API call and check for failures in kernel 
launches by calling <code class="docutils literal notranslate"><span class="pre">cudaGetLastError()</span></code>.</p>
</div>
</section>
<section id="assessing-your-application">
<h2>
<span class="section-number">1.4. </span>Assessing Your Application<a class="headerlink" href="#assessing-your-application" title="Permalink to this headline"></a>
</h2>
<p>From supercomputers to mobile phones, modern processors increasingly 
rely on parallelism to provide performance. The core computational unit,
 which includes control, arithmetic, registers and typically some cache,
 is replicated some number of times and connected to memory via a 
network. As a result, all modern processors require parallel code in 
order to achieve good utilization of their computational power.</p>
<p>While processors are evolving to expose more fine-grained parallelism
 to the programmer, many existing applications have evolved either as 
serial codes or as coarse-grained parallel codes (for example, where the
 data is decomposed into regions processed in parallel, with sub-regions
 shared using MPI). In order to profit from any modern processor 
architecture, GPUs included, the first steps are to assess the 
application to identify the hotspots, determine whether they can be 
parallelized, and understand the relevant workloads both now and in the 
future.</p>
</section>
</section>
<section id="heterogeneous-computing">
<span id="id1"></span><h1>
<span class="section-number">2. </span>Heterogeneous Computing<a class="headerlink" href="#heterogeneous-computing" title="Permalink to this headline"></a>
</h1>
<p>CUDA programming involves running code on two different platforms concurrently: a <em>host</em> system with one or more CPUs and one or more CUDA-enabled NVIDIA GPU <em>devices</em>.</p>
<p>While NVIDIA GPUs are frequently associated with graphics, they are 
also powerful arithmetic engines capable of running thousands of 
lightweight threads in parallel. This capability makes them well suited 
to computations that can leverage parallel execution.</p>
<p>However, the device is based on a distinctly different design from 
the host system, and it’s important to understand those differences and 
how they determine the performance of CUDA applications in order to use 
CUDA effectively.</p>
<section id="differences-between-host-and-device">
<span id="id2"></span><h2>
<span class="section-number">2.1. </span>Differences between Host and Device<a class="headerlink" href="#differences-between-host-and-device" title="Permalink to this headline"></a>
</h2>
<p>The primary differences are in threading model and in separate physical memories:</p>
<dl class="simple">
<dt>Threading resources</dt>
<dd>
<p>Execution pipelines on host systems can support a limited number of 
concurrent threads. For example, servers that have two 32 core 
processors can run only 64 threads concurrently (or small multiple of 
that if the CPUs support simultaneous multithreading). By comparison, 
the <em>smallest</em> executable unit of parallelism on a CUDA device comprises 32 threads (termed a <em>warp</em>
 of threads). Modern NVIDIA GPUs can support up to 2048 active threads 
concurrently per multiprocessor (see Features and Specifications of the 
CUDA C++ Programming Guide) On GPUs with 80 multiprocessors, this leads 
to more than 160,000 concurrently active threads.</p>
</dd>
<dt>Threads</dt>
<dd>
<p>Threads on a CPU are generally heavyweight entities. The operating 
system must swap threads on and off CPU execution channels to provide 
multithreading capability. Context switches (when two threads are 
swapped) are therefore slow and expensive. By comparison, threads on 
GPUs are extremely lightweight. In a typical system, thousands of 
threads are queued up for work (in warps of 32 threads each). If the GPU
 must wait on one warp of threads, it simply begins executing work on 
another. Because separate registers are allocated to all active threads,
 no swapping of registers or other state need occur when switching among
 GPU threads. Resources stay allocated to each thread until it completes
 its execution. In short, CPU cores are designed to <em>minimize latency</em>
 for a small number of threads at a time each, whereas GPUs are designed
 to handle a large number of concurrent, lightweight threads in order to
 <em>maximize throughput</em>.</p>
</dd>
<dt>RAM</dt>
<dd>
<p>The host system and the device each have their own distinct attached physical memories <a class="footnote-reference brackets" href="#fn1" id="id3">1</a>.
 As the host and device memories are separated, items in the host memory
 must occasionally be communicated between device memory and host memory
 as described in <a class="reference internal" href="#what-runs-on-cuda-enabled-device"><span class="std std-ref">What Runs on a CUDA-Enabled Device?</span></a>.</p>
</dd>
</dl>
<p>These are the primary hardware differences between CPU hosts and GPU 
devices with respect to parallel programming. Other differences are 
discussed as they arise elsewhere in this document. Applications 
composed with these differences in mind can treat the host and device 
together as a cohesive heterogeneous system wherein each processing unit
 is leveraged to do the kind of work it does best: sequential work on 
the host and parallel work on the device.</p>
</section>
<section id="what-runs-on-a-cuda-enabled-device">
<span id="what-runs-on-cuda-enabled-device"></span><h2>
<span class="section-number">2.2. </span>What Runs on a CUDA-Enabled Device?<a class="headerlink" href="#what-runs-on-a-cuda-enabled-device" title="Permalink to this headline"></a>
</h2>
<p>The following issues should be considered when determining what parts of an application to run on the device:</p>
<ul>
<li><p>The device is ideally suited for computations that can be run on 
numerous data elements simultaneously in parallel. This typically 
involves arithmetic on large data sets (such as matrices) where the same
 operation can be performed across thousands, if not millions, of 
elements at the same time. This is a requirement for good performance on
 CUDA: the software must use a large number (generally thousands or tens
 of thousands) of concurrent threads. The support for running numerous 
threads in parallel derives from CUDA’s use of a lightweight threading 
model described above.</p></li>
<li>
<p>To use CUDA, data values must be transferred from the host to the 
device. These transfers are costly in terms of performance and should be
 minimized. (See <a class="reference internal" href="#data-transfer-between-host-and-device"><span class="std std-ref">Data Transfer Between Host and Device</span></a>.) This cost has several ramifications:</p>
<ul>
<li>
<p>The complexity of operations should justify the cost of moving data 
to and from the device. Code that transfers data for brief use by a 
small number of threads will see little or no performance benefit. The 
ideal scenario is one in which many threads perform a substantial amount
 of work.</p>
<p>For example, transferring two matrices to the device to perform a 
matrix addition and then transferring the results back to the host will 
not realize much performance benefit. The issue here is the number of 
operations performed per data element transferred. For the preceding 
procedure, assuming matrices of size NxN, there are N<sup>2</sup> operations (additions) and 3N<sup>2</sup>
 elements transferred, so the ratio of operations to elements 
transferred is 1:3 or O(1). Performance benefits can be more readily 
achieved when this ratio is higher. For example, a matrix multiplication
 of the same matrices requires N<sup>3</sup> operations (multiply-add), 
so the ratio of operations to elements transferred is O(N), in which 
case the larger the matrix the greater the performance benefit. The 
types of operations are an additional factor, as additions have 
different complexity profiles than, for example, trigonometric 
functions. It is important to include the overhead of transferring data 
to and from the device in determining whether operations should be 
performed on the host or on the device.</p>
</li>
<li><p>Data should be kept on the device as long as possible. Because 
transfers should be minimized, programs that run multiple kernels on the
 same data should favor leaving the data on the device between kernel 
calls, rather than transferring intermediate results to the host and 
then sending them back to the device for subsequent calculations. So, in
 the previous example, had the two matrices to be added already been on 
the device as a result of some previous calculation, or if the results 
of the addition would be used in some subsequent calculation, the matrix
 addition should be performed locally on the device. This approach 
should be used even if one of the steps in a sequence of calculations 
could be performed faster on the host. Even a relatively slow kernel may
 be advantageous if it avoids one or more transfers between host and 
device memory. <a class="reference internal" href="#data-transfer-between-host-and-device"><span class="std std-ref">Data Transfer Between Host and Device</span></a>
 provides further details, including the measurements of bandwidth 
between the host and the device versus within the device proper.</p></li>
</ul>
</li>
<li><p>For best performance, there should be some coherence in memory 
access by adjacent threads running on the device. Certain memory access 
patterns enable the hardware to coalesce groups of reads or writes of 
multiple data items into one operation. Data that cannot be laid out so 
as to enable <em>coalescing</em>, or that doesn’t have enough locality 
to use the L1 or texture caches effectively, will tend to see lesser 
speedups when used in computations on GPUs. A noteworthy exception to 
this are completely random memory access patterns. In general, they 
should be avoided, because compared to peak capabilities any 
architecture processes these memory access patterns at a low efficiency.
 However, compared to cache based architectures, like CPUs, latency 
hiding architectures, like GPUs, tend to cope better with completely 
random memory access patterns.</p></li>
</ul>
<dl class="footnote brackets">
<dt class="label" id="fn1"><span class="brackets"><a class="fn-backref" href="#id3">1</a></span></dt>
<dd>
<p>On Systems on a Chip with integrated GPUs, such as NVIDIA® Tegra®, 
host and device memory are physically the same, but there is still a 
logical distinction between host and device memory. See the <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote">Application Note on CUDA for Tegra</a> for details.</p>
</dd>
</dl>
</section>
</section>
<section id="application-profiling">
<span id="id4"></span><h1>
<span class="section-number">3. </span>Application Profiling<a class="headerlink" href="#application-profiling" title="Permalink to this headline"></a>
</h1>
<section id="profile">
<span id="id5"></span><h2>
<span class="section-number">3.1. </span>Profile<a class="headerlink" href="#profile" title="Permalink to this headline"></a>
</h2>
<p>Many codes accomplish a significant portion of the work with a 
relatively small amount of code. Using a profiler, the developer can 
identify such hotspots and start to compile a list of candidates for 
parallelization.</p>
<section id="creating-the-profile">
<span id="creating-profile"></span><h3>
<span class="section-number">3.1.1. </span>Creating the Profile<a class="headerlink" href="#creating-the-profile" title="Permalink to this headline"></a>
</h3>
<p>There are many possible approaches to profiling the code, but in all 
cases the objective is the same: to identify the function or functions 
in which the application is spending most of its execution time.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>High Priority:</strong> To maximize developer productivity, profile the application to determine hotspots and bottlenecks.</p>
</div>
<p>The most important consideration with any profiling activity is to 
ensure that the workload is realistic - i.e., that information gained 
from the test and decisions based upon that information are relevant to 
real data. Using unrealistic workloads can lead to sub-optimal results 
and wasted effort both by causing developers to optimize for unrealistic
 problem sizes and by causing developers to concentrate on the wrong 
functions.</p>
<p>There are a number of tools that can be used to generate the profile. The following example is based on <code class="docutils literal notranslate"><span class="pre">gprof</span></code>, which is an open-source profiler for Linux platforms from the GNU Binutils collection.</p>
<div class="highlight-text notranslate">
<div class="highlight"><pre id="codecell0"><span></span>$ gcc -O2 -g -pg myprog.c
$ gprof ./a.out &gt; profile.txt
Each sample counts as 0.01 seconds.
  %   cumulative   self              self     total
 time   seconds   seconds    calls  ms/call  ms/call  name
 33.34      0.02     0.02     7208     0.00     0.00  genTimeStep
 16.67      0.03     0.01      240     0.04     0.12  calcStats
 16.67      0.04     0.01        8     1.25     1.25  calcSummaryData
 16.67      0.05     0.01        7     1.43     1.43  write
 16.67      0.06     0.01                             mcount
  0.00      0.06     0.00      236     0.00     0.00  tzset
  0.00      0.06     0.00      192     0.00     0.00  tolower
  0.00      0.06     0.00       47     0.00     0.00  strlen
  0.00      0.06     0.00       45     0.00     0.00  strchr
  0.00      0.06     0.00        1     0.00    50.00  main
  0.00      0.06     0.00        1     0.00     0.00  memcpy
  0.00      0.06     0.00        1     0.00    10.11  print
  0.00      0.06     0.00        1     0.00     0.00  profil
  0.00      0.06     0.00        1     0.00    50.00  report
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell0">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
</section>
<section id="identifying-hotspots">
<span id="id6"></span><h3>
<span class="section-number">3.1.2. </span>Identifying Hotspots<a class="headerlink" href="#identifying-hotspots" title="Permalink to this headline"></a>
</h3>
<p>In the example above, we can clearly see that the function <code class="docutils literal notranslate"><span class="pre">genTimeStep()</span></code> takes one-third of the total running time of the application. This should be our first candidate function for parallelization. <a class="reference internal" href="#understanding-scaling"><span class="std std-ref">Understanding Scaling</span></a> discusses the potential benefit we might expect from such parallelization.</p>
<p>It is worth noting that several of the other functions in the above 
example also take up a significant portion of the overall running time, 
such as <code class="docutils literal notranslate"><span class="pre">calcStats()</span></code> and <code class="docutils literal notranslate"><span class="pre">calcSummaryData()</span></code>.
 Parallelizing these functions as well should increase our speedup 
potential. However, since APOD is a cyclical process, we might opt to 
parallelize these functions in a subsequent APOD pass, thereby limiting 
the scope of our work in any given pass to a smaller set of incremental 
changes.</p>
</section>
<section id="understanding-scaling">
<span id="id7"></span><h3>
<span class="section-number">3.1.3. </span>Understanding Scaling<a class="headerlink" href="#understanding-scaling" title="Permalink to this headline"></a>
</h3>
<p>The amount of performance benefit an application will realize by 
running on CUDA depends entirely on the extent to which it can be 
parallelized. Code that cannot be sufficiently parallelized should run 
on the host, unless doing so would result in excessive transfers between
 the host and the device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>High Priority:</strong> To get the maximum benefit from CUDA, focus first on finding ways to parallelize sequential code.</p>
</div>
<p>By understanding how applications can scale it is possible to set 
expectations and plan an incremental parallelization strategy. <a class="reference internal" href="#strong-scaling-and-amdahls-law"><span class="std std-ref">Strong Scaling and Amdahl’s Law</span></a> describes strong scaling, which allows us to set an upper bound for the speedup with a fixed problem size. <a class="reference internal" href="#weak-scaling-and-gustafsons-law"><span class="std std-ref">Weak Scaling and Gustafson’s Law</span></a>
 describes weak scaling, where the speedup is attained by growing the 
problem size. In many applications, a combination of strong and weak 
scaling is desirable.</p>
<section id="strong-scaling-and-amdahl-s-law">
<span id="strong-scaling-and-amdahls-law"></span><h4>
<span class="section-number">3.1.3.1. </span>Strong Scaling and Amdahl’s Law<a class="headerlink" href="#strong-scaling-and-amdahl-s-law" title="Permalink to this headline"></a>
</h4>
<p>Strong scaling is a measure of how, for a fixed overall problem size,
 the time to solution decreases as more processors are added to a 
system. An application that exhibits linear strong scaling has a speedup
 equal to the number of processors used.</p>
<p>Strong scaling is usually equated with Amdahl’s Law, which specifies 
the maximum speedup that can be expected by parallelizing portions of a 
serial program. Essentially, it states that the maximum speedup <em>S</em> of a program is:</p>
<p><span class="math notranslate nohighlight"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 119.5%; position: relative;" tabindex="0" ctxtmenu_counter="0"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mfrac space="4"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow size="s"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>S</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>P</mi><mo stretchy="false">)</mo><mo>+</mo><mfrac><mi>P</mi><mi>N</mi></mfrac></mrow></mfrac></math></mjx-assistive-mml></mjx-container></span></p>
<p>Here <em>P</em> is the fraction of the total serial execution time taken by the portion of code that can be parallelized and <em>N</em> is the number of processors over which the parallel portion of the code runs.</p>
<p>The larger <em>N</em> is(that is, the greater the number of processors), the smaller the <em>P/N</em> fraction. It can be simpler to view <em>N</em> as a very large number, which essentially transforms the equation into <span class="math notranslate nohighlight"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 119.5%; position: relative;" tabindex="0" ctxtmenu_counter="1"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>S</mi><mo>=</mo><mn>1</mn><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>P</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></span>.
 Now, if 3/4 of the running time of a sequential program is 
parallelized, the maximum speedup over serial code is 1 / (1 - 3/4) = 4.</p>
<p>In reality, most applications do not exhibit perfectly linear strong 
scaling, even if they do exhibit some degree of strong scaling. For most
 purposes, the key point is that the larger the parallelizable portion <em>P</em> is, the greater the potential speedup. Conversely, if <em>P</em> is a small number (meaning that the application is not substantially parallelizable), increasing the number of processors <em>N</em>
 does little to improve performance. Therefore, to get the largest 
speedup for a fixed problem size, it is worthwhile to spend effort on 
increasing <em>P</em>, maximizing the amount of code that can be parallelized.</p>
</section>
<section id="weak-scaling-and-gustafson-s-law">
<span id="weak-scaling-and-gustafsons-law"></span><h4>
<span class="section-number">3.1.3.2. </span>Weak Scaling and Gustafson’s Law<a class="headerlink" href="#weak-scaling-and-gustafson-s-law" title="Permalink to this headline"></a>
</h4>
<p>Weak scaling is a measure of how the time to solution changes as more
 processors are added to a system with a fixed problem size <em>per processor</em>; i.e., where the overall problem size increases as the number of processors is increased.</p>
<p>Weak scaling is often equated with Gustafson’s Law, which states that
 in practice, the problem size scales with the number of processors. 
Because of this, the maximum speedup <em>S</em> of a program is:</p>
<p><span class="math notranslate nohighlight"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 119.5%; position: relative;" tabindex="0" ctxtmenu_counter="2"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>S</mi><mo>=</mo><mi>N</mi><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>P</mi><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>N</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></span></p>
<p>Here <em>P</em> is the fraction of the total serial execution time taken by the portion of code that can be parallelized and <em>N</em> is the number of processors over which the parallel portion of the code runs.</p>
<p>Another way of looking at Gustafson’s Law is that it is not the 
problem size that remains constant as we scale up the system but rather 
the execution time. Note that Gustafson’s Law assumes that the ratio of 
serial to parallel execution remains constant, reflecting additional 
cost in setting up and handling the larger problem.</p>
</section>
<section id="applying-strong-and-weak-scaling">
<span id="id8"></span><h4>
<span class="section-number">3.1.3.3. </span>Applying Strong and Weak Scaling<a class="headerlink" href="#applying-strong-and-weak-scaling" title="Permalink to this headline"></a>
</h4>
<p>Understanding which type of scaling is most applicable to an 
application is an important part of estimating speedup. For some 
applications the problem size will remain constant and hence only strong
 scaling is applicable. An example would be modeling how two molecules 
interact with each other, where the molecule sizes are fixed.</p>
<p>For other applications, the problem size will grow to fill the 
available processors. Examples include modeling fluids or structures as 
meshes or grids and some Monte Carlo simulations, where increasing the 
problem size provides increased accuracy.</p>
<p>Having understood the application profile, the developer should 
understand how the problem size would change if the computational 
performance changes and then apply either Amdahl’s or Gustafson’s Law to
 determine an upper bound for the speedup.</p>
</section>
</section>
</section>
</section>
<section id="parallelizing-your-application">
<h1>
<span class="section-number">4. </span>Parallelizing Your Application<a class="headerlink" href="#parallelizing-your-application" title="Permalink to this headline"></a>
</h1>
<p>Having identified the hotspots and having done the basic exercises to
 set goals and expectations, the developer needs to parallelize the 
code. Depending on the original code, this can be as simple as calling 
into an existing GPU-optimized library such as <code class="docutils literal notranslate"><span class="pre">cuBLAS</span></code>, <code class="docutils literal notranslate"><span class="pre">cuFFT</span></code>, or <code class="docutils literal notranslate"><span class="pre">Thrust</span></code>, or it could be as simple as adding a few preprocessor directives as hints to a parallelizing compiler.</p>
<p>On the other hand, some applications’ designs will require some 
amount of refactoring to expose their inherent parallelism. As even CPU 
architectures require exposing this parallelism in order to improve or 
simply maintain the performance of sequential applications, the CUDA 
family of parallel programming languages (CUDA C++, CUDA Fortran, etc.) 
aims to make the expression of this parallelism as simple as possible, 
while simultaneously enabling operation on CUDA-capable GPUs designed 
for maximum parallel throughput.</p>
</section>
<section id="getting-started">
<h1>
<span class="section-number">5. </span>Getting Started<a class="headerlink" href="#getting-started" title="Permalink to this headline"></a>
</h1>
<p>There are several key strategies for parallelizing sequential code. 
While the details of how to apply these strategies to a particular 
application is a complex and problem-specific topic, the general themes 
listed here apply regardless of whether we are parallelizing code to run
 on for multicore CPUs or for use on CUDA GPUs.</p>
<section id="parallel-libraries">
<h2>
<span class="section-number">5.1. </span>Parallel Libraries<a class="headerlink" href="#parallel-libraries" title="Permalink to this headline"></a>
</h2>
<p>The most straightforward approach to parallelizing an application is 
to leverage existing libraries that take advantage of parallel 
architectures on our behalf. The CUDA Toolkit includes a number of such 
libraries that have been fine-tuned for NVIDIA CUDA GPUs, such as <code class="docutils literal notranslate"><span class="pre">cuBLAS</span></code>, <code class="docutils literal notranslate"><span class="pre">cuFFT</span></code>, and so on.</p>
<p>The key here is that libraries are most useful when they match well 
with the needs of the application. Applications already using other BLAS
 libraries can often quite easily switch to <code class="docutils literal notranslate"><span class="pre">cuBLAS</span></code>, for example, whereas applications that do little to no linear algebra will have little use for <code class="docutils literal notranslate"><span class="pre">cuBLAS</span></code>. The same goes for other CUDA Toolkit libraries: <code class="docutils literal notranslate"><span class="pre">cuFFT</span></code> has an interface similar to that of <code class="docutils literal notranslate"><span class="pre">FFTW</span></code>, etc.</p>
<p>Also of note is the Thrust library, which is a parallel C++ template 
library similar to the C++ Standard Template Library. Thrust provides a 
rich collection of data parallel primitives such as scan, sort, and 
reduce, which can be composed together to implement complex algorithms 
with concise, readable source code. By describing your computation in 
terms of these high-level abstractions you provide Thrust with the 
freedom to select the most efficient implementation automatically. As a 
result, Thrust can be utilized in rapid prototyping of CUDA 
applications, where programmer productivity matters most, as well as in 
production, where robustness and absolute performance are crucial.</p>
</section>
<section id="parallelizing-compilers">
<h2>
<span class="section-number">5.2. </span>Parallelizing Compilers<a class="headerlink" href="#parallelizing-compilers" title="Permalink to this headline"></a>
</h2>
<p>Another common approach to parallelization of sequential codes is to 
make use of parallelizing compilers. Often this means the use of 
directives-based approaches, where the programmer uses a pragma or other
 similar notation to provide hints to the compiler about where 
parallelism can be found without needing to modify or adapt the 
underlying code itself. By exposing parallelism to the compiler, 
directives allow the compiler to do the detailed work of mapping the 
computation onto the parallel architecture.</p>
<p>The OpenACC standard provides a set of compiler directives to specify
 loops and regions of code in standard C, C++ and Fortran that should be
 offloaded from a host CPU to an attached accelerator such as a CUDA 
GPU. The details of managing the accelerator device are handled 
implicitly by an OpenACC-enabled compiler and runtime.</p>
<p>See <a class="reference external" href="http://www.openacc.org/">http://www.openacc.org/</a> for details.</p>
</section>
<section id="coding-to-expose-parallelism">
<h2>
<span class="section-number">5.3. </span>Coding to Expose Parallelism<a class="headerlink" href="#coding-to-expose-parallelism" title="Permalink to this headline"></a>
</h2>
<p>For applications that need additional functionality or performance 
beyond what existing parallel libraries or parallelizing compilers can 
provide, parallel programming languages such as CUDA C++ that integrate 
seamlessly with existing sequential code are essential.</p>
<p>Once we have located a hotspot in our application’s profile 
assessment and determined that custom code is the best approach, we can 
use CUDA C++ to expose the parallelism in that portion of our code as a 
CUDA kernel. We can then launch this kernel onto the GPU and retrieve 
the results without requiring major rewrites to the rest of our 
application.</p>
<p>This approach is most straightforward when the majority of the total 
running time of our application is spent in a few relatively isolated 
portions of the code. More difficult to parallelize are applications 
with a very flat profile - i.e., applications where the time spent is 
spread out relatively evenly across a wide portion of the code base. For
 the latter variety of application, some degree of code refactoring to 
expose the inherent parallelism in the application might be necessary, 
but keep in mind that this refactoring work will tend to benefit all 
future architectures, CPU and GPU alike, so it is well worth the effort 
should it become necessary.</p>
</section>
</section>
<section id="getting-the-right-answer">
<span id="getting-right-answer"></span><h1>
<span class="section-number">6. </span>Getting the Right Answer<a class="headerlink" href="#getting-the-right-answer" title="Permalink to this headline"></a>
</h1>
<p>Obtaining the right answer is clearly the principal goal of all 
computation. On parallel systems, it is possible to run into 
difficulties not typically found in traditional serial-oriented 
programming. These include threading issues, unexpected values due to 
the way floating-point values are computed, and challenges arising from 
differences in the way CPU and GPU processors operate. This chapter 
examines issues that can affect the correctness of returned data and 
points to appropriate solutions.</p>
<section id="verification">
<span id="id9"></span><h2>
<span class="section-number">6.1. </span>Verification<a class="headerlink" href="#verification" title="Permalink to this headline"></a>
</h2>
<section id="reference-comparison">
<span id="id10"></span><h3>
<span class="section-number">6.1.1. </span>Reference Comparison<a class="headerlink" href="#reference-comparison" title="Permalink to this headline"></a>
</h3>
<p>A key aspect of correctness verification for modifications to any 
existing program is to establish some mechanism whereby previous 
known-good reference outputs from representative inputs can be compared 
to new results. After each change is made, ensure that the results match
 using whatever criteria apply to the particular algorithm. Some will 
expect bitwise identical results, which is not always possible, 
especially where floating-point arithmetic is concerned; see <a class="reference internal" href="#numerical-accuracy-and-precision"><span class="std std-ref">Numerical Accuracy and Precision</span></a>
 regarding numerical accuracy. For other algorithms, implementations may
 be considered correct if they match the reference within some small 
epsilon.</p>
<p>Note that the process used for validating numerical results can 
easily be extended to validate performance results as well. We want to 
ensure that each change we make is correct <em>and</em> that it improves
 performance (and by how much). Checking these things frequently as an 
integral part of our cyclical APOD process will help ensure that we 
achieve the desired results as rapidly as possible.</p>
</section>
<section id="unit-testing">
<span id="id11"></span><h3>
<span class="section-number">6.1.2. </span>Unit Testing<a class="headerlink" href="#unit-testing" title="Permalink to this headline"></a>
</h3>
<p>A useful counterpart to the reference comparisons described above is 
to structure the code itself in such a way that is readily verifiable at
 the unit level. For example, we can write our CUDA kernels as a 
collection of many short <code class="docutils literal notranslate"><span class="pre">__device__</span></code> functions rather than one large monolithic <code class="docutils literal notranslate"><span class="pre">__global__</span></code> function; each device function can be tested independently before hooking them all together.</p>
<p>For example, many kernels have complex addressing logic for accessing
 memory in addition to their actual computation. If we validate our 
addressing logic separately prior to introducing the bulk of the 
computation, then this will simplify any later debugging efforts. (Note 
that the CUDA compiler considers any device code that does not 
contribute to a write to global memory as dead code subject to 
elimination, so we must at least write <em>something</em> out to global memory as a result of our addressing logic in order to successfully apply this strategy.)</p>
<p>Going a step further, if most functions are defined as <code class="docutils literal notranslate"><span class="pre">__host__</span> <span class="pre">__device__</span></code> rather than just <code class="docutils literal notranslate"><span class="pre">__device__</span></code>
 functions, then these functions can be tested on both the CPU and the 
GPU, thereby increasing our confidence that the function is correct and 
that there will not be any unexpected differences in the results. If 
there <em>are</em> differences, then those differences will be seen early and can be understood in the context of a simple function.</p>
<p>As a useful side effect, this strategy will allow us a means to 
reduce code duplication should we wish to include both CPU and GPU 
execution paths in our application: if the bulk of the work of our CUDA 
kernels is done in <code class="docutils literal notranslate"><span class="pre">__host__</span> <span class="pre">__device__</span></code> functions, we can easily call those functions from both the host code <em>and</em> the device code without duplication.</p>
</section>
</section>
<section id="debugging">
<span id="id12"></span><h2>
<span class="section-number">6.2. </span>Debugging<a class="headerlink" href="#debugging" title="Permalink to this headline"></a>
</h2>
<p>CUDA-GDB is a port of the GNU Debugger that runs on Linux and Mac; see: <a class="reference external" href="https://developer.nvidia.com/cuda-gdb">https://developer.nvidia.com/cuda-gdb</a>.</p>
<p>The NVIDIA Nsight Visual Studio Edition is available as a free plugin for Microsoft Visual Studio; see: <a class="reference external" href="https://developer.nvidia.com/nsight-visual-studio-edition">https://developer.nvidia.com/nsight-visual-studio-edition</a>.</p>
<p>Several third-party debuggers support CUDA debugging as well; see: <a class="reference external" href="https://developer.nvidia.com/debugging-solutions">https://developer.nvidia.com/debugging-solutions</a> for more details.</p>
</section>
<section id="numerical-accuracy-and-precision">
<span id="id13"></span><h2>
<span class="section-number">6.3. </span>Numerical Accuracy and Precision<a class="headerlink" href="#numerical-accuracy-and-precision" title="Permalink to this headline"></a>
</h2>
<p>Incorrect or unexpected results arise principally from issues of 
floating-point accuracy due to the way floating-point values are 
computed and stored. The following sections explain the principal items 
of interest. Other peculiarities of floating-point arithmetic are 
presented in Features and Technical Specifications of the CUDA C++ 
Programming Guide as well as in a whitepaper and accompanying webinar on
 floating-point precision and performance available from <a class="reference external" href="https://developer.nvidia.com/content/precision-performance-floating-point-and-ieee-754-compliance-nvidia-gpus">https://developer.nvidia.com/content/precision-performance-floating-point-and-ieee-754-compliance-nvidia-gpus</a>.</p>
<section id="single-vs-double-precision">
<span id="id14"></span><h3>
<span class="section-number">6.3.1. </span>Single vs. Double Precision<a class="headerlink" href="#single-vs-double-precision" title="Permalink to this headline"></a>
</h3>
<p>Devices of <a class="reference internal" href="#cuda-compute-capability"><span class="std std-ref">CUDA Compute Capability</span></a>
 1.3 and higher provide native support for double-precision 
floating-point values (that is, values 64 bits wide). Results obtained 
using double-precision arithmetic will frequently differ from the same 
operation performed via single-precision arithmetic due to the greater 
precision of the former and due to rounding issues. Therefore, it is 
important to be sure to compare values of like precision and to express 
the results within a certain tolerance rather than expecting them to be 
exact.</p>
</section>
<section id="floating-point-math-is-not-associative">
<span id="id15"></span><h3>
<span class="section-number">6.3.2. </span>Floating Point Math Is Not Associative<a class="headerlink" href="#floating-point-math-is-not-associative" title="Permalink to this headline"></a>
</h3>
<p>Each floating-point arithmetic operation involves a certain amount of
 rounding. Consequently, the order in which arithmetic operations are 
performed is important. If A, B, and C are floating-point values, 
(A+B)+C is not guaranteed to equal A+(B+C) as it is in symbolic math. 
When you parallelize computations, you potentially change the order of 
operations and therefore the parallel results might not match sequential
 results. This limitation is not specific to CUDA, but an inherent part 
of parallel computation on floating-point values.</p>
</section>
<section id="ieee-754-compliance">
<span id="id16"></span><h3>
<span class="section-number">6.3.3. </span>IEEE 754 Compliance<a class="headerlink" href="#ieee-754-compliance" title="Permalink to this headline"></a>
</h3>
<p>All CUDA compute devices follow the IEEE 754 standard for binary 
floating-point representation, with some small exceptions. These 
exceptions, which are detailed in Features and Technical Specifications 
of the CUDA C++ Programming Guide, can lead to results that differ from 
IEEE 754 values computed on the host system.</p>
<p>One of the key differences is the fused multiply-add (FMA) 
instruction, which combines multiply-add operations into a single 
instruction execution. Its result will often differ slightly from 
results obtained by doing the two operations separately.</p>
</section>
<section id="x86-80-bit-computations">
<span id="id17"></span><h3>
<span class="section-number">6.3.4. </span>x86 80-bit Computations<a class="headerlink" href="#x86-80-bit-computations" title="Permalink to this headline"></a>
</h3>
<p>x86 processors can use an 80-bit <em>double extended precision</em> 
math when performing floating-point calculations. The results of these 
calculations can frequently differ from pure 64-bit operations performed
 on the CUDA device. To get a closer match between values, set the x86 
host processor to use regular double or single precision (64 bits and 32
 bits, respectively). This is done with the <code class="docutils literal notranslate"><span class="pre">FLDCW</span></code> x86 assembly instruction or the equivalent operating system API.</p>
</section>
</section>
</section>
<section id="optimizing-cuda-applications">
<h1>
<span class="section-number">7. </span>Optimizing CUDA Applications<a class="headerlink" href="#optimizing-cuda-applications" title="Permalink to this headline"></a>
</h1>
<p>After each round of application parallelization is complete, the 
developer can move to optimizing the implementation to improve 
performance. Since there are many possible optimizations that can be 
considered, having a good understanding of the needs of the application 
can help to make the process as smooth as possible. However, as with 
APOD as a whole, program optimization is an iterative process (identify 
an opportunity for optimization, apply and test the optimization, verify
 the speedup achieved, and repeat), meaning that it is not necessary for
 a programmer to spend large amounts of time memorizing the bulk of all 
possible optimization strategies prior to seeing good speedups. Instead,
 strategies can be applied incrementally as they are learned.</p>
<p>Optimizations can be applied at various levels, from overlapping data
 transfers with computation all the way down to fine-tuning 
floating-point operation sequences. The available profiling tools are 
invaluable for guiding this process, as they can help suggest a 
next-best course of action for the developer’s optimization efforts and 
provide references into the relevant portions of the optimization 
section of this guide.</p>
</section>
<section id="performance-metrics">
<span id="id18"></span><h1>
<span class="section-number">8. </span>Performance Metrics<a class="headerlink" href="#performance-metrics" title="Permalink to this headline"></a>
</h1>
<p>When attempting to optimize CUDA code, it pays to know how to measure
 performance accurately and to understand the role that bandwidth plays 
in performance measurement. This chapter discusses how to correctly 
measure performance using CPU timers and CUDA events. It then explores 
how bandwidth affects performance metrics and how to mitigate some of 
the challenges it poses.</p>
<section id="timing">
<span id="id19"></span><h2>
<span class="section-number">8.1. </span>Timing<a class="headerlink" href="#timing" title="Permalink to this headline"></a>
</h2>
<p>CUDA calls and kernel executions can be timed using either CPU or GPU
 timers. This section examines the functionality, advantages, and 
pitfalls of both approaches.</p>
<section id="using-cpu-timers">
<span id="id20"></span><h3>
<span class="section-number">8.1.1. </span>Using CPU Timers<a class="headerlink" href="#using-cpu-timers" title="Permalink to this headline"></a>
</h3>
<p>Any CPU timer can be used to measure the elapsed time of a CUDA call 
or kernel execution. The details of various CPU timing approaches are 
outside the scope of this document, but developers should always be 
aware of the resolution their timing calls provide.</p>
<p>When using CPU timers, it is critical to remember that many CUDA API 
functions are asynchronous; that is, they return control back to the 
calling CPU thread prior to completing their work. All kernel launches 
are asynchronous, as are memory-copy functions with the <code class="docutils literal notranslate"><span class="pre">Async</span></code>
 suffix on their names. Therefore, to accurately measure the elapsed 
time for a particular call or sequence of CUDA calls, it is necessary to
 synchronize the CPU thread with the GPU by calling <code class="docutils literal notranslate"><span class="pre">cudaDeviceSynchronize()</span></code> immediately before starting and stopping the CPU timer. <code class="docutils literal notranslate"><span class="pre">cudaDeviceSynchronize()</span></code>blocks the calling CPU thread until all CUDA calls previously issued by the thread are completed.</p>
<p>Although it is also possible to synchronize the CPU thread with a 
particular stream or event on the GPU, these synchronization functions 
are not suitable for timing code in streams other than the default 
stream. <code class="docutils literal notranslate"><span class="pre">cudaStreamSynchronize()</span></code> blocks the CPU thread until all CUDA calls previously issued into the given stream have completed. <code class="docutils literal notranslate"><span class="pre">cudaEventSynchronize()</span></code>
 blocks until a given event in a particular stream has been recorded by 
the GPU. Because the driver may interleave execution of CUDA calls from 
other non-default streams, calls in other streams may be included in the
 timing.</p>
<p>Because the default stream, stream 0, exhibits serializing behavior 
for work on the device (an operation in the default stream can begin 
only after all preceding calls in any stream have completed; and no 
subsequent operation in any stream can begin until it finishes), these 
functions can be used reliably for timing in the default stream.</p>
<p>Be aware that CPU-to-GPU synchronization points such as those 
mentioned in this section imply a stall in the GPU’s processing pipeline
 and should thus be used sparingly to minimize their performance impact.</p>
</section>
<section id="using-cuda-gpu-timers">
<span id="id21"></span><h3>
<span class="section-number">8.1.2. </span>Using CUDA GPU Timers<a class="headerlink" href="#using-cuda-gpu-timers" title="Permalink to this headline"></a>
</h3>
<p>The CUDA event API provides calls that create and destroy events, 
record events (including a timestamp), and convert timestamp differences
 into a floating-point value in milliseconds. <a class="reference internal" href="#how-to-time-code-using-cuda-events-figure"><span class="std std-ref">How to time code using CUDA events</span></a> illustrates their use.</p>
<p class="title sectiontitle rubric" id="how-to-time-code-using-cuda-events-figure">How to time code using CUDA events</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell1"><span></span><span class="n">cudaEvent_t</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="n">stop</span><span class="p">;</span><span class="w"></span>
<span class="kt">float</span><span class="w"> </span><span class="n">time</span><span class="p">;</span><span class="w"></span>

<span class="n">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">start</span><span class="p">);</span><span class="w"></span>
<span class="n">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stop</span><span class="p">);</span><span class="w"></span>

<span class="n">cudaEventRecord</span><span class="p">(</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">);</span><span class="w"></span>
<span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="n">threads</span><span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="n">d_odata</span><span class="p">,</span><span class="w"> </span><span class="n">d_idata</span><span class="p">,</span><span class="w"> </span><span class="n">size_x</span><span class="p">,</span><span class="w"> </span><span class="n">size_y</span><span class="p">,</span><span class="w"></span>
<span class="w">                           </span><span class="n">NUM_REPS</span><span class="p">);</span><span class="w"></span>
<span class="n">cudaEventRecord</span><span class="p">(</span><span class="w"> </span><span class="n">stop</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">);</span><span class="w"></span>
<span class="n">cudaEventSynchronize</span><span class="p">(</span><span class="w"> </span><span class="n">stop</span><span class="w"> </span><span class="p">);</span><span class="w"></span>

<span class="n">cudaEventElapsedTime</span><span class="p">(</span><span class="w"> </span><span class="o">&amp;</span><span class="n">time</span><span class="p">,</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="n">stop</span><span class="w"> </span><span class="p">);</span><span class="w"></span>
<span class="n">cudaEventDestroy</span><span class="p">(</span><span class="w"> </span><span class="n">start</span><span class="w"> </span><span class="p">);</span><span class="w"></span>
<span class="n">cudaEventDestroy</span><span class="p">(</span><span class="w"> </span><span class="n">stop</span><span class="w"> </span><span class="p">);</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell1">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>Here <code class="docutils literal notranslate"><span class="pre">cudaEventRecord()</span></code> is used to place the <code class="docutils literal notranslate"><span class="pre">start</span></code> and <code class="docutils literal notranslate"><span class="pre">stop</span></code>
 events into the default stream, stream 0. The device will record a 
timestamp for the event when it reaches that event in the stream. The <code class="docutils literal notranslate"><span class="pre">cudaEventElapsedTime()</span></code> function returns the time elapsed between the recording of the <code class="docutils literal notranslate"><span class="pre">start</span></code> and <code class="docutils literal notranslate"><span class="pre">stop</span></code>
 events. This value is expressed in milliseconds and has a resolution of
 approximately half a microsecond. Like the other calls in this listing,
 their specific operation, parameters, and return values are described 
in the <em>CUDA Toolkit Reference Manual</em>. Note that the timings are measured on the GPU clock, so the timing resolution is operating-system-independent.</p>
</section>
</section>
<section id="bandwidth">
<span id="id22"></span><h2>
<span class="section-number">8.2. </span>Bandwidth<a class="headerlink" href="#bandwidth" title="Permalink to this headline"></a>
</h2>
<p>Bandwidth - the rate at which data can be transferred - is one of the
 most important gating factors for performance. Almost all changes to 
code should be made in the context of how they affect bandwidth. As 
described in <a class="reference internal" href="#memory-optimizations"><span class="std std-ref">Memory Optimizations</span></a>
 of this guide, bandwidth can be dramatically affected by the choice of 
memory in which data is stored, how the data is laid out and the order 
in which it is accessed, as well as other factors.</p>
<p>To measure performance accurately, it is useful to calculate 
theoretical and effective bandwidth. When the latter is much lower than 
the former, design or implementation details are likely to reduce 
bandwidth, and it should be the primary goal of subsequent optimization 
efforts to increase it.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>High Priority:</strong> Use the effective bandwidth of your computation as a metric when measuring performance and optimization benefits.</p>
</div>
<section id="theoretical-bandwidth-calculation">
<span id="id23"></span><h3>
<span class="section-number">8.2.1. </span>Theoretical Bandwidth Calculation<a class="headerlink" href="#theoretical-bandwidth-calculation" title="Permalink to this headline"></a>
</h3>
<p>Theoretical bandwidth can be calculated using hardware specifications
 available in the product literature. For example, the NVIDIA Tesla V100
 uses HBM2 (double data rate) RAM with a memory clock rate of 877 MHz 
and a 4096-bit-wide memory interface.</p>
<p>Using these data items, the peak theoretical memory bandwidth of the NVIDIA Tesla V100 is 898 GB/s:</p>
<p><span class="math notranslate nohighlight"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 119.5%; position: relative;" tabindex="0" ctxtmenu_counter="3"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n" style="vertical-align: 0.25em;"></mjx-mo><mjx-mrow><mjx-mo class="mjx-sop"><mjx-c class="mjx-c28 TEX-S1"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c38"></mjx-c><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c37"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-msup space="3"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-script style="vertical-align: 0.393em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c39"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n" style="vertical-align: 0.25em;"></mjx-mo></mjx-mrow><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c39"></mjx-c><mjx-c class="mjx-c36"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mn class="mjx-n"><mjx-c class="mjx-c38"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-sop"><mjx-c class="mjx-c29 TEX-S1"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cF7"></mjx-c></mjx-mo><mjx-msup space="3"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-script style="vertical-align: 0.393em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c39"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c38"></mjx-c><mjx-c class="mjx-c39"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn><mjx-mtext class="mjx-n"><mjx-c class="mjx-c47"></mjx-c><mjx-c class="mjx-c42"></mjx-c><mjx-c class="mjx-c2F"></mjx-c><mjx-c class="mjx-c73"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN" fence="true" stretchy="true" symmetric="true"></mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mn>0.877</mn><mo>×</mo><msup><mn>10</mn><mrow data-mjx-texclass="ORD"><mn>9</mn></mrow></msup><mo data-mjx-texclass="CLOSE" fence="true" stretchy="true" symmetric="true"></mo></mrow><mo>×</mo><mo stretchy="false">(</mo><mn>4096</mn><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mn>8</mn><mo stretchy="false">)</mo><mo>×</mo><mn>2</mn><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>÷</mo><msup><mn>10</mn><mrow data-mjx-texclass="ORD"><mn>9</mn></mrow></msup><mo>=</mo><mn>898</mn><mtext>GB/s</mtext></math></mjx-assistive-mml></mjx-container></span></p>
<p>In this calculation, the memory clock rate is converted in to Hz, 
multiplied by the interface width (divided by 8, to convert bits to 
bytes) and multiplied by 2 due to the double data rate. Finally, this 
product is divided by 10<sup>9</sup> to convert the result to GB/s.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some calculations use 1024<sup>3</sup> instead of 10<sup>9</sup> for 
the final calculation. In such a case, the bandwidth would be 836.4 
GiB/s. It is important to use the same divisor when calculating 
theoretical and effective bandwidth so that the comparison is valid.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>On GPUs with GDDR memory with ECC enabled the available DRAM is 
reduced by 6.25% to allow for the storage of ECC bits. Fetching ECC bits
 for each memory transaction also reduced the effective bandwidth by 
approximately 20% compared to the same GPU with ECC disabled, though the
 exact impact of ECC on bandwidth can be higher and depends on the 
memory access pattern. HBM2 memories, on the other hand, provide 
dedicated ECC resources, allowing overhead-free ECC protection.<a class="footnote-reference brackets" href="#fn2" id="id24">2</a></p>
</div>
</section>
<section id="effective-bandwidth-calculation">
<span id="id25"></span><h3>
<span class="section-number">8.2.2. </span>Effective Bandwidth Calculation<a class="headerlink" href="#effective-bandwidth-calculation" title="Permalink to this headline"></a>
</h3>
<p>Effective bandwidth is calculated by timing specific program 
activities and by knowing how data is accessed by the program. To do so,
 use this equation:</p>
<p><span class="math notranslate nohighlight"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 119.5%; position: relative;" tabindex="0" ctxtmenu_counter="4"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c45"></mjx-c><mjx-c class="mjx-c66"></mjx-c><mjx-c class="mjx-c66"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c63"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c76"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c5C"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c62"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c64"></mjx-c><mjx-c class="mjx-c77"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c64"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c68"></mjx-c></mjx-mtext><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mrow space="4"><mjx-mo class="mjx-sop"><mjx-c class="mjx-c28 TEX-S1"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D435 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-msub space="3"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D435 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D464 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cF7"></mjx-c></mjx-mo><mjx-msup space="3"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-script style="vertical-align: 0.393em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c39"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-texatom><mjx-mo class="mjx-sop"><mjx-c class="mjx-c29 TEX-S1"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cF7"></mjx-c></mjx-mo><mjx-mtext class="mjx-n" space="3"><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c6D"></mjx-c><mjx-c class="mjx-c65"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>Effective\ bandwidth</mtext><mo>=</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><msub><mi>B</mi><mrow data-mjx-texclass="ORD"><mi>r</mi></mrow></msub><mo>+</mo><msub><mi>B</mi><mrow data-mjx-texclass="ORD"><mi>w</mi></mrow></msub><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>÷</mo><msup><mn>10</mn><mrow data-mjx-texclass="ORD"><mn>9</mn></mrow></msup></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>÷</mo><mtext>time</mtext></math></mjx-assistive-mml></mjx-container></span></p>
<p>Here, the effective bandwidth is in units of GB/s, B<sub>r</sub> is the number of bytes read per kernel, B<sub>w</sub> is the number of bytes written per kernel, and time is given in seconds.</p>
<p>For example, to compute the effective bandwidth of a 2048 x 2048 matrix copy, the following formula could be used:</p>
<p><span class="math notranslate nohighlight"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 119.5%; position: relative;" tabindex="0" ctxtmenu_counter="5"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c45"></mjx-c><mjx-c class="mjx-c66"></mjx-c><mjx-c class="mjx-c66"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c63"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c76"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c5C"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c62"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c64"></mjx-c><mjx-c class="mjx-c77"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c64"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c68"></mjx-c></mjx-mtext><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mrow space="4"><mjx-mo class="mjx-sop"><mjx-c class="mjx-c28 TEX-S1"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mrow><mjx-mo class="mjx-sop"><mjx-c class="mjx-c28 TEX-S1"></mjx-c></mjx-mo><mjx-msup><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn><mjx-script style="vertical-align: 0.404em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c34"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-sop"><mjx-c class="mjx-c29 TEX-S1"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cF7"></mjx-c></mjx-mo><mjx-msup space="3"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-script style="vertical-align: 0.393em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c39"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-texatom><mjx-mo class="mjx-sop"><mjx-c class="mjx-c29 TEX-S1"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cF7"></mjx-c></mjx-mo><mjx-mtext class="mjx-n" space="3"><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c6D"></mjx-c><mjx-c class="mjx-c65"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>Effective\ bandwidth</mtext><mo>=</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><msup><mn>2048</mn><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msup><mo>×</mo><mn>4</mn><mo>×</mo><mn>2</mn><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>÷</mo><msup><mn>10</mn><mrow data-mjx-texclass="ORD"><mn>9</mn></mrow></msup></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>÷</mo><mtext>time</mtext></math></mjx-assistive-mml></mjx-container></span></p>
<p>The number of elements is multiplied by the size of each element (4 bytes for a float), multiplied by 2 (because of the read <em>and</em> write), divided by 10<sup>9</sup> (or 1,024<sup>3</sup>) to obtain GB of memory transferred. This number is divided by the time in seconds to obtain GB/s.</p>
</section>
<section id="throughput-reported-by-visual-profiler">
<span id="id26"></span><h3>
<span class="section-number">8.2.3. </span>Throughput Reported by Visual Profiler<a class="headerlink" href="#throughput-reported-by-visual-profiler" title="Permalink to this headline"></a>
</h3>
<p>For devices with <a class="reference internal" href="#cuda-compute-capability"><span class="std std-ref">compute capability</span></a>
 of 2.0 or greater, the Visual Profiler can be used to collect several 
different memory throughput measures. The following throughput metrics 
can be displayed in the Details or Detail Graphs view:</p>
<ul class="simple">
<li><p>Requested Global Load Throughput</p></li>
<li><p>Requested Global Store Throughput</p></li>
<li><p>Global Load Throughput</p></li>
<li><p>Global Store Throughput</p></li>
<li><p>DRAM Read Throughput</p></li>
<li><p>DRAM Write Throughput</p></li>
</ul>
<p>The Requested Global Load Throughput and Requested Global Store 
Throughput values indicate the global memory throughput requested by the
 kernel and therefore correspond to the effective bandwidth obtained by 
the calculation shown under <a class="reference internal" href="#effective-bandwidth-calculation"><span class="std std-ref">Effective Bandwidth Calculation</span></a>.</p>
<p>Because the minimum memory transaction size is larger than most word 
sizes, the actual memory throughput required for a kernel can include 
the transfer of data not used by the kernel. For global memory accesses,
 this actual throughput is reported by the Global Load Throughput and 
Global Store Throughput values.</p>
<p>It’s important to note that both numbers are useful. The actual 
memory throughput shows how close the code is to the hardware limit, and
 a comparison of the effective or requested bandwidth to the actual 
bandwidth presents a good estimate of how much bandwidth is wasted by 
suboptimal coalescing of memory accesses (see <a class="reference internal" href="#coalesced-access-to-global-memory"><span class="std std-ref">Coalesced Access to Global Memory</span></a>).
 For global memory accesses, this comparison of requested memory 
bandwidth to actual memory bandwidth is reported by the Global Memory 
Load Efficiency and Global Memory Store Efficiency metrics.</p>
<dl class="footnote brackets">
<dt class="label" id="fn2"><span class="brackets"><a class="fn-backref" href="#id24">2</a></span></dt>
<dd>
<p>As an exception, scattered writes to HBM2 see some overhead from ECC 
but much less than the overhead with similar access patterns on 
ECC-protected GDDR5 memory.</p>
</dd>
</dl>
</section>
</section>
</section>
<section id="memory-optimizations">
<span id="id27"></span><h1>
<span class="section-number">9. </span>Memory Optimizations<a class="headerlink" href="#memory-optimizations" title="Permalink to this headline"></a>
</h1>
<p>Memory optimizations are the most important area for performance. The
 goal is to maximize the use of the hardware by maximizing bandwidth. 
Bandwidth is best served by using as much fast memory and as little 
slow-access memory as possible. This chapter discusses the various kinds
 of memory on the host and device and how best to set up data items to 
use the memory effectively.</p>
<section id="data-transfer-between-host-and-device">
<span id="id28"></span><h2>
<span class="section-number">9.1. </span>Data Transfer Between Host and Device<a class="headerlink" href="#data-transfer-between-host-and-device" title="Permalink to this headline"></a>
</h2>
<p>The peak theoretical bandwidth between the device memory and the GPU 
is much higher (898 GB/s on the NVIDIA Tesla V100, for example) than the
 peak theoretical bandwidth between host memory and device memory (16 
GB/s on the PCIe x16 Gen3). Hence, for best overall application 
performance, it is important to minimize data transfer between the host 
and the device, even if that means running kernels on the GPU that do 
not demonstrate any speedup compared with running them on the host CPU.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>High Priority:</strong> Minimize data transfer between the 
host and the device, even if it means running some kernels on the device
 that do not show performance gains when compared with running them on 
the host CPU.</p>
</div>
<p>Intermediate data structures should be created in device memory, 
operated on by the device, and destroyed without ever being mapped by 
the host or copied to host memory.</p>
<p>Also, because of the overhead associated with each transfer, batching
 many small transfers into one larger transfer performs significantly 
better than making each transfer separately, even if doing so requires 
packing non-contiguous regions of memory into a contiguous buffer and 
then unpacking after the transfer.</p>
<p>Finally, higher bandwidth between the host and the device is achieved when using <em>page-locked</em> (or <em>pinned</em>) memory, as discussed in the CUDA C++ Programming Guide and the <a class="reference internal" href="#pinned-memory"><span class="std std-ref">Pinned Memory</span></a> section of this document.</p>
<section id="pinned-memory">
<span id="id29"></span><h3>
<span class="section-number">9.1.1. </span>Pinned Memory<a class="headerlink" href="#pinned-memory" title="Permalink to this headline"></a>
</h3>
<p>Page-locked or pinned memory transfers attain the highest bandwidth 
between the host and the device. On PCIe x16 Gen3 cards, for example, 
pinned memory can attain roughly 12 GB/s transfer rates.</p>
<p>Pinned memory is allocated using the <code class="docutils literal notranslate"><span class="pre">cudaHostAlloc()</span></code> functions in the Runtime API. The <code class="docutils literal notranslate"><span class="pre">bandwidthTest</span></code> CUDA Sample shows how to use these functions as well as how to measure memory transfer performance.</p>
<p>For regions of system memory that have already been pre-allocated, <code class="docutils literal notranslate"><span class="pre">cudaHostRegister()</span></code> can be used to pin the memory on-the-fly without the need to allocate a separate buffer and copy the data into it.</p>
<p>Pinned memory should not be overused. Excessive use can reduce 
overall system performance because pinned memory is a scarce resource, 
but how much is too much is difficult to know in advance. Furthermore, 
the pinning of system memory is a heavyweight operation compared to most
 normal system memory allocations, so as with all optimizations, test 
the application and the systems it runs on for optimal performance 
parameters.</p>
</section>
<section id="asynchronous-and-overlapping-transfers-with-computation">
<span id="asynchronous-transfers-and-overlapping-transfers-with-computation"></span><h3>
<span class="section-number">9.1.2. </span>Asynchronous and Overlapping Transfers with Computation<a class="headerlink" href="#asynchronous-and-overlapping-transfers-with-computation" title="Permalink to this headline"></a>
</h3>
<p>Data transfers between the host and the device using <code class="docutils literal notranslate"><span class="pre">cudaMemcpy()</span></code> are blocking transfers; that is, control is returned to the host thread only after the data transfer is complete. The <code class="docutils literal notranslate"><span class="pre">cudaMemcpyAsync()</span></code> function is a non-blocking variant of <code class="docutils literal notranslate"><span class="pre">cudaMemcpy()</span></code> in which control is returned immediately to the host thread. In contrast with <code class="docutils literal notranslate"><span class="pre">cudaMemcpy()</span></code>, the asynchronous transfer version <em>requires</em> pinned host memory (see <a class="reference internal" href="#pinned-memory"><span class="std std-ref">Pinned Memory</span></a>), and it contains an additional argument, a stream ID. A <em>stream</em>
 is simply a sequence of operations that are performed in order on the 
device. Operations in different streams can be interleaved and in some 
cases overlapped - a property that can be used to hide data transfers 
between the host and the device.</p>
<p>Asynchronous transfers enable overlap of data transfers with 
computation in two different ways. On all CUDA-enabled devices, it is 
possible to overlap host computation with asynchronous data transfers 
and with device computations. For example, <a class="reference internal" href="#asynchronous-transfers-and-overlapping-transfers-with-computation"><span class="std std-ref">Asynchronous and Overlapping Transfers with Computation</span></a> demonstrates how host computation in the routine <code class="docutils literal notranslate"><span class="pre">cpuFunction()</span></code> is performed while data is transferred to the device and a kernel using the device is executed.</p>
<p class="title sectiontitle rubric" id="overlapping-computation-and-data-transfers">Overlapping computation and data transfers</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell2"><span></span><span class="n">cudaMemcpyAsync</span><span class="p">(</span><span class="n">a_d</span><span class="p">,</span><span class="w"> </span><span class="n">a_h</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemcpyHostToDevice</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">a_d</span><span class="p">);</span><span class="w"></span>
<span class="n">cpuFunction</span><span class="p">();</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell2">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>The last argument to the <code class="docutils literal notranslate"><span class="pre">cudaMemcpyAsync()</span></code>
 function is the stream ID, which in this case uses the default stream, 
stream 0. The kernel also uses the default stream, and it will not begin
 execution until the memory copy completes; therefore, no explicit 
synchronization is needed. Because the memory copy and the kernel both 
return control to the host immediately, the host function <code class="docutils literal notranslate"><span class="pre">cpuFunction()</span></code> overlaps their execution.</p>
<p>In <a class="reference internal" href="#asynchronous-transfers-and-overlapping-transfers-with-computation"><span class="std std-ref">Asynchronous and Overlapping Transfers with Computation</span></a>,
 the memory copy and kernel execution occur sequentially. On devices 
that are capable of concurrent copy and compute, it is possible to 
overlap kernel execution on the device with data transfers between the 
host and the device. Whether a device has this capability is indicated 
by the <code class="docutils literal notranslate"><span class="pre">asyncEngineCount</span></code> field of the <code class="docutils literal notranslate"><span class="pre">cudaDeviceProp</span></code> structure (or listed in the output of the <code class="docutils literal notranslate"><span class="pre">deviceQuery</span></code>
 CUDA Sample). On devices that have this capability, the overlap once 
again requires pinned host memory, and, in addition, the data transfer 
and kernel must use different, non-default streams (streams with 
non-zero stream IDs). Non-default streams are required for this overlap 
because memory copy, memory set functions, and kernel calls that use the
 default stream begin only after all preceding calls on the device (in 
any stream) have completed, and no operation on the device (in any 
stream) commences until they are finished.</p>
<p><a class="reference internal" href="#asynchronous-transfers-and-overlapping-transfers-with-computation"><span class="std std-ref">Asynchronous and Overlapping Transfers with Computation</span></a> illustrates the basic technique.</p>
<p class="title sectiontitle rubric" id="concurrent-copy-and-execute">Concurrent copy and execute</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell3"><span></span><span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream1</span><span class="p">);</span><span class="w"></span>
<span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream2</span><span class="p">);</span><span class="w"></span>
<span class="n">cudaMemcpyAsync</span><span class="p">(</span><span class="n">a_d</span><span class="p">,</span><span class="w"> </span><span class="n">a_h</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemcpyHostToDevice</span><span class="p">,</span><span class="w"> </span><span class="n">stream1</span><span class="p">);</span><span class="w"></span>
<span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream2</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">otherData_d</span><span class="p">);</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell3">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>In this code, two streams are created and used in the data transfer 
and kernel executions as specified in the last arguments of the <code class="docutils literal notranslate"><span class="pre">cudaMemcpyAsync</span></code> call and the kernel’s execution configuration.</p>
<p><a class="reference internal" href="#asynchronous-transfers-and-overlapping-transfers-with-computation"><span class="std std-ref">Asynchronous and Overlapping Transfers with Computation</span></a>
 demonstrates how to overlap kernel execution with asynchronous data 
transfer. This technique could be used when the data dependency is such 
that the data can be broken into chunks and transferred in multiple 
stages, launching multiple kernels to operate on each chunk as it 
arrives. <a class="reference internal" href="#sequential-copy-and-execute"><span class="std std-ref">Sequential copy and execute</span></a> and <a class="reference internal" href="#staged-concurrent-copy-and-execute"><span class="std std-ref">Staged concurrent copy and execute</span></a>
 demonstrate this. They produce equivalent results. The first segment 
shows the reference sequential implementation, which transfers and 
operates on an array of <em>N</em> floats (where <em>N</em> is assumed to be evenly divisible by nThreads).</p>
<p class="title sectiontitle rubric" id="sequential-copy-and-execute">Sequential copy and execute</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell4"><span></span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">a_d</span><span class="p">,</span><span class="w"> </span><span class="n">a_h</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span><span class="w"> </span><span class="n">dir</span><span class="p">);</span><span class="w"></span>
<span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">N</span><span class="o">/</span><span class="n">nThreads</span><span class="p">,</span><span class="w"> </span><span class="n">nThreads</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">a_d</span><span class="p">);</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell4">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p><a class="reference internal" href="#staged-concurrent-copy-and-execute"><span class="std std-ref">Staged concurrent copy and execute</span></a>
 shows how the transfer and kernel execution can be broken up into 
nStreams stages. This approach permits some overlapping of the data 
transfer and execution.</p>
<p class="title sectiontitle rubric" id="staged-concurrent-copy-and-execute">Staged concurrent copy and execute</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell5"><span></span><span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="o">/</span><span class="n">nStreams</span><span class="p">;</span><span class="w"></span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">&lt;</span><span class="n">nStreams</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">offset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="o">*</span><span class="n">N</span><span class="o">/</span><span class="n">nStreams</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">cudaMemcpyAsync</span><span class="p">(</span><span class="n">a_d</span><span class="o">+</span><span class="n">offset</span><span class="p">,</span><span class="w"> </span><span class="n">a_h</span><span class="o">+</span><span class="n">offset</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">dir</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span><span class="w"></span>
<span class="w">    </span><span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">N</span><span class="o">/</span><span class="p">(</span><span class="n">nThreads</span><span class="o">*</span><span class="n">nStreams</span><span class="p">),</span><span class="w"> </span><span class="n">nThreads</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"></span>
<span class="w">             </span><span class="n">stream</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">a_d</span><span class="o">+</span><span class="n">offset</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell5">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>(In <a class="reference internal" href="#staged-concurrent-copy-and-execute"><span class="std std-ref">Staged concurrent copy and execute</span></a>, it is assumed that <em>N</em> is evenly divisible by <code class="docutils literal notranslate"><span class="pre">nThreads*nStreams</span></code>.)
 Because execution within
a stream occurs sequentially, none of the kernels will launch until the 
data transfers in their respective streams complete. Current GPUs can
simultaneously process asynchronous data transfers and execute kernels. 
GPUs with a single copy engine can perform one asynchronous data
transfer and execute kernels whereas GPUs with two copy engines can 
simultaneously perform one asynchronous data transfer from the host to
the device, one asynchronous data transfer from the device to the host, 
and execute kernels. The number of copy engines on a GPU is given
by the <code class="docutils literal notranslate"><span class="pre">asyncEngineCount</span></code> field of the <code class="docutils literal notranslate"><span class="pre">cudaDeviceProp</span></code> structure, which is also listed in the output of the <code class="docutils literal notranslate"><span class="pre">deviceQuery</span></code>
 CUDA
Sample. (It should be mentioned that it is not possible to overlap a 
blocking transfer with an asynchronous transfer, because the blocking
transfer occurs in the default stream, so it will not begin until all 
previous CUDA calls complete. It will not allow any other CUDA call
to begin until it has completed.) A diagram depicting the timeline of 
execution for the two code segments is shown
in <a class="reference internal" href="#timeline-comparison-for-copy-and-kernel-execution-figure"><span class="std std-ref">Figure 1</span></a>, and <code class="docutils literal notranslate"><span class="pre">nStreams</span></code> is equal to 4
for <a class="reference internal" href="#staged-concurrent-copy-and-execute"><span class="std std-ref">Staged concurrent copy and execute</span></a> in the bottom half of the figure.</p>
<figure class="align-center" id="timeline-comparison-for-copy-and-kernel-execution-figure">
<img alt="Timeline comparison for copy and kernel execution" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/timeline-comparison-for-copy-and-kernel-execution.png">
<figcaption>
<p><span class="caption-number">Figure 1 </span><span class="caption-text">Timeline comparison for copy and kernel execution</span><a class="headerlink" href="#timeline-comparison-for-copy-and-kernel-execution-figure" title="Permalink to this image"></a></p>
<div class="legend">
<dl class="simple">
<dt>Top</dt>
<dd>
<p>Sequential</p>
</dd>
<dt>Bottom</dt>
<dd>
<p>Concurrent</p>
</dd>
</dl>
</div>
</figcaption>
</figure>
<p>For this example, it is assumed that the data transfer and kernel 
execution times are comparable. In such cases, and when the execution 
time (<em>tE</em>) exceeds the transfer time (<em>tT</em>), a rough estimate for the overall time is <em>tE + tT/nStreams</em> for the staged version versus <em>tE + tT</em> for the sequential version. If the transfer time exceeds the execution time, a rough estimate for the overall time is <em>tT + tE/nStreams</em>.</p>
</section>
<section id="zero-copy">
<span id="id30"></span><h3>
<span class="section-number">9.1.3. </span>Zero Copy<a class="headerlink" href="#zero-copy" title="Permalink to this headline"></a>
</h3>
<p><em>Zero copy</em> is a feature that was added in version 2.2 of the 
CUDA Toolkit. It enables GPU threads to directly access host memory. For
 this purpose, it requires mapped pinned (non-pageable) memory. On 
integrated GPUs (i.e., GPUs with the integrated field of the CUDA device
 properties structure set to 1), mapped pinned memory is always a 
performance gain because it avoids superfluous copies as integrated GPU 
and CPU memory are physically the same. On discrete GPUs, mapped pinned 
memory is advantageous only in certain cases. Because the data is not 
cached on the GPU, mapped pinned memory should be read or written only 
once, and the global loads and stores that read and write the memory 
should be coalesced. Zero copy can be used in place of streams because 
kernel-originated data transfers automatically overlap kernel execution 
without the overhead of setting up and determining the optimal number of
 streams.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Low Priority:</strong> Use zero-copy operations on integrated GPUs for CUDA Toolkit version 2.2 and later.</p>
</div>
<p>The host code in <a class="reference internal" href="#zero-copy-host-code"><span class="std std-ref">Zero-copy host code</span></a> shows how zero copy is typically set up.</p>
<p class="title sectiontitle rubric" id="zero-copy-host-code">Zero-copy host code</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell6"><span></span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">a_h</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">a_map</span><span class="p">;</span><span class="w"></span>
<span class="p">...</span><span class="w"></span>
<span class="n">cudaGetDeviceProperties</span><span class="p">(</span><span class="o">&amp;</span><span class="n">prop</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">prop</span><span class="p">.</span><span class="n">canMapHostMemory</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="n">cudaSetDeviceFlags</span><span class="p">(</span><span class="n">cudaDeviceMapHost</span><span class="p">);</span><span class="w"></span>
<span class="n">cudaHostAlloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">a_h</span><span class="p">,</span><span class="w"> </span><span class="n">nBytes</span><span class="p">,</span><span class="w"> </span><span class="n">cudaHostAllocMapped</span><span class="p">);</span><span class="w"></span>
<span class="n">cudaHostGetDevicePointer</span><span class="p">(</span><span class="o">&amp;</span><span class="n">a_map</span><span class="p">,</span><span class="w"> </span><span class="n">a_h</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">gridSize</span><span class="p">,</span><span class="w"> </span><span class="n">blockSize</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">a_map</span><span class="p">);</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell6">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>In this code, the <code class="docutils literal notranslate"><span class="pre">canMapHostMemory</span></code> field of the structure returned by <code class="docutils literal notranslate"><span class="pre">cudaGetDeviceProperties()</span></code> is used to check that the device
supports mapping host memory to the device’s address space. Page-locked memory mapping is enabled by calling <code class="docutils literal notranslate"><span class="pre">cudaSetDeviceFlags()</span></code>
with <code class="docutils literal notranslate"><span class="pre">cudaDeviceMapHost</span></code>. Note that <code class="docutils literal notranslate"><span class="pre">cudaSetDeviceFlags()</span></code> must be called prior to setting a device or making a CUDA call that
requires state (that is, essentially, before a context is created). Page-locked mapped host memory is allocated using <code class="docutils literal notranslate"><span class="pre">cudaHostAlloc()</span></code>,
and the pointer to the mapped device address space is obtained via the function <code class="docutils literal notranslate"><span class="pre">cudaHostGetDevicePointer()</span></code>. In the code
in <a class="reference internal" href="#zero-copy-host-code"><span class="std std-ref">Zero-copy host code</span></a>, <code class="docutils literal notranslate"><span class="pre">kernel()</span></code> can reference the mapped pinned host memory using the pointer <code class="docutils literal notranslate"><span class="pre">a_map</span></code> in exactly the
same was as it would if a_map referred to a location in device memory.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Mapped pinned host memory allows you to overlap CPU-GPU memory 
transfers with computation while avoiding the use of CUDA streams. But 
since any repeated access to such memory areas causes repeated CPU-GPU 
transfers, consider creating a second area in device memory to manually 
cache the previously read host memory data.</p>
</div>
</section>
<section id="unified-virtual-addressing">
<span id="id31"></span><h3>
<span class="section-number">9.1.4. </span>Unified Virtual Addressing<a class="headerlink" href="#unified-virtual-addressing" title="Permalink to this headline"></a>
</h3>
<p>Devices of <a class="reference internal" href="#cuda-compute-capability"><span class="std std-ref">compute capability</span></a> 2.0 and later support a special addressing mode called <em>Unified Virtual Addressing</em>
 (UVA) on 64-bit Linux and Windows. With UVA, the host memory and the 
device memories of all installed supported devices share a single 
virtual address space.</p>
<p>Prior to UVA, an application had to keep track of which pointers 
referred to device memory (and for which device) and which referred to 
host memory as a separate bit of metadata (or as hard-coded information 
in the program) for each pointer. Using UVA, on the other hand, the 
physical memory space to which a pointer points can be determined simply
 by inspecting the value of the pointer using <code class="docutils literal notranslate"><span class="pre">cudaPointerGetAttributes()</span></code>.</p>
<p>Under UVA, pinned host memory allocated with <code class="docutils literal notranslate"><span class="pre">cudaHostAlloc()</span></code> will have identical host and device pointers, so it is not necessary to call <code class="docutils literal notranslate"><span class="pre">cudaHostGetDevicePointer()</span></code> for such allocations. Host memory allocations pinned after-the-fact via <code class="docutils literal notranslate"><span class="pre">cudaHostRegister()</span></code>, however, will continue to have different device pointers than their host pointers, so <code class="docutils literal notranslate"><span class="pre">cudaHostGetDevicePointer()</span></code> remains necessary in that case.</p>
<p>UVA is also a necessary precondition for enabling peer-to-peer (P2P) 
transfer of data directly across the PCIe bus or NVLink for supported 
GPUs in supported configurations, bypassing host memory.</p>
<p>See the CUDA C++ Programming Guide for further explanations and software requirements for UVA and P2P.</p>
</section>
</section>
<section id="device-memory-spaces">
<span id="id32"></span><h2>
<span class="section-number">9.2. </span>Device Memory Spaces<a class="headerlink" href="#device-memory-spaces" title="Permalink to this headline"></a>
</h2>
<p>CUDA devices use several memory spaces, which have different 
characteristics that reflect their distinct usages in CUDA applications.
 These
memory spaces include global, local, shared, texture, and registers, as 
shown in <a class="reference internal" href="#memory-spaces-cuda-device-figure"><span class="std std-ref">Figure 2</span></a>.</p>
<figure class="align-center" id="memory-spaces-cuda-device-figure">
<img alt="Memory spaces on a CUDA device" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/memory-spaces-on-cuda-device.png">
<figcaption>
<p><span class="caption-number">Figure 2 </span><span class="caption-text">Memory spaces on a CUDA device</span><a class="headerlink" href="#memory-spaces-cuda-device-figure" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>Of these different memory spaces, global memory is the most 
plentiful; see Features and Technical Specifications of the CUDA C++ 
Programming Guide for the amounts of memory available in each memory 
space at each <a class="reference internal" href="#cuda-compute-capability"><span class="std std-ref">compute capability</span></a>
 level. Global, local, and texture memory have the greatest access 
latency, followed by constant memory, shared memory, and the register 
file.</p>
<p>The various principal traits of the memory types are shown in <a class="reference internal" href="#salient-features-device-memory-table"><span class="std std-ref">Table 1</span></a>.</p>
<div class="wy-table-responsive"><table class="table-no-stripes docutils align-default" id="salient-features-device-memory-table">
<caption>
<span class="caption-number">Table 1 </span><span class="caption-text">Salient Features of Device Memory</span><a class="headerlink" href="#salient-features-device-memory-table" title="Permalink to this table"></a>
</caption>
<colgroup>
<col style="width: 75%">
<col style="width: 7%">
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 7%">
<col style="width: 6%">
</colgroup>
<thead>
<tr class="row-odd">
<th class="head"><p>Memory</p></th>
<th class="head"><p>Location on/off chip</p></th>
<th class="head"><p>Cached</p></th>
<th class="head"><p>Access</p></th>
<th class="head"><p>Scope</p></th>
<th class="head"><p>Lifetime</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even">
<td><p>Register</p></td>
<td><p>On</p></td>
<td><p>n/a</p></td>
<td><p>R/W</p></td>
<td><p>1 thread</p></td>
<td><p>Thread</p></td>
</tr>
<tr class="row-odd">
<td><p>Local</p></td>
<td><p>Off</p></td>
<td><p>Yes††</p></td>
<td><p>R/W</p></td>
<td><p>1 thread</p></td>
<td><p>Thread</p></td>
</tr>
<tr class="row-even">
<td><p>Shared</p></td>
<td><p>On</p></td>
<td><p>n/a</p></td>
<td><p>R/W</p></td>
<td><p>All threads in block</p></td>
<td><p>Block</p></td>
</tr>
<tr class="row-odd">
<td><p>Global</p></td>
<td><p>Off</p></td>
<td><p>†</p></td>
<td><p>R/W</p></td>
<td><p>All threads + host</p></td>
<td><p>Host allocation</p></td>
</tr>
<tr class="row-even">
<td><p>Constant</p></td>
<td><p>Off</p></td>
<td><p>Yes</p></td>
<td><p>R</p></td>
<td><p>All threads + host</p></td>
<td><p>Host allocation</p></td>
</tr>
<tr class="row-odd">
<td><p>Texture</p></td>
<td><p>Off</p></td>
<td><p>Yes</p></td>
<td><p>R</p></td>
<td><p>All threads + host</p></td>
<td><p>Host allocation</p></td>
</tr>
<tr class="row-even">
<td><p><sup>†</sup> Cached in L1 and L2 by default on devices of compute
 capability 6.0 and 7.x; cached only in L2 by default on devices of 
lower compute capabilities, though some allow opt-in to caching in L1 as
 well via compilation flags.</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd">
<td><p><sup>††</sup> Cached in L1 and L2 by default except on devices of
 compute capability 5.x; devices of compute capability 5.x cache locals 
only in L2.</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>In the case of texture access, if a texture reference is bound to a 
linear array in global memory, then the device code can write to the 
underlying array. Texture references that are bound to CUDA arrays can 
be written to via surface-write operations by binding a surface to the 
same underlying CUDA array storage). Reading from a texture while 
writing to its underlying global memory array in the same kernel launch 
should be avoided because the texture caches are read-only and are not 
invalidated when the associated global memory is modified.</p>
<section id="coalesced-access-to-global-memory">
<span id="id33"></span><h3>
<span class="section-number">9.2.1. </span>Coalesced Access to Global Memory<a class="headerlink" href="#coalesced-access-to-global-memory" title="Permalink to this headline"></a>
</h3>
<p>A very important performance consideration in programming for 
CUDA-capable GPU architectures is the coalescing of global memory 
accesses. Global memory loads and stores by threads of a warp are 
coalesced by the device into as few as possible transactions.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>High Priority:</strong> Ensure global memory accesses are coalesced whenever possible.</p>
</div>
<p>The access requirements for coalescing depend on the compute 
capability of the device and are documented in the CUDA C++ Programming 
Guide.</p>
<p>For devices of compute capability 6.0 or higher, the requirements can
 be summarized quite easily: the concurrent accesses of the threads of a
 warp will coalesce into a number of transactions equal to the number of
 32-byte transactions necessary to service all of the threads of the 
warp.</p>
<p>For certain devices of compute capability 5.2, L1-caching of accesses
 to global memory can be optionally enabled. If L1-caching is enabled on
 these devices, the number of required transactions is equal to the 
number of required 128-byte aligned segments.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>On devices of compute capability 6.0 or higher, L1-caching is the 
default, however the data access unit is 32-byte regardless of whether 
global loads are cached in L1 or not.</p>
</div>
<p>On devices with GDDR memory, accessing memory in a coalesced way is 
even more important when ECC is turned on. Scattered accesses increase 
ECC memory transfer overhead, especially when writing data to global 
memory.</p>
<p>Coalescing concepts are illustrated in the following simple examples.
 These examples assume compute capability 6.0 or higher and that 
accesses are for 4-byte words, unless otherwise noted.</p>
<section id="a-simple-access-pattern">
<span id="simple-access-pattern"></span><h4>
<span class="section-number">9.2.1.1. </span>A Simple Access Pattern<a class="headerlink" href="#a-simple-access-pattern" title="Permalink to this headline"></a>
</h4>
<p>The first and simplest case of coalescing can be achieved by any CUDA-enabled device of compute capability 6.0 or higher: the <em>k</em>-th thread accesses the <em>k</em>-th word in a 32-byte aligned array. Not all threads need to participate.</p>
<p>For example, if the threads of a warp access adjacent 4-byte words (e.g., adjacent <code class="docutils literal notranslate"><span class="pre">float</span></code> values), four coalesced 32-byte
transactions will service that memory access. Such a pattern is shown in <cite>Figure 3 &lt;coalesced-access-figure&gt;</cite>.</p>
<figure class="align-center" id="coalesced-access-figure">
<img alt="Coalesced access" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/coalesced-access.png">
<figcaption>
<p><span class="caption-number">Figure 3 </span><span class="caption-text">Coalesced access</span><a class="headerlink" href="#coalesced-access-figure" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>This access pattern results in four 32-byte transactions, indicated by the red rectangles.</p>
<p>If from any of the four 32-byte segments only a subset of the words are requested (e.g. if several threads had accessed the
same word or if some threads did not participate in the access), the full segment is fetched anyway. Furthermore, if accesses
by the threads of the warp had been permuted within or accross the four segments, still only four 32-byte transactions would
have been performed by a device with <a class="reference internal" href="#cuda-compute-capability"><span class="std std-ref">compute capability</span></a> 6.0 or higher.</p>
</section>
<section id="a-sequential-but-misaligned-access-pattern">
<span id="sequential-but-misaligned-access-pattern"></span><h4>
<span class="section-number">9.2.1.2. </span>A Sequential but Misaligned Access Pattern<a class="headerlink" href="#a-sequential-but-misaligned-access-pattern" title="Permalink to this headline"></a>
</h4>
<p>If sequential threads in a warp access memory that is sequential but not aligned with a 32-byte segment, five 32-byte segments
will be requested, as shown in <a class="reference internal" href="#misaligned-sequential-addresses-fall-5-32-byte-l2-cache-seqments"><span class="std std-ref">Figure 4</span></a>.</p>
<figure class="align-center" id="misaligned-sequential-addresses-fall-5-32-byte-l2-cache-seqments">
<img alt="Misaligned sequential addresses that fall within five 32-byte segments" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/misaligned-sequential-addresses.png">
<figcaption>
<p><span class="caption-number">Figure 4 </span><span class="caption-text">Misaligned sequential addresses that fall within five 32-byte segments</span><a class="headerlink" href="#misaligned-sequential-addresses-fall-5-32-byte-l2-cache-seqments" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>Memory allocated through the CUDA Runtime API, such as via <code class="docutils literal notranslate"><span class="pre">cudaMalloc()</span></code>,
 is guaranteed to be aligned to at least 256 bytes. Therefore, choosing 
sensible thread block sizes, such as multiples of the warp size (i.e., 
32 on current GPUs), facilitates memory accesses by warps that are 
properly aligned. (Consider what would happen to the memory addresses 
accessed by the second, third, and subsequent thread blocks if the 
thread block size was not a multiple of warp size, for example.)</p>
</section>
<section id="effects-of-misaligned-accesses">
<span id="id34"></span><h4>
<span class="section-number">9.2.1.3. </span>Effects of Misaligned Accesses<a class="headerlink" href="#effects-of-misaligned-accesses" title="Permalink to this headline"></a>
</h4>
<p>It is easy and informative to explore the ramifications of misaligned accesses using a simple copy kernel, such as the one
in <a class="reference internal" href="#a-copy-kernel-that-illustrates-misaligned-accesses"><span class="std std-ref">A copy kernel that illustrates misaligned accesses</span></a>.</p>
<p class="title sectiontitle rubric" id="a-copy-kernel-that-illustrates-misaligned-accesses">A copy kernel that illustrates misaligned accesses</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell7"><span></span><span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">offsetCopy</span><span class="p">(</span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">odata</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">idata</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">offset</span><span class="p">)</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">xid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">offset</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">odata</span><span class="p">[</span><span class="n">xid</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">idata</span><span class="p">[</span><span class="n">xid</span><span class="p">];</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell7">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>In <a class="reference internal" href="#a-copy-kernel-that-illustrates-misaligned-accesses"><span class="std std-ref">A copy kernel that illustrates misaligned accesses</span></a>, data is copied from the input array <code class="docutils literal notranslate"><span class="pre">idata</span></code> to the output array, both
of which exist in global memory. The kernel is executed within a loop in host code that varies the parameter <code class="docutils literal notranslate"><span class="pre">offset</span></code> from 0 to 32
(for example, <a class="reference internal" href="#misaligned-sequential-addresses-fall-5-32-byte-l2-cache-seqments"><span class="std std-ref">Figure 4</span></a> corresponds to this misalignments).
The effective bandwidth for the copy with various offsets on an NVIDIA Tesla V100 (<a class="reference internal" href="#cuda-compute-capability"><span class="std std-ref">compute capability</span></a> 7.0)
is shown in <a class="reference internal" href="#performance-offsetcopy-kernel-figure"><span class="std std-ref">Figure 5</span></a>.</p>
<figure class="align-center" id="performance-offsetcopy-kernel-figure">
<img alt="Performance of offsetCopy kernel" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/performance-of-offsetcopy-kernel.png">
<figcaption>
<p><span class="caption-number">Figure 5 </span><span class="caption-text">Performance of offsetCopy kernel</span><a class="headerlink" href="#performance-offsetcopy-kernel-figure" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>For the NVIDIA Tesla V100, global memory accesses with no offset or 
with offsets that are multiples of 8 words result in four 32-byte 
transactions. The achieved bandwidth is approximately 790 GB/s. 
Otherwise, five 32-byte segments are loaded per warp, and we would 
expect approximately 4/5<sup>th</sup> of the memory throughput achieved with no offsets.</p>
<p>In this particular example, the offset memory throughput achieved is, however, approximately 9/10<sup>th</sup>,
 because adjacent warps reuse the cache lines their neighbors fetched. 
So while the impact is still evident it is not as large as we might have
 expected. It would have been more so if adjacent warps had not 
exhibited such a high degree of reuse of the over-fetched cache lines.</p>
</section>
<section id="strided-accesses">
<span id="id35"></span><h4>
<span class="section-number">9.2.1.4. </span>Strided Accesses<a class="headerlink" href="#strided-accesses" title="Permalink to this headline"></a>
</h4>
<p>As seen above, in the case of misaligned sequential accesses, caches 
help to alleviate the performance impact. It may be different with 
non-unit-strided accesses, however, and this is a pattern that occurs 
frequently when dealing with multidimensional data or matrices. For this
 reason, ensuring that as much as possible of the data in each cache 
line fetched is actually used is an important part of performance 
optimization of memory accesses on these devices.</p>
<p>To illustrate the effect of strided access on effective bandwidth, see the kernel <code class="docutils literal notranslate"><span class="pre">strideCopy()</span></code> in <a class="reference internal" href="#a-kernel-to-illustrate-non-unit-stride-data-copy"><span class="std std-ref">A kernel to illustrate non-unit stride data copy</span></a>, which copies data with a stride of stride elements between threads from <code class="docutils literal notranslate"><span class="pre">idata</span></code> to <code class="docutils literal notranslate"><span class="pre">odata</span></code>.</p>
<p class="title sectiontitle rubric" id="a-kernel-to-illustrate-non-unit-stride-data-copy">A kernel to illustrate non-unit stride data copy</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell8"><span></span><span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">strideCopy</span><span class="p">(</span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">odata</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">idata</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">stride</span><span class="p">)</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">xid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="n">stride</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">odata</span><span class="p">[</span><span class="n">xid</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">idata</span><span class="p">[</span><span class="n">xid</span><span class="p">];</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell8">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p><a class="reference internal" href="#adjacent-threads-accessing-memory-with-stride-of-2-figure"><span class="std std-ref">Figure 6</span></a>
 illustrates such a situation; in this case, threads within a warp 
access words in memory with a stride of 2. This action leads to a load 
of eight L2 cache segments per warp on the Tesla V100 (compute 
capability 7.0).</p>
<figure class="align-center" id="adjacent-threads-accessing-memory-with-stride-of-2-figure">
<img alt="Adjacent threads accessing memory with a stride of 2" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/adjacent-threads-accessing-memory-with-stride-of-2.png">
<figcaption>
<p><span class="caption-number">Figure 6 </span><span class="caption-text">Adjacent threads accessing memory with a stride of 2</span><a class="headerlink" href="#adjacent-threads-accessing-memory-with-stride-of-2-figure" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>A stride of 2 results in a 50% of load/store efficiency since half 
the elements in the transaction are not used and represent
wasted bandwidth. As the stride increases, the effective bandwidth 
decreases until the point where 32 32-byte segments are loaded
for the 32 threads in a warp, as indicated in <a class="reference internal" href="#performance-of-stridecopy-kernel-figure"><span class="std std-ref">Figure 7</span></a>.</p>
<figure class="align-center" id="performance-of-stridecopy-kernel-figure">
<img alt="Performance of strideCopy kernel" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/performance-of-stridecopy-kernel.png">
<figcaption>
<p><span class="caption-number">Figure 7 </span><span class="caption-text">Performance of strideCopy kernel</span><a class="headerlink" href="#performance-of-stridecopy-kernel-figure" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>As illustrated in <a class="reference internal" href="#performance-of-stridecopy-kernel-figure"><span class="std std-ref">Figure 7</span></a>,
 non-unit-stride global memory accesses should be avoided whenever 
possible. One method for doing so utilizes shared memory, which is 
discussed in the next section.</p>
</section>
</section>
<section id="l2-cache">
<span id="id36"></span><h3>
<span class="section-number">9.2.2. </span>L2 Cache<a class="headerlink" href="#l2-cache" title="Permalink to this headline"></a>
</h3>
<p>Starting with CUDA 11.0, devices of compute capability 8.0 and above 
have the capability to influence persistence of data in the L2 cache. 
Because L2 cache is on-chip, it potentially provides higher bandwidth 
and lower latency accesses to global memory.</p>
<p>For more details refer to the L2 Access Management section in the <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#L2_access_intro">CUDA C++ Programming Guide</a>.</p>
<section id="l2-cache-access-window">
<span id="l2-cache-window"></span><h4>
<span class="section-number">9.2.2.1. </span>L2 Cache Access Window<a class="headerlink" href="#l2-cache-access-window" title="Permalink to this headline"></a>
</h4>
<p>When a CUDA kernel accesses a data region in the global memory repeatedly, such data accesses can be considered to be <em>persisting</em>. On the other hand, if the data is only accessed once, such data accesses can be considered to be <em>streaming</em>.
 A portion of the L2 cache can be set aside for persistent accesses to a
 data region in global memory. If this set-aside portion is not used by 
persistent accesses, then streaming or normal data accesses can use it.</p>
<p>The L2 cache set-aside size for persisting accesses may be adjusted, within limits:</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell9"><span></span><span class="n">cudaGetDeviceProperties</span><span class="p">(</span><span class="o">&amp;</span><span class="n">prop</span><span class="p">,</span><span class="w"> </span><span class="n">device_id</span><span class="p">);</span><span class="w"></span>
<span class="n">cudaDeviceSetLimit</span><span class="p">(</span><span class="n">cudaLimitPersistingL2CacheSize</span><span class="p">,</span><span class="w"> </span><span class="n">prop</span><span class="p">.</span><span class="n">persistingL2CacheMaxSize</span><span class="p">);</span><span class="w"> </span><span class="cm">/* Set aside max possible size of L2 cache for persisting accesses */</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell9">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>Mapping of user data to L2 set-aside portion can be controlled using 
an access policy window on a CUDA stream or CUDA graph kernel node. The 
example below shows how to use the access policy window on a CUDA 
stream.</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell10"><span></span><span class="n">cudaStreamAttrValue</span><span class="w"> </span><span class="n">stream_attribute</span><span class="p">;</span><span class="w">                                         </span><span class="c1">// Stream level attributes data structure</span>
<span class="n">stream_attribute</span><span class="p">.</span><span class="n">accessPolicyWindow</span><span class="p">.</span><span class="n">base_ptr</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">ptr</span><span class="p">);</span><span class="w"> </span><span class="c1">// Global Memory data pointer</span>
<span class="n">stream_attribute</span><span class="p">.</span><span class="n">accessPolicyWindow</span><span class="p">.</span><span class="n">num_bytes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">num_bytes</span><span class="p">;</span><span class="w">                    </span><span class="c1">// Number of bytes for persisting accesses.</span>
<span class="w">                                                                              </span><span class="c1">// (Must be less than cudaDeviceProp::accessPolicyMaxWindowSize)</span>
<span class="n">stream_attribute</span><span class="p">.</span><span class="n">accessPolicyWindow</span><span class="p">.</span><span class="n">hitRatio</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0</span><span class="p">;</span><span class="w">                          </span><span class="c1">// Hint for L2 cache hit ratio for persisting accesses in the num_bytes region</span>
<span class="n">stream_attribute</span><span class="p">.</span><span class="n">accessPolicyWindow</span><span class="p">.</span><span class="n">hitProp</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="n">cudaAccessPropertyPersisting</span><span class="p">;</span><span class="w"> </span><span class="c1">// Type of access property on cache hit</span>
<span class="n">stream_attribute</span><span class="p">.</span><span class="n">accessPolicyWindow</span><span class="p">.</span><span class="n">missProp</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">cudaAccessPropertyStreaming</span><span class="p">;</span><span class="w">  </span><span class="c1">// Type of access property on cache miss.</span>

<span class="c1">//Set the attributes to a CUDA stream of type cudaStream_t</span>
<span class="n">cudaStreamSetAttribute</span><span class="p">(</span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="n">cudaStreamAttributeAccessPolicyWindow</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">stream_attribute</span><span class="p">);</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell10">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>The access policy window requires a value for <code class="docutils literal notranslate"><span class="pre">hitRatio</span></code> and <code class="docutils literal notranslate"><span class="pre">num_bytes</span></code>. Depending on the value of the <code class="docutils literal notranslate"><span class="pre">num_bytes</span></code> parameter and the size of L2 cache, one may need to tune the value of <code class="docutils literal notranslate"><span class="pre">hitRatio</span></code> to avoid thrashing of L2 cache lines.</p>
</section>
<section id="tuning-the-access-window-hit-ratio">
<span id="l2-cache-hit-ratio"></span><h4>
<span class="section-number">9.2.2.2. </span>Tuning the Access Window Hit-Ratio<a class="headerlink" href="#tuning-the-access-window-hit-ratio" title="Permalink to this headline"></a>
</h4>
<p>The <code class="docutils literal notranslate"><span class="pre">hitRatio</span></code> parameter can be used to specify the fraction of accesses that receive the <code class="docutils literal notranslate"><span class="pre">hitProp</span></code> property. For example, if the <code class="docutils literal notranslate"><span class="pre">hitRatio</span></code>
 value is 0.6, 60% of the memory accesses in the global memory region 
[ptr..ptr+num_bytes) have the persisting property and 40% of the memory 
accesses have the streaming property. To understand the effect of <code class="docutils literal notranslate"><span class="pre">hitRatio</span></code> and <code class="docutils literal notranslate"><span class="pre">num_bytes</span></code>, we use a sliding window micro benchmark.</p>
<p>This microbenchmark uses a 1024 MB region in GPU global memory. 
First, we set aside 30 MB of the L2 cache for persisting accesses using <code class="docutils literal notranslate"><span class="pre">cudaDeviceSetLimit()</span></code>, as discussed above. Then, as shown in the figure below, we specify that the accesses to the first <code class="docutils literal notranslate"><span class="pre">freqSize</span> <span class="pre">*</span> <span class="pre">sizeof(int)</span></code>
 bytes of the memory region are persistent. This data will thus use the 
L2 set-aside portion. In our experiment, we vary the size of this 
persistent data region from 10 MB to 60 MB to model various scenarios 
where data fits in or exceeds the available L2 set-aside portion of 30 
MB. Note that the NVIDIA Tesla A100 GPU has 40 MB of total L2 cache 
capacity. Accesses to the remaining data of the memory region (i.e., 
streaming data) are considered normal or streaming accesses and will 
thus use the remaining 10 MB of the non set-aside L2 portion (unless 
part of the L2 set-aside portion is unused).</p>
<figure class="align-center" id="l2-cache-hit-ratio-sliding-window-l2">
<a class="reference internal image-reference" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/_images/sliding-window-l2.png"><img alt="Mapping Persistent data accesses to set-aside L2 in sliding window experiment" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/sliding-window-l2.png" style="width: 800px;"></a>
<figcaption>
<p><span class="caption-number">Figure 8 </span><span class="caption-text">Mapping Persistent data accesses to set-aside L2 in sliding window experiment</span><a class="headerlink" href="#l2-cache-hit-ratio-sliding-window-l2" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>Consider the following kernel code and access window parameters, as the implementation of the sliding window experiment.</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell11"><span></span><span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">kernel</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">data_persistent</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">data_streaming</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">dataSize</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">freqSize</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">tid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span><span class="w"></span>

<span class="w">    </span><span class="cm">/*Each CUDA thread accesses one element in the persistent data section</span>
<span class="cm">      and one element in the streaming data section.</span>
<span class="cm">      Because the size of the persistent memory region (freqSize * sizeof(int) bytes) is much</span>
<span class="cm">      smaller than the size of the streaming memory region (dataSize * sizeof(int) bytes), data</span>
<span class="cm">      in the persistent region is accessed more frequently*/</span><span class="w"></span>

<span class="w">    </span><span class="n">data_persistent</span><span class="p">[</span><span class="n">tid</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">freqSize</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">data_persistent</span><span class="p">[</span><span class="n">tid</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">freqSize</span><span class="p">];</span><span class="w"></span>
<span class="w">    </span><span class="n">data_streaming</span><span class="p">[</span><span class="n">tid</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">dataSize</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">data_streaming</span><span class="p">[</span><span class="n">tid</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">dataSize</span><span class="p">];</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="n">stream_attribute</span><span class="p">.</span><span class="n">accessPolicyWindow</span><span class="p">.</span><span class="n">base_ptr</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">data_persistent</span><span class="p">);</span><span class="w"></span>
<span class="n">stream_attribute</span><span class="p">.</span><span class="n">accessPolicyWindow</span><span class="p">.</span><span class="n">num_bytes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">freqSize</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span><span class="w">   </span><span class="c1">//Number of bytes for persisting accesses in range 10-60 MB</span>
<span class="n">stream_attribute</span><span class="p">.</span><span class="n">accessPolicyWindow</span><span class="p">.</span><span class="n">hitRatio</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0</span><span class="p">;</span><span class="w">                      </span><span class="c1">//Hint for cache hit ratio. Fixed value 1.0</span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell11">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>The performance of the above kernel is shown in the chart below. When
 the persistent data region fits well into the 30 MB set-aside portion 
of the L2 cache, a performance increase of as much as 50% is observed. 
However, once the size of this persistent data region exceeds the size 
of the L2 set-aside cache portion, approximately 10% performance drop is
 observed due to thrashing of L2 cache lines.</p>
<figure class="align-center" id="l2-cache-hit-ratio-l2-hitratio-before">
<a class="reference internal image-reference" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/_images/l2-hitratio-before.png"><img alt="The performance of the sliding-window benchmark with fixed hit-ratio of 1.0" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/l2-hitratio-before.png" style="width: 800px;"></a>
<figcaption>
<p><span class="caption-number">Figure 9 </span><span class="caption-text">The performance of the sliding-window benchmark with fixed hit-ratio of 1.0</span><a class="headerlink" href="#l2-cache-hit-ratio-l2-hitratio-before" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>In order to optimize the performance, when the size of the persistent
 data is more than the size of the set-aside L2 cache portion, we tune 
the <code class="docutils literal notranslate"><span class="pre">num_bytes</span></code> and <code class="docutils literal notranslate"><span class="pre">hitRatio</span></code> parameters in the access window as below.</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell12"><span></span><span class="n">stream_attribute</span><span class="p">.</span><span class="n">accessPolicyWindow</span><span class="p">.</span><span class="n">base_ptr</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">data_persistent</span><span class="p">);</span><span class="w"></span>
<span class="n">stream_attribute</span><span class="p">.</span><span class="n">accessPolicyWindow</span><span class="p">.</span><span class="n">num_bytes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">20</span><span class="o">*</span><span class="mi">1024</span><span class="o">*</span><span class="mi">1024</span><span class="p">;</span><span class="w">                                  </span><span class="c1">//20 MB</span>
<span class="n">stream_attribute</span><span class="p">.</span><span class="n">accessPolicyWindow</span><span class="p">.</span><span class="n">hitRatio</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="mi">20</span><span class="o">*</span><span class="mi">1024</span><span class="o">*</span><span class="mi">1024</span><span class="p">)</span><span class="o">/</span><span class="p">((</span><span class="kt">float</span><span class="p">)</span><span class="n">freqSize</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span><span class="w">  </span><span class="c1">//Such that up to 20MB of data is resident.</span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell12">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>We fix the <code class="docutils literal notranslate"><span class="pre">num_bytes</span></code> in the access window to 20 MB and tune the <code class="docutils literal notranslate"><span class="pre">hitRatio</span></code>
 such that a random 20 MB of the total persistent data is resident in 
the L2 set-aside cache portion. The remaining portion of this persistent
 data will be accessed using the streaming property. This helps in 
reducing cache thrashing. The results are shown in the chart below, 
where we see good performance regardless of whether the persistent data 
fits in the L2 set-aside or not.</p>
<figure class="align-center" id="l2-cache-hit-ratio-l2-hitratio-after">
<a class="reference internal image-reference" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/_images/l2-hitratio-after.png"><img alt="The performance of the sliding-window benchmark with tuned hit-ratio" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/l2-hitratio-after.png" style="width: 800px;"></a>
<figcaption>
<p><span class="caption-number">Figure 10 </span><span class="caption-text">The performance of the sliding-window benchmark with tuned hit-ratio</span><a class="headerlink" href="#l2-cache-hit-ratio-l2-hitratio-after" title="Permalink to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="shared-memory">
<span id="id37"></span><h3>
<span class="section-number">9.2.3. </span>Shared Memory<a class="headerlink" href="#shared-memory" title="Permalink to this headline"></a>
</h3>
<p>Because it is on-chip, shared memory has much higher bandwidth and 
lower latency than local and global memory - provided there are no bank 
conflicts between the threads, as detailed in the following section.</p>
<section id="shared-memory-and-memory-banks">
<span id="id38"></span><h4>
<span class="section-number">9.2.3.1. </span>Shared Memory and Memory Banks<a class="headerlink" href="#shared-memory-and-memory-banks" title="Permalink to this headline"></a>
</h4>
<p>To achieve high memory bandwidth for concurrent accesses, shared memory is divided into equally sized memory modules (<em>banks</em>) that can be accessed simultaneously. Therefore, any memory load or store of <em>n</em> addresses that spans <em>n</em> distinct memory banks can be serviced simultaneously, yielding an effective bandwidth that is <em>n</em> times as high as the bandwidth of a single bank.</p>
<p>However, if multiple addresses of a memory request map to the same 
memory bank, the accesses are serialized. The hardware splits a memory 
request that has bank conflicts into as many separate conflict-free 
requests as necessary, decreasing the effective bandwidth by a factor 
equal to the number of separate memory requests. The one exception here 
is when multiple threads in a warp address the same shared memory 
location, resulting in a broadcast. In this case, multiple broadcasts 
from different banks are coalesced into a single multicast from the 
requested shared memory locations to the threads.</p>
<p>To minimize bank conflicts, it is important to understand how memory 
addresses map to memory banks and how to optimally schedule memory 
requests.</p>
<p>On devices of compute capability 5.x or newer, each bank has a 
bandwidth of 32 bits every clock cycle, and successive 32-bit words are 
assigned to successive banks. The warp size is 32 threads and the number
 of banks is also 32, so bank conflicts can occur between any threads in
 the warp. See <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-5-x">Compute Capability 5.x</a> for further details.</p>
</section>
<section id="shared-memory-in-matrix-multiplication-c-ab">
<span id="id39"></span><h4>
<span class="section-number">9.2.3.2. </span>Shared Memory in Matrix Multiplication (C=AB)<a class="headerlink" href="#shared-memory-in-matrix-multiplication-c-ab" title="Permalink to this headline"></a>
</h4>
<p>Shared memory enables cooperation between threads in a block. When 
multiple threads in a block use the same data from global memory, shared
 memory can be used to access the data from global memory only once. 
Shared memory can also be used to avoid uncoalesced memory accesses by 
loading and storing data in a coalesced pattern from global memory and 
then reordering it in shared memory. Aside from memory bank conflicts, 
there is no penalty for non-sequential or unaligned accesses by a warp 
in shared memory.</p>
<p>The use of shared memory is illustrated via the simple example of a 
matrix multiplication C = AB for the case with A of dimension Mxw, B of 
dimension wxN, and C of dimension MxN. To keep the kernels simple, M and
 N are multiples of 32, since the warp size (w) is 32 for current 
devices.</p>
<p>A natural decomposition of the problem is to use a block and tile 
size of wxw threads. Therefore, in terms of wxw tiles, A is a column 
matrix, B is a row matrix, and C is their outer product; see <a class="reference internal" href="#shared-memory-in-matrix-multiplication-c-ab-block-column-matrix-a-multiplied-block-row-matrix-b-product-matrix-c"><span class="std std-ref">Figure 11</span></a>.
 A grid of N/w by M/w blocks is launched, where each thread block 
calculates the elements of a different tile in C from a single tile of A
 and a single tile of B.</p>
<figure class="align-center" id="shared-memory-in-matrix-multiplication-c-ab-block-column-matrix-a-multiplied-block-row-matrix-b-product-matrix-c">
<img alt="Block-column matrix multiplied by block-row matrix. Block-column matrix (A) multiplied by block-row matrix (B) with resulting product matrix (C)." src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/matrix-multiplication-block-column-by-block-row.png">
<figcaption>
<p><span class="caption-number">Figure 11 </span><span class="caption-text">Block-
column matrix multiplied by block-row matrix. Block-column matrix (A) 
multiplied by block-row matrix (B) with resulting product matrix (C).</span><a class="headerlink" href="#shared-memory-in-matrix-multiplication-c-ab-block-column-matrix-a-multiplied-block-row-matrix-b-product-matrix-c" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>To do this, the <code class="docutils literal notranslate"><span class="pre">simpleMultiply</span></code> kernel (<a class="reference internal" href="#unoptimized-matrix-multiplication-example"><span class="std std-ref">Unoptimized matrix multiplication</span></a>) calculates the output elements of a tile of matrix C.</p>
<p class="title sectiontitle rubric" id="unoptimized-matrix-multiplication-example">Unoptimized matrix multiplication</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell13"><span></span><span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">simpleMultiply</span><span class="p">(</span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">c</span><span class="p">,</span><span class="w"></span>
<span class="w">                               </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">row</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">TILE_DIM</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">TILE_DIM</span><span class="o">+</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">N</span><span class="o">+</span><span class="n">col</span><span class="p">];</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="n">c</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">N</span><span class="o">+</span><span class="n">col</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sum</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell13">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>In <a class="reference internal" href="#unoptimized-matrix-multiplication-example"><span class="std std-ref">Unoptimized matrix multiplication</span></a>, <code class="docutils literal notranslate"><span class="pre">a</span></code>, <code class="docutils literal notranslate"><span class="pre">b</span></code>, and <code class="docutils literal notranslate"><span class="pre">c</span></code> are pointers to global memory for the matrices A, B, and C, respectively; <code class="docutils literal notranslate"><span class="pre">blockDim.x</span></code>, <code class="docutils literal notranslate"><span class="pre">blockDim.y</span></code>, and <code class="docutils literal notranslate"><span class="pre">TILE_DIM</span></code> are all equal to w. Each thread in the wxw-thread block calculates one element in a tile of C. <code class="docutils literal notranslate"><span class="pre">row</span></code> and <code class="docutils literal notranslate"><span class="pre">col</span></code> are the row and column of the element in C being calculated by a particular thread. The <code class="docutils literal notranslate"><span class="pre">for</span></code> loop over <code class="docutils literal notranslate"><span class="pre">i</span></code> multiplies a row of A by a column of B, which is then written to C.</p>
<p>The effective bandwidth of this kernel is 119.9 GB/s on an NVIDIA 
Tesla V100. To analyze performance, it is necessary to consider how 
warps access global memory in the <code class="docutils literal notranslate"><span class="pre">for</span></code>
 loop. Each warp of threads calculates one row of a tile of C, which 
depends on a single row of A and an entire tile of B as illustrated in <a class="reference internal" href="#shared-memory-in-matrix-multiplication-c-ab-computing-row-c-tile-c-row-a-tile-b"><span class="std std-ref">Figure 12</span></a>.</p>
<figure class="align-center" id="shared-memory-in-matrix-multiplication-c-ab-computing-row-c-tile-c-row-a-tile-b">
<img alt="Computing a row of a tile. Computing a row of a tile in C using one row of A and an entire tile of B." src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/computing-row-of-tile.png">
<figcaption>
<p><span class="caption-number">Figure 12 </span><span class="caption-text">Computing a row of a tile. Computing a row of a tile in C using one row of A and an entire tile of B.</span><a class="headerlink" href="#shared-memory-in-matrix-multiplication-c-ab-computing-row-c-tile-c-row-a-tile-b" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>For each iteration <em>i</em> of the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop, the threads in a warp read a row of the B tile, which is a sequential and coalesced access for all compute capabilities.</p>
<p>However, for each iteration <em>i</em>, all threads in a warp read the same value from global memory for matrix A, as the index <code class="docutils literal notranslate"><span class="pre">row*TILE_DIM+i</span></code>
 is constant within a warp. Even though such an access requires only 1 
transaction on devices of compute capability 2.0 or higher, there is 
wasted bandwidth in the transaction, because only one 4-byte word out of
 8 words in a 32-byte cache segment is used. We can reuse this cache 
line in subsequent iterations of the loop, and we would eventually 
utilize all 8 words; however, when many warps execute on the same 
multiprocessor simultaneously, as is generally the case, the cache line 
may easily be evicted from the cache between iterations <em>i</em> and <em>i+1</em>.</p>
<p>The performance on a device of any compute capability can be improved by reading a tile of A into shared memory as shown
in <a class="reference internal" href="#using-shared-memory-to-improve-the-global-memory-load-efficiency-in-matrix-multiplication"><span class="std std-ref">Using shared memory to improve the global memory load efficiency in matrix multiplication</span></a>.</p>
<p class="title sectiontitle rubric" id="using-shared-memory-to-improve-the-global-memory-load-efficiency-in-matrix-multiplication">Using shared memory to improve the global memory load efficiency in matrix multiplication</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell14"><span></span><span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">coalescedMultiply</span><span class="p">(</span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">c</span><span class="p">,</span><span class="w"></span>
<span class="w">                                  </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">aTile</span><span class="p">[</span><span class="n">TILE_DIM</span><span class="p">][</span><span class="n">TILE_DIM</span><span class="p">];</span><span class="w"></span>

<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">row</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">aTile</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">TILE_DIM</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span><span class="w"></span>
<span class="w">    </span><span class="n">__syncwarp</span><span class="p">();</span><span class="w"></span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">TILE_DIM</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">aTile</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">N</span><span class="o">+</span><span class="n">col</span><span class="p">];</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="n">c</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">N</span><span class="o">+</span><span class="n">col</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sum</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell14">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>In <a class="reference internal" href="#using-shared-memory-to-improve-the-global-memory-load-efficiency-in-matrix-multiplication"><span class="std std-ref">Using shared memory to improve the global memory load efficiency in matrix multiplication</span></a>,
 each element in a tile of A is read from global memory only once, in a 
fully coalesced fashion (with no wasted bandwidth), to shared memory. 
Within each iteration of the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop, a value in shared memory is broadcast to all threads in a warp. Instead of a <code class="docutils literal notranslate"><span class="pre">__syncthreads()</span></code>synchronization barrier call, a <code class="docutils literal notranslate"><span class="pre">__syncwarp()</span></code>
 is sufficient after reading the tile of A into shared memory because 
only threads within the warp that write the data into shared memory read
 this data. This kernel has an effective bandwidth of 144.4 GB/s on an 
NVIDIA Tesla V100. This illustrates the use of the shared memory as a <em>user-managed cache</em>
 when the hardware L1 cache eviction policy does not match up well with 
the needs of the application or when L1 cache is not used for reads from
 global memory.</p>
<p>A further improvement can be made to how <a class="reference internal" href="#using-shared-memory-to-improve-the-global-memory-load-efficiency-in-matrix-multiplication"><span class="std std-ref">Using shared memory to improve the global memory load efficiency in matrix multiplication</span></a>
 deals with matrix B. In calculating each of the rows of a tile of 
matrix C, the entire tile of B is read. The repeated reading of the B 
tile can be eliminated by reading it into shared memory once (<a class="reference internal" href="#improvement-by-reading-additional-data-into-shared-memory"><span class="std std-ref">Improvement by reading additional data into shared memory</span></a>).</p>
<p class="title sectiontitle rubric" id="improvement-by-reading-additional-data-into-shared-memory">Improvement by reading additional data into shared memory</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell15"><span></span><span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">sharedABMultiply</span><span class="p">(</span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">c</span><span class="p">,</span><span class="w"></span>
<span class="w">                                 </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">aTile</span><span class="p">[</span><span class="n">TILE_DIM</span><span class="p">][</span><span class="n">TILE_DIM</span><span class="p">],</span><span class="w"></span>
<span class="w">                     </span><span class="n">bTile</span><span class="p">[</span><span class="n">TILE_DIM</span><span class="p">][</span><span class="n">TILE_DIM</span><span class="p">];</span><span class="w"></span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">row</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">aTile</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">TILE_DIM</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span><span class="w"></span>
<span class="w">    </span><span class="n">bTile</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="o">*</span><span class="n">N</span><span class="o">+</span><span class="n">col</span><span class="p">];</span><span class="w"></span>
<span class="w">    </span><span class="n">__syncthreads</span><span class="p">();</span><span class="w"></span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">TILE_DIM</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">aTile</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="w"> </span><span class="n">bTile</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="n">c</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">N</span><span class="o">+</span><span class="n">col</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sum</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell15">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>Note that in <a class="reference internal" href="#improvement-by-reading-additional-data-into-shared-memory"><span class="std std-ref">Improvement by reading additional data into shared memory</span></a>, a <code class="docutils literal notranslate"><span class="pre">__syncthreads()</span></code>
 call is required after reading the B tile because a warp reads data 
from shared memory that were written to shared memory by different 
warps. The effective bandwidth of this routine is 195.5 GB/s on an 
NVIDIA Tesla V100. Note that the performance improvement is not due to 
improved coalescing in either case, but to avoiding redundant transfers 
from global memory.</p>
<p>The results of the various optimizations are summarized in <a class="reference internal" href="#performance-improvements-optimizing-c-ab-matrix-table"><span class="std std-ref">Table 2</span></a>.</p>
<div class="wy-table-responsive"><table class="table-no-stripes docutils align-default" id="performance-improvements-optimizing-c-ab-matrix-table">
<caption>
<span class="caption-number">Table 2 </span><span class="caption-text">Performance Improvements Optimizing C = AB Matrix Multiply</span><a class="headerlink" href="#performance-improvements-optimizing-c-ab-matrix-table" title="Permalink to this table"></a>
</caption>
<colgroup>
<col style="width: 77%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="row-odd">
<th class="head"><p>Optimization</p></th>
<th class="head"><p>NVIDIA Tesla V100</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even">
<td><p>No optimization</p></td>
<td><p>119.9 GB/s</p></td>
</tr>
<tr class="row-odd">
<td><p>Coalesced using shared memory to store a tile of A</p></td>
<td><p>144.4 GB/s</p></td>
</tr>
<tr class="row-even">
<td><p>Using shared memory to eliminate redundant reads of a tile of B</p></td>
<td><p>195.5 GB/s</p></td>
</tr>
</tbody>
</table></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Medium Priority:</strong> Use shared memory to avoid redundant transfers from global memory.</p>
</div>
</section>
<section id="shared-memory-in-matrix-multiplication-c-aat">
<span id="shared-memory-in-matrix-multiplication-c-aa"></span><h4>
<span class="section-number">9.2.3.3. </span>Shared Memory in Matrix Multiplication (C=AAT)<a class="headerlink" href="#shared-memory-in-matrix-multiplication-c-aat" title="Permalink to this headline"></a>
</h4>
<p>A variant of the previous matrix multiplication can be used to 
illustrate how strided accesses to global memory, as well as shared 
memory bank conflicts, are handled. This variant simply uses the 
transpose of A in place of B, so C = AA<sup>T</sup>.</p>
<p>A simple implementation for C = AA<sup>T</sup> is shown in <a class="reference internal" href="#unoptimized-handling-of-strided-accesses-to-global-memory"><span class="std std-ref">Unoptimized handling of strided accesses to global memory</span></a>.</p>
<p class="title sectiontitle rubric" id="unoptimized-handling-of-strided-accesses-to-global-memory">Unoptimized handling of strided accesses to global memory</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell16"><span></span><span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">simpleMultiply</span><span class="p">(</span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">M</span><span class="p">)</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">row</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">TILE_DIM</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">TILE_DIM</span><span class="o">+</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">col</span><span class="o">*</span><span class="n">TILE_DIM</span><span class="o">+</span><span class="n">i</span><span class="p">];</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="n">c</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">M</span><span class="o">+</span><span class="n">col</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sum</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell16">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>In the example above, the <em>row</em>-th, <em>col</em>-th element of C is obtained by taking the dot product of the <em>row</em>-th and <em>col</em>-
th rows of A. The effective bandwidth for this kernel is 12.8 GB/s on an
 NVIDIA Tesla V100. These results are substantially lower than the 
corresponding measurements for the C = AB kernel. The difference is in 
how threads in a half warp access elements of A in the second term, <code class="docutils literal notranslate"><span class="pre">a[col*TILE_DIM+i]</span></code>, for each iteration <code class="docutils literal notranslate"><span class="pre">i</span></code>. For a warp of threads, <code class="docutils literal notranslate"><span class="pre">col</span></code> represents sequential columns of the transpose of A, and therefore <code class="docutils literal notranslate"><span class="pre">col*TILE_DIM</span></code> represents a strided access of global memory with a stride of w, resulting in plenty of wasted bandwidth.</p>
<p>The way to avoid strided access is to use shared memory as before, 
except in this case a warp reads a row of A into a column of a shared 
memory tile, as
shown in <a class="reference internal" href="#an-optimized-handling-of-strided-accesses-using-coalesced-reads-from-global-memory"><span class="std std-ref">An optimized handling of strided accesses using coalesced reads from global memory</span></a>.</p>
<p class="title sectiontitle rubric" id="an-optimized-handling-of-strided-accesses-using-coalesced-reads-from-global-memory">An optimized handling of strided accesses using coalesced reads from global memory</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell17"><span></span><span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">coalescedMultiply</span><span class="p">(</span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">M</span><span class="p">)</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">aTile</span><span class="p">[</span><span class="n">TILE_DIM</span><span class="p">][</span><span class="n">TILE_DIM</span><span class="p">],</span><span class="w"></span>
<span class="w">                     </span><span class="n">transposedTile</span><span class="p">[</span><span class="n">TILE_DIM</span><span class="p">][</span><span class="n">TILE_DIM</span><span class="p">];</span><span class="w"></span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">row</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">aTile</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">TILE_DIM</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span><span class="w"></span>
<span class="w">    </span><span class="n">transposedTile</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"></span>
<span class="w">        </span><span class="n">a</span><span class="p">[(</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">TILE_DIM</span><span class="w"> </span><span class="o">+</span><span class="w"></span>
<span class="w">        </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span><span class="w"></span>
<span class="w">    </span><span class="n">__syncthreads</span><span class="p">();</span><span class="w"></span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">TILE_DIM</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">aTile</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="w"> </span><span class="n">transposedTile</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="n">c</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">M</span><span class="o">+</span><span class="n">col</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sum</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell17">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p><a class="reference internal" href="#an-optimized-handling-of-strided-accesses-using-coalesced-reads-from-global-memory"><span class="std std-ref">An optimized handling of strided accesses using coalesced reads from global memory</span></a> uses the shared <code class="docutils literal notranslate"><span class="pre">transposedTile</span></code> to avoid uncoalesced accesses in the second term in the dot product and the shared <code class="docutils literal notranslate"><span class="pre">aTile</span></code>
 technique from the previous example to avoid uncoalesced accesses in 
the first term. The effective bandwidth of this kernel is 140.2 GB/s on 
an NVIDIA Tesla V100.These results are lower than those obtained by the 
final kernel for C = AB. The cause of the difference is shared memory 
bank conflicts.</p>
<p>The reads of elements in <code class="docutils literal notranslate"><span class="pre">transposedTile</span></code>
 within the for loop are free of conflicts, because threads of each half
 warp read across rows of the tile, resulting in unit stride across the 
banks. However, bank conflicts occur when copying the tile from global 
memory into shared memory. To enable the loads from global memory to be 
coalesced, data are read from global memory sequentially. However, this 
requires writing to shared memory in columns, and because of the use of 
wxw tiles in shared memory, this results in a stride between threads of w
 banks - every thread of the warp hits the same bank (Recall that w is 
selected as 32). These many-way bank conflicts are very expensive. The 
simple remedy is to pad the shared memory array so that it has an extra 
column, as in the following line of code.</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell18"><span></span><span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">transposedTile</span><span class="p">[</span><span class="n">TILE_DIM</span><span class="p">][</span><span class="n">TILE_DIM</span><span class="o">+</span><span class="mi">1</span><span class="p">];</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell18">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>This padding eliminates the conflicts entirely, because now the 
stride between threads is w+1 banks (i.e., 33 for current devices), 
which, due to modulo arithmetic used to compute bank indices, is 
equivalent to a unit stride. After this change, the effective bandwidth 
is 199.4 GB/s on an NVIDIA Tesla V100, which is comparable to the 
results from the last C = AB kernel.</p>
<p>The results of these optimizations are summarized in <a class="reference internal" href="#performance-inmprovements-optimizing-c-aa-matrix-multiplication-table"><span class="std std-ref">Table 3</span></a>.</p>
<div class="wy-table-responsive"><table class="table-no-stripes docutils align-default" id="performance-inmprovements-optimizing-c-aa-matrix-multiplication-table">
<caption>
<span class="caption-number">Table 3 </span><span class="caption-text">Performance Improvements Optimizing C = AA<sup>T</sup> Matrix Multiplication</span><a class="headerlink" href="#performance-inmprovements-optimizing-c-aa-matrix-multiplication-table" title="Permalink to this table"></a>
</caption>
<colgroup>
<col style="width: 72%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="row-odd">
<th class="head"><p>Optimization</p></th>
<th class="head"><p>NVIDIA Tesla V100</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even">
<td><p>No optimization</p></td>
<td><p>12.8 GB/s</p></td>
</tr>
<tr class="row-odd">
<td><p>Using shared memory to coalesce global reads</p></td>
<td><p>140.2 GB/s</p></td>
</tr>
<tr class="row-even">
<td><p>Removing bank conflicts</p></td>
<td><p>199.4 GB/s</p></td>
</tr>
</tbody>
</table></div>
<p>These results should be compared with those in <a class="reference internal" href="#performance-improvements-optimizing-c-ab-matrix-table"><span class="std std-ref">Table 2</span></a>. As can be seen from these tables, judicious use of shared memory can dramatically improve performance.</p>
<p>The examples in this section have illustrated three reasons to use shared memory:</p>
<ul class="simple">
<li><p>To enable coalesced accesses to global memory, especially to 
avoid large strides (for general matrices, strides are much larger than 
32)</p></li>
<li><p>To eliminate (or reduce) redundant loads from global memory</p></li>
<li><p>To avoid wasted bandwidth</p></li>
</ul>
</section>
<section id="asynchronous-copy-from-global-memory-to-shared-memory">
<span id="async-copy"></span><h4>
<span class="section-number">9.2.3.4. </span>Asynchronous Copy from Global Memory to Shared Memory<a class="headerlink" href="#asynchronous-copy-from-global-memory-to-shared-memory" title="Permalink to this headline"></a>
</h4>
<p>CUDA 11.0 introduces an <em>async-copy</em> feature that can be used 
within device code to explicitly manage the asynchronous copying of data
 from global memory to shared memory. This feature enables CUDA kernels 
to overlap copying data from global to shared memory with computation. 
It also avoids an intermediary register file access traditionally 
present between the global memory read and the shared memory write.</p>
<p>For more details refer to the <code class="docutils literal notranslate"><span class="pre">memcpy_async</span></code> section in the <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#async_data_operations">CUDA C++ Programming Guide</a>.</p>
<p>To understand the performance difference between synchronous copy and
 asynchronous copy of data from global memory to shared memory, consider
 the following micro benchmark CUDA kernels for demonstrating the 
synchronous and asynchronous approaches. Asynchronous copies are 
hardware accelerated for NVIDIA A100 GPU.</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell19"><span></span><span class="n">template</span><span class="w"> </span><span class="o">&lt;</span><span class="n">typename</span><span class="w"> </span><span class="n">T</span><span class="o">&gt;</span><span class="w"></span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">pipeline_kernel_sync</span><span class="p">(</span><span class="n">T</span><span class="w"> </span><span class="o">*</span><span class="n">global</span><span class="p">,</span><span class="w"> </span><span class="kt">uint64_t</span><span class="w"> </span><span class="o">*</span><span class="n">clock</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">copy_count</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="k">extern</span><span class="w"> </span><span class="n">__shared__</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="n">s</span><span class="p">[];</span><span class="w"></span>
<span class="w">  </span><span class="n">T</span><span class="w"> </span><span class="o">*</span><span class="n">shared</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">s</span><span class="p">);</span><span class="w"></span>

<span class="w">  </span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">clock_start</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">clock64</span><span class="p">();</span><span class="w"></span>

<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">copy_count</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">shared</span><span class="p">[</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">global</span><span class="p">[</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">clock_end</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">clock64</span><span class="p">();</span><span class="w"></span>

<span class="w">  </span><span class="n">atomicAdd</span><span class="p">(</span><span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">unsigned</span><span class="w"> </span><span class="kt">long</span><span class="w"> </span><span class="kt">long</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">clock</span><span class="p">),</span><span class="w"></span>
<span class="w">            </span><span class="n">clock_end</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">clock_start</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="n">template</span><span class="w"> </span><span class="o">&lt;</span><span class="n">typename</span><span class="w"> </span><span class="n">T</span><span class="o">&gt;</span><span class="w"></span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">pipeline_kernel_async</span><span class="p">(</span><span class="n">T</span><span class="w"> </span><span class="o">*</span><span class="n">global</span><span class="p">,</span><span class="w"> </span><span class="kt">uint64_t</span><span class="w"> </span><span class="o">*</span><span class="n">clock</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">copy_count</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="k">extern</span><span class="w"> </span><span class="n">__shared__</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="n">s</span><span class="p">[];</span><span class="w"></span>
<span class="w">  </span><span class="n">T</span><span class="w"> </span><span class="o">*</span><span class="n">shared</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">s</span><span class="p">);</span><span class="w"></span>

<span class="w">  </span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">clock_start</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">clock64</span><span class="p">();</span><span class="w"></span>

<span class="w">  </span><span class="c1">//pipeline pipe;</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">copy_count</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">__pipeline_memcpy_async</span><span class="p">(</span><span class="o">&amp;</span><span class="n">shared</span><span class="p">[</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">],</span><span class="w"></span>
<span class="w">                            </span><span class="o">&amp;</span><span class="n">global</span><span class="p">[</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">],</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">T</span><span class="p">));</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="n">__pipeline_commit</span><span class="p">();</span><span class="w"></span>
<span class="w">  </span><span class="n">__pipeline_wait_prior</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>

<span class="w">  </span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">clock_end</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">clock64</span><span class="p">();</span><span class="w"></span>

<span class="w">  </span><span class="n">atomicAdd</span><span class="p">(</span><span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">unsigned</span><span class="w"> </span><span class="kt">long</span><span class="w"> </span><span class="kt">long</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">clock</span><span class="p">),</span><span class="w"></span>
<span class="w">            </span><span class="n">clock_end</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">clock_start</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell19">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>The synchronous version for the kernel loads an element from global 
memory to an intermediate register and then stores the intermediate 
register value to shared memory. In the asynchronous version of the 
kernel, instructions to load from global memory and store directly into 
shared memory are issued as soon as <code class="docutils literal notranslate"><span class="pre">__pipeline_memcpy_async()</span></code> function is called. The <code class="docutils literal notranslate"><span class="pre">__pipeline_wait_prior(0)</span></code>
 will wait until all the instructions in the pipe object have been 
executed. Using asynchronous copies does not use any intermediate 
register. Not using intermediate registers can help reduce register 
pressure and can increase kernel occupancy. Data copied from global 
memory to shared memory using asynchronous copy instructions can be 
cached in the L1 cache or the L1 cache can be optionally bypassed. If 
individual CUDA threads are copying elements of 16 bytes, the L1 cache 
can be bypassed. This difference is illustrated in <a class="reference internal" href="#async-copy-sync-vs-async-figure"><span class="std std-ref">Figure 13</span></a>.</p>
<figure class="align-center" id="async-copy-sync-vs-async-figure">
<a class="reference internal image-reference" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/_images/sync-vs-async.png"><img alt="Comparing Synchronous vs Asynchronous Copy from Global Memory to Shared Memory" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/sync-vs-async.png" style="width: 800px;"></a>
<figcaption>
<p><span class="caption-number">Figure 13 </span><span class="caption-text">Comparing Synchronous vs Asynchronous Copy from Global Memory to Shared Memory</span><a class="headerlink" href="#async-copy-sync-vs-async-figure" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>We evaluate the performance of both kernels using elements of size 4B, 8B and 16B per thread i.e., using <code class="docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">int2</span></code> and <code class="docutils literal notranslate"><span class="pre">int4</span></code> for the template parameter. We adjust the <code class="docutils literal notranslate"><span class="pre">copy_count</span></code> in the kernels such that each thread block copies from 512 bytes up to 48 MB. The performance of the kernels is shown in <a class="reference internal" href="#async-copy-async-perf-figure"><span class="std std-ref">Figure 14</span></a>.</p>
<figure class="align-center" id="async-copy-async-perf-figure">
<a class="reference internal image-reference" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/_images/async-perf.png"><img alt="Comparing Performance of Synchronous vs Asynchronous Copy from Global Memory to Shared Memory" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/async-perf.png" style="width: 800px;"></a>
<figcaption>
<p><span class="caption-number">Figure 14 </span><span class="caption-text">Comparing Performance of Synchronous vs Asynchronous Copy from Global Memory to Shared Memory</span><a class="headerlink" href="#async-copy-async-perf-figure" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>From the performance chart, the following observations can be made for this experiment.</p>
<ul class="simple">
<li><p>Best performance with synchronous copy is achieved when the <code class="docutils literal notranslate"><span class="pre">copy_count</span></code>
 parameter is a multiple of 4 for all three element sizes. The compiler 
can optimize groups of 4 load and store instructions. This is evident 
from the saw tooth curves.</p></li>
<li><p>Asynchronous copy achieves better performance in nearly all cases.</p></li>
<li><p>The async-copy does not require the <code class="docutils literal notranslate"><span class="pre">copy_count</span></code> parameter to be a multiple of 4, to maximize performance through compiler optimizations.</p></li>
<li><p>Overall, best performance is achieved when using asynchronous copies with an element of size 8 or 16 bytes.</p></li>
</ul>
</section>
</section>
<section id="local-memory">
<span id="id40"></span><h3>
<span class="section-number">9.2.4. </span>Local Memory<a class="headerlink" href="#local-memory" title="Permalink to this headline"></a>
</h3>
<p>Local memory is so named because its scope is local to the thread, 
not because of its physical location. In fact, local memory is off-chip.
 Hence, access to local memory is as expensive as access to global 
memory. In other words, the term <em>local</em> in the name does not imply faster access.</p>
<p>Local memory is used only to hold automatic variables. This is done by the <code class="docutils literal notranslate"><span class="pre">nvcc</span></code>
 compiler when it determines that there is insufficient register space 
to hold the variable. Automatic variables that are likely to be placed 
in local memory are large structures or arrays that would consume too 
much register space and arrays that the compiler determines may be 
indexed dynamically.</p>
<p>Inspection of the PTX assembly code (obtained by compiling with <code class="docutils literal notranslate"><span class="pre">-ptx</span></code> or <code class="docutils literal notranslate"><span class="pre">-keep</span></code> command-line options to <code class="docutils literal notranslate"><span class="pre">nvcc</span></code>)
 reveals whether a variable has been placed in local memory during the 
first compilation phases. If it has, it will be declared using the <code class="docutils literal notranslate"><span class="pre">.local</span></code> mnemonic and accessed using the <code class="docutils literal notranslate"><span class="pre">ld.local</span></code> and <code class="docutils literal notranslate"><span class="pre">st.local</span></code>
 mnemonics. If it has not, subsequent compilation phases might still 
decide otherwise, if they find the variable consumes too much register 
space for the targeted architecture. There is no way to check this for a
 specific variable, but the compiler reports total local memory usage 
per kernel (lmem) when run with the<code class="docutils literal notranslate"><span class="pre">--ptxas-options=-v</span></code> option.</p>
</section>
<section id="texture-memory">
<span id="id41"></span><h3>
<span class="section-number">9.2.5. </span>Texture Memory<a class="headerlink" href="#texture-memory" title="Permalink to this headline"></a>
</h3>
<p>The read-only texture memory space is cached. Therefore, a texture 
fetch costs one device memory read only on a cache miss; otherwise, it 
just costs one read from the texture cache. The texture cache is 
optimized for 2D spatial locality, so threads of the same warp that read
 texture addresses that are close together will achieve best 
performance. Texture memory is also designed for streaming fetches with a
 constant latency; that is, a cache hit reduces DRAM bandwidth demand, 
but not fetch latency.</p>
<p>In certain addressing situations, reading device memory through 
texture fetching can be an advantageous alternative to reading device 
memory from global or constant memory.</p>
<section id="additional-texture-capabilities">
<span id="id42"></span><h4>
<span class="section-number">9.2.5.1. </span>Additional Texture Capabilities<a class="headerlink" href="#additional-texture-capabilities" title="Permalink to this headline"></a>
</h4>
<p>If textures are fetched using <code class="docutils literal notranslate"><span class="pre">tex1D()</span></code>,<code class="docutils literal notranslate"><span class="pre">tex2D()</span></code>, or <code class="docutils literal notranslate"><span class="pre">tex3D()</span></code> rather than <code class="docutils literal notranslate"><span class="pre">tex1Dfetch()</span></code>, the hardware provides other capabilities that might be useful for some applications such as image processing, as shown in <a class="reference internal" href="#useful-features-for-tex1d-tex2d-tex3d-fetches-table"><span class="std std-ref">Table 4</span></a>.</p>
<div class="wy-table-responsive"><table class="table-no-stripes docutils align-default" id="useful-features-for-tex1d-tex2d-tex3d-fetches-table">
<caption>
<span class="caption-number">Table 4 </span><span class="caption-text">Useful Features for tex1D(), tex2D(), and tex3D() Fetches</span><a class="headerlink" href="#useful-features-for-tex1d-tex2d-tex3d-fetches-table" title="Permalink to this table"></a>
</caption>
<colgroup>
<col style="width: 85%">
<col style="width: 7%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="row-odd">
<th class="head"><p>Feature</p></th>
<th class="head"><p>Use</p></th>
<th class="head"><p>Caveat</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even">
<td><p>Filtering</p></td>
<td><p>Fast, low-precision interpolation between texels</p></td>
<td><p>Valid only if the texture reference returns floating-point data</p></td>
</tr>
<tr class="row-odd">
<td><p>Normalized texture coordinates</p></td>
<td><p>Resolution-independent coding</p></td>
<td><p>None</p></td>
</tr>
<tr class="row-even">
<td><p>Addressing modes</p></td>
<td><p>Automatic handling of boundary cases<sup>1</sup></p></td>
<td><p>Can be used only with normalized texture coordinates</p></td>
</tr>
<tr class="row-odd">
<td><p><sup>1</sup> The automatic handling of boundary cases in the 
bottom row of Table 4 refers to how a texture coordinate is resolved 
when it falls outside the valid addressing range. There are two options:
 <em>clamp</em> and <em>wrap</em>. If <em>x</em> is the coordinate and <em>N</em> is the number of texels for a one-dimensional texture, then with clamp, <em>x</em> is replaced by <em>0</em> if <em>x</em> &lt; 0 and by 1-1/<em>N</em> if 1 <em>&lt;</em><em>x</em>. With wrap, <em>x</em> is replaced by <em>frac(x)</em> where <em>frac(x) = x - floor(x)</em>. Floor returns the largest integer less than or equal to <em>x</em>. So, in clamp mode where <em>N</em> = 1, an <em>x</em> of 1.3 is clamped to 1.0; whereas in wrap mode, it is converted to 0.3</p></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>Within a kernel call, the texture cache is not kept coherent with 
respect to global memory writes, so texture fetches from addresses that 
have been written via global stores in the same kernel call return 
undefined data. That is, a thread can safely read a memory location via 
texture if the location has been updated by a previous kernel call or 
memory copy, but not if it has been previously updated by the same 
thread or another thread within the same kernel call.</p>
</section>
</section>
<section id="constant-memory">
<span id="id43"></span><h3>
<span class="section-number">9.2.6. </span>Constant Memory<a class="headerlink" href="#constant-memory" title="Permalink to this headline"></a>
</h3>
<p>There is a total of 64 KB constant memory on a device. The constant 
memory space is cached. As a result, a read from constant memory costs 
one memory read from device memory only on a cache miss; otherwise, it 
just costs one read from the constant cache. Accesses to different 
addresses by threads within a warp are serialized, thus the cost scales 
linearly with the number of unique addresses read by all threads within a
 warp. As such, the constant cache is best when threads in the same warp
 accesses only a few distinct locations. If all threads of a warp access
 the same location, then constant memory can be as fast as a register 
access.</p>
</section>
<section id="registers">
<span id="id44"></span><h3>
<span class="section-number">9.2.7. </span>Registers<a class="headerlink" href="#registers" title="Permalink to this headline"></a>
</h3>
<p>Generally, accessing a register consumes zero extra clock cycles per 
instruction, but delays may occur due to register read-after-write 
dependencies and register memory bank conflicts.</p>
<p>The compiler and hardware thread scheduler will schedule instructions
 as optimally as possible to avoid register memory bank conflicts. An 
application has no direct control over these bank conflicts. In 
particular, there is no register-related reason to pack data into vector
 data types such as <code class="docutils literal notranslate"><span class="pre">float4</span></code> or <code class="docutils literal notranslate"><span class="pre">int4</span></code> types.</p>
<section id="register-pressure">
<span id="id45"></span><h4>
<span class="section-number">9.2.7.1. </span>Register Pressure<a class="headerlink" href="#register-pressure" title="Permalink to this headline"></a>
</h4>
<p>Register pressure occurs when there are not enough registers 
available for a given task. Even though each multiprocessor contains 
thousands of 32-bit registers (see Features and Technical Specifications
 of the CUDA C++ Programming Guide), these are partitioned among 
concurrent threads. To prevent the compiler from allocating too many 
registers, use the <code class="docutils literal notranslate"><span class="pre">-maxrregcount=N</span></code>
 compiler command-line option or the launch bounds kernel definition 
qualifier (see Execution Configuration of the CUDA C++ Programming 
Guide) to control the maximum number of registers to allocated per 
thread.</p>
</section>
</section>
</section>
<section id="allocation">
<span id="id46"></span><h2>
<span class="section-number">9.3. </span>Allocation<a class="headerlink" href="#allocation" title="Permalink to this headline"></a>
</h2>
<p>Device memory allocation and de-allocation via <code class="docutils literal notranslate"><span class="pre">cudaMalloc()</span></code> and <code class="docutils literal notranslate"><span class="pre">cudaFree()</span></code> are expensive operations. It is recommended to use <code class="docutils literal notranslate"><span class="pre">cudaMallocAsync()</span></code> and <code class="docutils literal notranslate"><span class="pre">cudaFreeAsync()</span></code> which are stream ordered pool allocators to manage device memory.</p>
</section>
<section id="numa-best-practices">
<span id="id47"></span><h2>
<span class="section-number">9.4. </span>NUMA Best Practices<a class="headerlink" href="#numa-best-practices" title="Permalink to this headline"></a>
</h2>
<p>Some recent Linux distributions enable automatic NUMA balancing (or “<a class="reference external" href="https://lwn.net/Articles/488709/">AutoNUMA</a>”)
 by default. In some instances, operations performed by automatic NUMA 
balancing may degrade the performance of applications running on NVIDIA 
GPUs. For optimal performance, users should manually tune the NUMA 
characteristics of their application.</p>
<p>The optimal NUMA tuning will depend on the characteristics and 
desired hardware affinities of each application and node, but in general
 applications computing on NVIDIA GPUs are advised to choose a policy 
that disables automatic NUMA balancing. For example, on IBM Newell 
POWER9 nodes (where the CPUs correspond to NUMA nodes 0 and 8), use:</p>
<div class="highlight-text notranslate">
<div class="highlight"><pre id="codecell20"><span></span>numactl --membind=0,8
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell20">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>to bind memory allocations to the CPUs.</p>
</section>
</section>
<section id="execution-configuration-optimizations">
<span id="id48"></span><h1>
<span class="section-number">10. </span>Execution Configuration Optimizations<a class="headerlink" href="#execution-configuration-optimizations" title="Permalink to this headline"></a>
</h1>
<p>One of the keys to good performance is to keep the multiprocessors on
 the device as busy as possible. A device in which work is poorly 
balanced across the multiprocessors will deliver suboptimal performance.
 Hence, it’s important to design your application to use threads and 
blocks in a way that maximizes hardware utilization and to limit 
practices that impede the free distribution of work. A key concept in 
this effort is occupancy, which is explained in the following sections.</p>
<p>Hardware utilization can also be improved in some cases by designing 
your application so that multiple, independent kernels can execute at 
the same time. Multiple kernels executing at the same time is known as 
concurrent kernel execution. Concurrent kernel execution is described 
below.</p>
<p>Another important concept is the management of system resources 
allocated for a particular task. How to manage this resource utilization
 is discussed in the final sections of this chapter.</p>
<section id="occupancy">
<span id="id49"></span><h2>
<span class="section-number">10.1. </span>Occupancy<a class="headerlink" href="#occupancy" title="Permalink to this headline"></a>
</h2>
<p>Thread instructions are executed sequentially in CUDA, and, as a 
result, executing other warps when one warp is paused or stalled is the 
only way to hide latencies and keep the hardware busy. Some metric 
related to the number of active warps on a multiprocessor is therefore 
important in determining how effectively the hardware is kept busy. This
 metric is <em>occupancy</em>.</p>
<p>Occupancy is the ratio of the number of active warps per 
multiprocessor to the maximum number of possible active warps. (To 
determine the latter number, see the <code class="docutils literal notranslate"><span class="pre">deviceQuery</span></code> CUDA Sample or refer to <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities">Compute Capabilities</a>.) Another way to view occupancy is the percentage of the hardware’s ability to process warps that is actively in use.</p>
<p>Higher occupancy does not always equate to higher performance-there 
is a point above which additional occupancy does not improve 
performance. However, low occupancy always interferes with the ability 
to hide memory latency, resulting in performance degradation.</p>
<p>Per thread resources required by a CUDA kernel might limit the 
maximum block size in an unwanted way. In order to maintain forward 
compatibility to future hardware and toolkits and to ensure that at 
least one thread block can run on an SM, developers should include the 
single argument <code class="docutils literal notranslate"><span class="pre">__launch_bounds__(maxThreadsPerBlock)</span></code>
 which specifies the largest block size that the kernel will be launched
 with. Failure to do so could lead to “too many resources requested for 
launch” errors. Providing the two argument version of <code class="docutils literal notranslate"><span class="pre">__launch_bounds__(maxThreadsPerBlock,minBlocksPerMultiprocessor)</span></code> can improve performance in some cases. The right value for <code class="docutils literal notranslate"><span class="pre">minBlocksPerMultiprocessor</span></code> should be determined using a detailed per kernel analysis.</p>
<section id="calculating-occupancy">
<span id="id50"></span><h3>
<span class="section-number">10.1.1. </span>Calculating Occupancy<a class="headerlink" href="#calculating-occupancy" title="Permalink to this headline"></a>
</h3>
<p>One of several factors that determine occupancy is register 
availability. Register storage enables threads to keep local variables 
nearby for low-latency access. However, the set of registers (known as 
the <em>register file</em>) is a limited commodity that all threads 
resident on a multiprocessor must share. Registers are allocated to an 
entire block all at once. So, if each thread block uses many registers, 
the number of thread blocks that can be resident on a multiprocessor is 
reduced, thereby lowering the occupancy of the multiprocessor. The 
maximum number of registers per thread can be set manually at 
compilation time per-file using the <code class="docutils literal notranslate"><span class="pre">-maxrregcount</span></code> option or per-kernel using the <code class="docutils literal notranslate"><span class="pre">__launch_bounds__</span></code> qualifier (see <a class="reference internal" href="#register-pressure"><span class="std std-ref">Register Pressure</span></a>).</p>
<p>For purposes of calculating occupancy, the number of registers used 
by each thread is one of the key factors. For example, on devices of <a class="reference internal" href="#cuda-compute-capability"><span class="std std-ref">CUDA Compute Capability</span></a>
 7.0 each multiprocessor has 65,536 32-bit registers and can have a 
maximum of 2048 simultaneous threads resident (64 warps x 32 threads per
 warp). This means that in one of these devices, for a multiprocessor to
 have 100% occupancy, each thread can use at most 32 registers. However,
 this approach of determining how register count affects occupancy does 
not take into account the register allocation granularity. For example, 
on a device of compute capability 7.0, a kernel with 128-thread blocks 
using 37 registers per thread results in an occupancy of 75% with 12 
active 128-thread blocks per multi-processor, whereas a kernel with 
320-thread blocks using the same 37 registers per thread results in an 
occupancy of 63% because only four 320-thread blocks can reside on a 
multiprocessor. Furthermore, register allocations are rounded up to the 
nearest 256 registers per warp.</p>
<p>The number of registers available, the maximum number of simultaneous
 threads resident on each multiprocessor, and the register allocation 
granularity vary over different
compute capabilities. Because of these nuances in register allocation 
and the fact that a multiprocessor’s shared memory is also partitioned 
between resident thread blocks,
the exact relationship between register usage and occupancy can be 
difficult to determine. The <code class="docutils literal notranslate"><span class="pre">--ptxas</span> <span class="pre">options=v</span></code> option of <code class="docutils literal notranslate"><span class="pre">nvcc</span></code>
 details the number of registers
used per thread for each kernel. See Hardware Multithreading of the CUDA
 C++ Programming Guide for the register allocation formulas for devices 
of various compute
capabilities and Features and Technical Specifications of the CUDA C++ 
Programming Guide for the total number of registers available on those 
devices. Alternatively,
NVIDIA provides an occupancy calculator as part of Nsight Compute; refer
 to <a class="reference external" href="https://docs.nvidia.com/nsight-compute/NsightCompute/index.html#occupancy-calculator">https://docs.nvidia.com/nsight-compute/NsightCompute/index.html#occupancy-calculator</a>.</p>
<figure class="align-center" id="cuda-occupancy-calculator-usage-project-gpu-multi-occupancy-figure">
<img alt="Using the CUDA Occupancy Calculator to project GPU multiprocessor occupancy" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/using-cuda-occupancy-calculator-usage.png">
<figcaption>
<p><span class="caption-number">Figure 15 </span><span class="caption-text">Using the CUDA Occupancy Calculator to project GPU multiprocessor occupancy</span><a class="headerlink" href="#cuda-occupancy-calculator-usage-project-gpu-multi-occupancy-figure" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>An application can also use the Occupancy API from the CUDA Runtime, e.g. <code class="docutils literal notranslate"><span class="pre">cudaOccupancyMaxActiveBlocksPerMultiprocessor</span></code>, to dynamically select launch configurations based on runtime parameters.</p>
</section>
</section>
<section id="hiding-register-dependencies">
<span id="id51"></span><h2>
<span class="section-number">10.2. </span>Hiding Register Dependencies<a class="headerlink" href="#hiding-register-dependencies" title="Permalink to this headline"></a>
</h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Medium Priority:</strong> To hide latency arising from 
register dependencies, maintain sufficient numbers of active threads per
 multiprocessor (i.e., sufficient occupancy).</p>
</div>
<p>Register dependencies arise when an instruction uses a result stored 
in a register written by an instruction before it. The latency of most 
arithmetic instructions is typically 4 cycles on devices of compute 
capability 7.0. So threads must wait approximatly 4 cycles before using 
an arithmetic result. However, this latency can be completely hidden by 
the execution of threads in other warps. See <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#registers">Registers</a> for details.</p>
</section>
<section id="thread-and-block-heuristics">
<span id="id52"></span><h2>
<span class="section-number">10.3. </span>Thread and Block Heuristics<a class="headerlink" href="#thread-and-block-heuristics" title="Permalink to this headline"></a>
</h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Medium Priority:</strong> The number of threads per block 
should be a multiple of 32 threads, because this provides optimal 
computing efficiency and facilitates coalescing.</p>
</div>
<p>The dimension and size of blocks per grid and the dimension and size 
of threads per block are both important factors. The multidimensional 
aspect of these parameters allows easier mapping of multidimensional 
problems to CUDA and does not play a role in performance. As a result, 
this section discusses size but not dimension.</p>
<p>Latency hiding and occupancy depend on the number of active warps per
 multiprocessor, which is implicitly determined by the execution 
parameters along with resource (register and shared memory) constraints.
 Choosing execution parameters is a matter of striking a balance between
 latency hiding (occupancy) and resource utilization.</p>
<p>Choosing the execution configuration parameters should be done in 
tandem; however, there are certain heuristics that apply to each 
parameter individually. When choosing the first execution configuration 
parameter-the number of blocks per grid, or <em>grid size</em> - the 
primary concern is keeping the entire GPU busy. The number of blocks in a
 grid should be larger than the number of multiprocessors so that all 
multiprocessors have at least one block to execute. Furthermore, there 
should be multiple active blocks per multiprocessor so that blocks that 
aren’t waiting for a <code class="docutils literal notranslate"><span class="pre">__syncthreads()</span></code>
 can keep the hardware busy. This recommendation is subject to resource 
availability; therefore, it should be determined in the context of the 
second execution parameter - the number of threads per block, or <em>block size</em>
 - as well as shared memory usage. To scale to future devices, the 
number of blocks per kernel launch should be in the thousands.</p>
<p>When choosing the block size, it is important to remember that 
multiple concurrent blocks can reside on a multiprocessor, so occupancy 
is not determined by block size alone. In particular, a larger block 
size does not imply a higher occupancy.</p>
<p>As mentioned in <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy">Occupancy</a>,
 higher occupancy does not always equate to better performance. For 
example, improving occupancy from 66 percent to 100 percent generally 
does not translate to a similar increase in performance. A lower 
occupancy kernel will have more registers available per thread than a 
higher occupancy kernel, which may result in less register spilling to 
local memory; in particular, with a high degree of exposed 
instruction-level parallelism (ILP) it is, in some cases, possible to 
fully cover latency with a low occupancy.</p>
<p>There are many such factors involved in selecting block size, and 
inevitably some experimentation is required. However, a few rules of 
thumb should be followed:</p>
<ul class="simple">
<li><p>Threads per block should be a multiple of warp size to avoid 
wasting computation on under-populated warps and to facilitate 
coalescing.</p></li>
<li><p>A minimum of 64 threads per block should be used, and only if there are multiple concurrent blocks per multiprocessor.</p></li>
<li><p>Between 128 and 256 threads per block is a good initial range for experimentation with different block sizes.</p></li>
<li><p>Use several smaller thread blocks rather than one large thread 
block per multiprocessor if latency affects performance. This is 
particularly beneficial to kernels that frequently call <code class="docutils literal notranslate"><span class="pre">__syncthreads()</span></code>.</p></li>
</ul>
<p>Note that when a thread block allocates more registers than are 
available on a multiprocessor, the kernel launch fails, as it will when 
too much shared memory or too many threads are requested.</p>
</section>
<section id="effects-of-shared-memory">
<span id="id53"></span><h2>
<span class="section-number">10.4. </span>Effects of Shared Memory<a class="headerlink" href="#effects-of-shared-memory" title="Permalink to this headline"></a>
</h2>
<p>Shared memory can be helpful in several situations, such as helping 
to coalesce or eliminate redundant access to global memory. However, it 
also can act as a constraint on occupancy. In many cases, the amount of 
shared memory required by a kernel is related to the block size that was
 chosen, but the mapping of threads to shared memory elements does not 
need to be one-to-one. For example, it may be desirable to use a 64x64 
element shared memory array in a kernel, but because the maximum number 
of threads per block is 1024, it is not possible to launch a kernel with
 64x64 threads per block. In such cases, kernels with 32x32 or 64x16 
threads can be launched with each thread processing four elements of the
 shared memory array. The approach of using a single thread to process 
multiple elements of a shared memory array can be beneficial even if 
limits such as threads per block are not an issue. This is because some 
operations common to each element can be performed by the thread once, 
amortizing the cost over the number of shared memory elements processed 
by the thread.</p>
<p>A useful technique to determine the sensitivity of performance to 
occupancy is through experimentation with the amount of dynamically 
allocated shared memory, as specified in the third parameter of the 
execution configuration. By simply increasing this parameter (without 
modifying the kernel), it is possible to effectively reduce the 
occupancy of the kernel and measure its effect on performance.</p>
</section>
<section id="concurrent-kernel-execution">
<span id="id54"></span><h2>
<span class="section-number">10.5. </span>Concurrent Kernel Execution<a class="headerlink" href="#concurrent-kernel-execution" title="Permalink to this headline"></a>
</h2>
<p>As described in <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-transfers-and-overlapping-transfers-with-computation">Asynchronous and Overlapping Transfers with Computation</a>,
 CUDA streams can be used to overlap kernel execution with data 
transfers. On devices that are capable of concurrent kernel execution, 
streams can also be used to execute multiple kernels simultaneously to 
more fully take advantage of the device’s multiprocessors. Whether a 
device has this capability is indicated by the <code class="docutils literal notranslate"><span class="pre">concurrentKernels</span></code> field of the <code class="docutils literal notranslate"><span class="pre">cudaDeviceProp</span></code> structure (or listed in the output of the <code class="docutils literal notranslate"><span class="pre">deviceQuery</span></code>
 CUDA Sample). Non-default streams (streams other than stream 0) are 
required for concurrent execution because kernel calls that use the 
default stream begin only after all preceding calls on the device (in 
any stream) have completed, and no operation on the device (in any 
stream) commences until they are finished.</p>
<p>The following example illustrates the basic technique. Because <code class="docutils literal notranslate"><span class="pre">kernel1</span></code> and <code class="docutils literal notranslate"><span class="pre">kernel2</span></code> are executed in different, non-default streams, a capable device can execute the kernels at the same time.</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell21"><span></span><span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream1</span><span class="p">);</span><span class="w"></span>
<span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream2</span><span class="p">);</span><span class="w"></span>
<span class="n">kernel1</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream1</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">data_1</span><span class="p">);</span><span class="w"></span>
<span class="n">kernel2</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream2</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">data_2</span><span class="p">);</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell21">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
</section>
<section id="multiple-contexts">
<span id="id55"></span><h2>
<span class="section-number">10.6. </span>Multiple contexts<a class="headerlink" href="#multiple-contexts" title="Permalink to this headline"></a>
</h2>
<p>CUDA work occurs within a process space for a particular GPU known as a <em>context</em>.
 The context encapsulates kernel launches and memory allocations for 
that GPU as well as supporting constructs such as the page tables. The 
context is explicit in the CUDA Driver API but is entirely implicit in 
the CUDA Runtime API, which creates and manages contexts automatically.</p>
<p>With the CUDA Driver API, a CUDA application process can potentially 
create more than one context for a given GPU. If multiple CUDA 
application processes access the same GPU concurrently, this almost 
always implies multiple contexts, since a context is tied to a 
particular host process unless <a class="reference external" href="https://docs.nvidia.com/deploy/mps/index.html">Multi-Process Service</a> is in use.</p>
<p>While multiple contexts (and their associated resources such as 
global memory allocations) can be allocated concurrently on a given GPU,
 only one of these contexts can execute work at any given moment on that
 GPU; contexts sharing the same GPU are time-sliced. Creating additional
 contexts incurs memory overhead for per-context data and time overhead 
for context switching. Furthermore, the need for context switching can 
reduce utilization when work from several contexts could otherwise 
execute concurrently (see also <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#concurrent-kernel-execution">Concurrent Kernel Execution</a>).</p>
<p>Therefore, it is best to avoid multiple contexts per GPU within the 
same CUDA application. To assist with this, the CUDA Driver API provides
 methods to access and manage a special context on each GPU called the <em>primary context</em>. These are the same contexts used implicitly by the CUDA Runtime when there is not already a current context for a thread.</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell22"><span></span><span class="c1">// When initializing the program/library</span>
<span class="n">CUcontext</span><span class="w"> </span><span class="n">ctx</span><span class="p">;</span><span class="w"></span>
<span class="n">cuDevicePrimaryCtxRetain</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ctx</span><span class="p">,</span><span class="w"> </span><span class="n">dev</span><span class="p">);</span><span class="w"></span>

<span class="c1">// When the program/library launches work</span>
<span class="n">cuCtxPushCurrent</span><span class="p">(</span><span class="n">ctx</span><span class="p">);</span><span class="w"></span>
<span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="p">...</span><span class="o">&gt;&gt;&gt;</span><span class="p">(...);</span><span class="w"></span>
<span class="n">cuCtxPopCurrent</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ctx</span><span class="p">);</span><span class="w"></span>

<span class="c1">// When the program/library is finished with the context</span>
<span class="n">cuDevicePrimaryCtxRelease</span><span class="p">(</span><span class="n">dev</span><span class="p">);</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell22">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>NVIDIA-SMI can be used to configure a GPU for <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-modes">exclusive process mode</a>,
 which limits the number of contexts per GPU to one. This context can be
 current to as many threads as desired within the creating process, and <code class="docutils literal notranslate"><span class="pre">cuDevicePrimaryCtxRetain</span></code> will fail if a non-primary context that was created with the CUDA driver API already exists on the device.</p>
</div>
</section>
</section>
<section id="instruction-optimization">
<span id="id56"></span><h1>
<span class="section-number">11. </span>Instruction Optimization<a class="headerlink" href="#instruction-optimization" title="Permalink to this headline"></a>
</h1>
<p>Awareness of how instructions are executed often permits low-level 
optimizations that can be useful, especially in code that is run 
frequently (the so-called hot spot in a program). Best practices suggest
 that this optimization be performed after all higher-level 
optimizations have been completed.</p>
<section id="arithmetic-instructions">
<span id="id57"></span><h2>
<span class="section-number">11.1. </span>Arithmetic Instructions<a class="headerlink" href="#arithmetic-instructions" title="Permalink to this headline"></a>
</h2>
<p>Single-precision floats provide the best performance, and their use 
is highly encouraged. The throughput of individual arithmetic operations
 is detailed in the <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA C++ Programming Guide</a>.</p>
<section id="division-modulo-operations">
<span id="division-and-modulo-operations"></span><h3>
<span class="section-number">11.1.1. </span>Division Modulo Operations<a class="headerlink" href="#division-modulo-operations" title="Permalink to this headline"></a>
</h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Low Priority:</strong> Use shift operations to avoid expensive division and modulo calculations.</p>
</div>
<p>Integer division and modulo operations are particularly costly and 
should be avoided or replaced with bitwise operations whenever possible:
 If <span class="math notranslate nohighlight"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 119.5%; position: relative;" tabindex="0" ctxtmenu_counter="6"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container></span> is a power of 2, ( <span class="math notranslate nohighlight"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 119.5%; position: relative;" tabindex="0" ctxtmenu_counter="7"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>i</mi><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mi>n</mi></math></mjx-assistive-mml></mjx-container></span> ) is equivalent to ( <span class="math notranslate nohighlight"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 119.5%; position: relative;" tabindex="0" ctxtmenu_counter="8"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c226B"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>i</mi><mo>≫</mo><mrow data-mjx-texclass="ORD"><mi>l</mi><mi>o</mi><mi>g</mi><mn>2</mn></mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></span> ) and ( <span class="math notranslate nohighlight"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 119.5%; position: relative;" tabindex="0" ctxtmenu_counter="9"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>i</mi><mi mathvariant="normal">%</mi><mi>n</mi></math></mjx-assistive-mml></mjx-container></span> ) is equivalent to ( <span class="math notranslate nohighlight"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" style="font-size: 119.5%; position: relative;" tabindex="0" ctxtmenu_counter="10"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c class="mjx-c26"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>i</mi><mi mathvariant="normal">&amp;</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>n</mi><mo>−</mo><mn>1</mn></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></span> ).</p>
<p>The compiler will perform these conversions if n is literal. (For further information, refer to Performance Guidelines in the <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA C++ Programming Guide</a>).</p>
</section>
<section id="loop-counters-signed-vs-unsigned">
<span id="id58"></span><h3>
<span class="section-number">11.1.2. </span>Loop Counters Signed vs. Unsigned<a class="headerlink" href="#loop-counters-signed-vs-unsigned" title="Permalink to this headline"></a>
</h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Low Medium Priority:</strong> Use signed integers rather than unsigned integers as loop counters.</p>
</div>
<p>In the C language standard, unsigned integer overflow semantics are 
well defined, whereas signed integer overflow causes undefined results. 
Therefore, the compiler can optimize more aggressively with signed 
arithmetic than it can with unsigned arithmetic. This is of particular 
note with loop counters: since it is common for loop counters to have 
values that are always positive, it may be tempting to declare the 
counters as unsigned. For slightly better performance, however, they 
should instead be declared as signed.</p>
<p>For example, consider the following code:</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell23"><span></span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">in</span><span class="p">[</span><span class="n">offset</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">stride</span><span class="o">*</span><span class="n">i</span><span class="p">];</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell23">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>Here, the sub-expression <code class="docutils literal notranslate"><span class="pre">stride*i</span></code> could overflow a 32-bit integer, so if <code class="docutils literal notranslate"><span class="pre">i</span></code>
 is declared as unsigned, the overflow semantics prevent the compiler 
from using some optimizations that might otherwise have applied, such as
 strength reduction. If instead <code class="docutils literal notranslate"><span class="pre">i</span></code> is declared as signed, where the overflow semantics are undefined, the compiler has more leeway to use these optimizations.</p>
</section>
<section id="reciprocal-square-root">
<span id="id59"></span><h3>
<span class="section-number">11.1.3. </span>Reciprocal Square Root<a class="headerlink" href="#reciprocal-square-root" title="Permalink to this headline"></a>
</h3>
<p>The reciprocal square root should always be invoked explicitly as <code class="docutils literal notranslate"><span class="pre">rsqrtf()</span></code> for single precision and <code class="docutils literal notranslate"><span class="pre">rsqrt()</span></code> for double precision. The compiler optimizes <code class="docutils literal notranslate"><span class="pre">1.0f/sqrtf(x)</span></code> into <code class="docutils literal notranslate"><span class="pre">rsqrtf()</span></code> only when this does not violate IEEE-754 semantics.</p>
</section>
<section id="other-arithmetic-instructions">
<span id="id60"></span><h3>
<span class="section-number">11.1.4. </span>Other Arithmetic Instructions<a class="headerlink" href="#other-arithmetic-instructions" title="Permalink to this headline"></a>
</h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Low Priority:</strong> Avoid automatic conversion of doubles to floats.</p>
</div>
<p>The compiler must on occasion insert conversion instructions, introducing additional execution cycles. This is the case for:</p>
<ul class="simple">
<li><p>Functions operating on <code class="docutils literal notranslate"><span class="pre">char</span></code> or <code class="docutils literal notranslate"><span class="pre">short</span></code> whose operands generally need to be converted to an <code class="docutils literal notranslate"><span class="pre">int</span></code></p></li>
<li><p>Double-precision floating-point constants (defined without any 
type suffix) used as input to single-precision floating-point 
computations</p></li>
</ul>
<p>The latter case can be avoided by using single-precision floating-point constants, defined with an <code class="docutils literal notranslate"><span class="pre">f</span></code> suffix such as <code class="docutils literal notranslate"><span class="pre">3.141592653589793f</span></code>, <code class="docutils literal notranslate"><span class="pre">1.0f</span></code>, <code class="docutils literal notranslate"><span class="pre">0.5f</span></code>.</p>
<p>For single-precision code, use of the float type and the single-precision math functions are highly recommended.</p>
<p>It should also be noted that the CUDA math library’s complementary error function, <code class="docutils literal notranslate"><span class="pre">erfcf()</span></code>, is particularly fast with full single-precision accuracy.</p>
</section>
<section id="exponentiation-with-small-fractional-arguments">
<span id="exponentiation-small-fractions"></span><h3>
<span class="section-number">11.1.5. </span>Exponentiation With Small Fractional Arguments<a class="headerlink" href="#exponentiation-with-small-fractional-arguments" title="Permalink to this headline"></a>
</h3>
<p>For some fractional exponents, exponentiation can be accelerated significantly compared to the use of <code class="docutils literal notranslate"><span class="pre">pow()</span></code>
 by using square roots, cube roots, and their inverses. For those 
exponentiations where the exponent is not exactly representable as a 
floating-point number, such as 1/3, this can also provide much more 
accurate results, as use of <code class="docutils literal notranslate"><span class="pre">pow()</span></code> magnifies the initial representational error.</p>
<p>The formulas in the table below are valid for <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">&gt;=</span> <span class="pre">0,</span> <span class="pre">x</span> <span class="pre">!=</span> <span class="pre">-0</span></code>, that is, <code class="docutils literal notranslate"><span class="pre">signbit(x)</span> <span class="pre">==</span> <span class="pre">0</span></code>.</p>
<div class="wy-table-responsive"><table class="table-no-stripes docutils align-default" id="id85">
<caption>
<span class="caption-number">Table 5 </span><span class="caption-text">Formulae for exponentiation by small fractions</span><a class="headerlink" href="#id85" title="Permalink to this table"></a>
</caption>
<colgroup>
<col style="width: 31%">
<col style="width: 69%">
</colgroup>
<thead>
<tr class="row-odd">
<th class="head"><p>Computation</p></th>
<th class="head"><p>Formula</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even">
<td><p>x<sup>1/9</sup></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">rcbrt(rcbrt(x))</span></code></p></td>
</tr>
<tr class="row-odd">
<td><p>x<sup>-1/9</sup></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">cbrt(rcbrt(x))</span></code></p></td>
</tr>
<tr class="row-even">
<td><p>x<sup>1/6</sup></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">rcbrt(rsqrt(x))</span></code></p></td>
</tr>
<tr class="row-odd">
<td><p>x<sup>-1/6</sup></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">rcbrt(sqrt(x))</span></code></p></td>
</tr>
<tr class="row-even">
<td><p>x<sup>1/4</sup></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">rsqrt(rsqrt(x))</span></code></p></td>
</tr>
<tr class="row-odd">
<td><p>x<sup>-1/4</sup></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">sqrt(rsqrt(x))</span></code></p></td>
</tr>
<tr class="row-even">
<td><p>x<sup>1/3</sup></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">cbrt(x)</span></code></p></td>
</tr>
<tr class="row-odd">
<td><p>x<sup>-1/3</sup></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">rcbrt(x)</span></code></p></td>
</tr>
<tr class="row-even">
<td><p>x<sup>1/2</sup></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">sqrt(x)</span></code></p></td>
</tr>
<tr class="row-odd">
<td><p>x<sup>-1/2</sup></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">rsqrt(x)</span></code></p></td>
</tr>
<tr class="row-even">
<td><p>x<sup>2/3</sup></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">cbrt(x);</span> <span class="pre">r</span> <span class="pre">=</span> <span class="pre">r*r</span></code></p></td>
</tr>
<tr class="row-odd">
<td><p>x<sup>-2/3</sup></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">rcbrt(x);</span> <span class="pre">r</span> <span class="pre">=</span> <span class="pre">r*r</span></code></p></td>
</tr>
<tr class="row-even">
<td><p>x<sup>3/4</sup></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">sqrt(x);</span> <span class="pre">r</span> <span class="pre">=</span> <span class="pre">r*sqrt(r)</span></code></p></td>
</tr>
<tr class="row-odd">
<td><p>x<sup>-3/4</sup></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">rsqrt(x);</span> <span class="pre">r</span> <span class="pre">=</span> <span class="pre">r*sqrt(r)</span></code></p></td>
</tr>
<tr class="row-even">
<td><p>x<sup>7/6</sup></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">x*rcbrt(rsqrt(x))</span></code></p></td>
</tr>
<tr class="row-odd">
<td><p>x<sup>-7/6</sup></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">(1/x)</span> <span class="pre">*</span> <span class="pre">rcbrt(sqrt(x))</span></code></p></td>
</tr>
<tr class="row-even">
<td><p>x<sup>5/4</sup></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">x*rsqrt(rsqrt(x))</span></code></p></td>
</tr>
<tr class="row-odd">
<td><p>x<sup>-5/4</sup></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">(1/x)*sqrt(rsqrt(x))</span></code></p></td>
</tr>
<tr class="row-even">
<td><p>x<sup>4/3</sup></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">x*cbrt(x)</span></code></p></td>
</tr>
<tr class="row-odd">
<td><p>x<sup>-4/3</sup></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">(1/x)*rcbrt(x)</span></code></p></td>
</tr>
<tr class="row-even">
<td><p>x<sup>3/2</sup></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">x*sqrt(x)</span></code></p></td>
</tr>
<tr class="row-odd">
<td><p>x<sup>-3/2</sup></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">(1/x)*rsqrt(x)</span></code></p></td>
</tr>
</tbody>
</table></div>
</section>
<section id="math-libraries">
<span id="id61"></span><h3>
<span class="section-number">11.1.6. </span>Math Libraries<a class="headerlink" href="#math-libraries" title="Permalink to this headline"></a>
</h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Medium Priority:</strong> Use the fast math library whenever speed trumps precision.</p>
</div>
<p>Two types of runtime math operations are supported. They can be 
distinguished by their names: some have names with prepended 
underscores, whereas others do not (e.g., <code class="docutils literal notranslate"><span class="pre">__functionName()</span></code> versus <code class="docutils literal notranslate"><span class="pre">functionName()</span></code>). Functions following the <code class="docutils literal notranslate"><span class="pre">__functionName()</span></code> naming convention map directly to the hardware level. They are faster but provide somewhat lower accuracy (e.g., <code class="docutils literal notranslate"><span class="pre">__sinf(x)</span></code> and <code class="docutils literal notranslate"><span class="pre">__expf(x)</span></code>). Functions following <code class="docutils literal notranslate"><span class="pre">functionName()</span></code> naming convention are slower but have higher accuracy (e.g., <code class="docutils literal notranslate"><span class="pre">sinf(x)</span></code> and <code class="docutils literal notranslate"><span class="pre">expf(x)</span></code>). The throughput of <code class="docutils literal notranslate"><span class="pre">__sinf(x)</span></code>, <code class="docutils literal notranslate"><span class="pre">__cosf(x)</span></code>, and<code class="docutils literal notranslate"><span class="pre">__expf(x)</span></code> is much greater than that of <code class="docutils literal notranslate"><span class="pre">sinf(x)</span></code>, <code class="docutils literal notranslate"><span class="pre">cosf(x)</span></code>, and <code class="docutils literal notranslate"><span class="pre">expf(x)</span></code>. The latter become even more expensive (about an order of magnitude slower) if the magnitude of the argument <code class="docutils literal notranslate"><span class="pre">x</span></code>
 needs to be reduced. Moreover, in such cases, the argument-reduction 
code uses local memory, which can affect performance even more because 
of the high latency of local memory. More details are available in the <em>CUDA C++ Programming Guide</em>.</p>
<p>Note also that whenever sine and cosine of the same argument are computed, the <code class="docutils literal notranslate"><span class="pre">sincos</span></code> family of instructions should be used to optimize performance:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">__sincosf()</span></code> for single-precision fast math (see next paragraph)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sincosf()</span></code> for regular single-precision</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sincos()</span></code> for double precision</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">-use_fast_math</span></code> compiler option of <code class="docutils literal notranslate"><span class="pre">nvcc</span></code> coerces every <code class="docutils literal notranslate"><span class="pre">functionName()</span></code> call to the equivalent <code class="docutils literal notranslate"><span class="pre">__functionName()</span></code>
 call. It also disables single-precision denormal support and lowers the
 precision of single-precision division in general. This is an 
aggressive optimization that can both reduce numerical accuracy and 
alter special case handling. A more robust approach is to selectively 
introduce calls to fast intrinsic functions only if merited by 
performance gains and where altered behavior can be tolerated. Note this
 switch is effective only on single-precision floating point.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Medium Priority:</strong> Prefer faster, more specialized math functions over slower, more general ones when possible.</p>
</div>
<p>For small integer powers (e.g., <em>x2</em> or <em>x3</em>), explicit multiplication is almost certainly faster than the use of general exponentiation routines such as <code class="docutils literal notranslate"><span class="pre">pow()</span></code>.
 While compiler optimization improvements continually seek to narrow 
this gap, explicit multiplication (or the use of an equivalent 
purpose-built inline function or macro) can have a significant 
advantage. This advantage is increased when several powers of the same 
base are needed (e.g., where both <em>x2</em> and <em>x5</em> are calculated in close proximity), as this aids the compiler in its common sub-expression elimination (CSE) optimization.</p>
<p>For exponentiation using base 2 or 10, use the functions <code class="docutils literal notranslate"><span class="pre">exp2()</span></code> or <code class="docutils literal notranslate"><span class="pre">expf2()</span></code> and <code class="docutils literal notranslate"><span class="pre">exp10()</span></code> or <code class="docutils literal notranslate"><span class="pre">expf10()</span></code> rather than the functions <code class="docutils literal notranslate"><span class="pre">pow()</span></code> or <code class="docutils literal notranslate"><span class="pre">powf()</span></code>. Both <code class="docutils literal notranslate"><span class="pre">pow()</span></code> and <code class="docutils literal notranslate"><span class="pre">powf()</span></code>
 are heavy-weight functions in terms of register pressure and 
instruction count due to the numerous special cases arising in general 
exponentiation and the difficulty of achieving good accuracy across the 
entire ranges of the base and the exponent. The functions <code class="docutils literal notranslate"><span class="pre">exp2()</span></code>, <code class="docutils literal notranslate"><span class="pre">exp2f()</span></code>, <code class="docutils literal notranslate"><span class="pre">exp10()</span></code>, and <code class="docutils literal notranslate"><span class="pre">exp10f()</span></code>, on the other hand, are similar to <code class="docutils literal notranslate"><span class="pre">exp()</span></code> and <code class="docutils literal notranslate"><span class="pre">expf()</span></code> in terms of performance, and can be as much as ten times faster than their <code class="docutils literal notranslate"><span class="pre">pow()</span></code>/<code class="docutils literal notranslate"><span class="pre">powf()</span></code> equivalents.</p>
<p>For exponentiation with an exponent of 1/3, use the <code class="docutils literal notranslate"><span class="pre">cbrt()</span></code> or <code class="docutils literal notranslate"><span class="pre">cbrtf()</span></code> function rather than the generic exponentiation functions <code class="docutils literal notranslate"><span class="pre">pow()</span></code> or <code class="docutils literal notranslate"><span class="pre">powf()</span></code>, as the former are significantly faster than the latter. Likewise, for exponentation with an exponent of -1/3, use <code class="docutils literal notranslate"><span class="pre">rcbrt()</span></code> or <code class="docutils literal notranslate"><span class="pre">rcbrtf()</span></code>.</p>
<p>Replace <code class="docutils literal notranslate"><span class="pre">sin(π*&lt;expr&gt;)</span></code> with <code class="docutils literal notranslate"><span class="pre">sinpi(&lt;expr&gt;)</span></code>, <code class="docutils literal notranslate"><span class="pre">cos(π*&lt;expr&gt;)</span></code> with <code class="docutils literal notranslate"><span class="pre">cospi(&lt;expr&gt;)</span></code>, and <code class="docutils literal notranslate"><span class="pre">sincos(π*&lt;expr&gt;)</span></code> with <code class="docutils literal notranslate"><span class="pre">sincospi(&lt;expr&gt;)</span></code>.
 This is advantageous with regard to both accuracy and performance. As a
 particular example, to evaluate the sine function in degrees instead of
 radians, use <code class="docutils literal notranslate"><span class="pre">sinpi(x/180.0)</span></code>. Similarly, the single-precision functions <code class="docutils literal notranslate"><span class="pre">sinpif()</span></code>, <code class="docutils literal notranslate"><span class="pre">cospif()</span></code>, and <code class="docutils literal notranslate"><span class="pre">sincospif()</span></code> should replace calls to <code class="docutils literal notranslate"><span class="pre">sinf()</span></code>, <code class="docutils literal notranslate"><span class="pre">cosf()</span></code>, and <code class="docutils literal notranslate"><span class="pre">sincosf()</span></code> when the function argument is of the form <code class="docutils literal notranslate"><span class="pre">π*&lt;expr&gt;</span></code>. (The performance advantage <code class="docutils literal notranslate"><span class="pre">sinpi()</span></code> has over <code class="docutils literal notranslate"><span class="pre">sin()</span></code> is due to simplified argument reduction; the accuracy advantage is because <code class="docutils literal notranslate"><span class="pre">sinpi()</span></code> multiplies by <code class="docutils literal notranslate"><span class="pre">π</span></code> only implicitly, effectively using an infinitely precise mathematical <code class="docutils literal notranslate"><span class="pre">π</span></code> rather than a single- or double-precision approximation thereof.)</p>
</section>
<section id="precision-related-compiler-flags">
<span id="id62"></span><h3>
<span class="section-number">11.1.7. </span>Precision-related Compiler Flags<a class="headerlink" href="#precision-related-compiler-flags" title="Permalink to this headline"></a>
</h3>
<p>By default, the <code class="docutils literal notranslate"><span class="pre">nvcc</span></code> compiler generates IEEE-compliant code, but it also provides options to generate code that somewhat less accurate but faster:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-ftz=true</span></code> (denormalized numbers are flushed to zero)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-prec-div=false</span></code> (less precise division)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-prec-sqrt=false</span></code> (less precise square root)</p></li>
</ul>
<p>Another, more aggressive, option is <code class="docutils literal notranslate"><span class="pre">-use_fast_math</span></code>, which coerces every <code class="docutils literal notranslate"><span class="pre">functionName()</span></code> call to the equivalent <code class="docutils literal notranslate"><span class="pre">__functionName()</span></code> call. This makes the code run faster at the cost of diminished precision and accuracy. See <a class="reference internal" href="#math-libraries"><span class="std std-ref">Math Libraries</span></a>.</p>
</section>
</section>
<section id="memory-instructions">
<span id="id63"></span><h2>
<span class="section-number">11.2. </span>Memory Instructions<a class="headerlink" href="#memory-instructions" title="Permalink to this headline"></a>
</h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>High Priority:</strong> Minimize the use of global memory. Prefer shared memory access where possible.</p>
</div>
<p>Memory instructions include any instruction that reads from or writes
 to shared, local, or global memory. When accessing uncached local or 
global memory, there are hundreds of clock cycles of memory latency.</p>
<p>As an example, the assignment operator in the following sample code 
has a high throughput, but, crucially, there is a latency of hundreds of
 clock cycles to read data from global memory:</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell24"><span></span><span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">shared</span><span class="p">[</span><span class="mi">32</span><span class="p">];</span><span class="w"></span>
<span class="n">__device__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">device</span><span class="p">[</span><span class="mi">32</span><span class="p">];</span><span class="w"></span>
<span class="n">shared</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">device</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell24">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>Much of this global memory latency can be hidden by the thread 
scheduler if there are sufficient independent arithmetic instructions 
that can be issued while waiting for the global memory access to 
complete. However, it is best to avoid accessing global memory whenever 
possible.</p>
</section>
</section>
<section id="control-flow">
<h1>
<span class="section-number">12. </span>Control Flow<a class="headerlink" href="#control-flow" title="Permalink to this headline"></a>
</h1>
<section id="branching-and-divergence">
<h2>
<span class="section-number">12.1. </span>Branching and Divergence<a class="headerlink" href="#branching-and-divergence" title="Permalink to this headline"></a>
</h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>High Priority:</strong> Avoid different execution paths within the same warp.</p>
</div>
<p>Flow control instructions (<code class="docutils literal notranslate"><span class="pre">if</span></code>, <code class="docutils literal notranslate"><span class="pre">switch</span></code>, <code class="docutils literal notranslate"><span class="pre">do</span></code>, <code class="docutils literal notranslate"><span class="pre">for</span></code>, <code class="docutils literal notranslate"><span class="pre">while</span></code>)
 can significantly affect the instruction throughput by causing threads 
of the same warp to diverge; that is, to follow different execution 
paths. If this happens, the different execution paths must be executed 
separately; this increases the total number of instructions executed for
 this warp.</p>
<p>To obtain best performance in cases where the control flow depends on
 the thread ID, the controlling condition should be written so as to 
minimize the number of divergent warps.</p>
<p>This is possible because the distribution of the warps across the 
block is deterministic as mentioned in SIMT Architecture of the CUDA C++
 Programming Guide. A trivial example is when the controlling condition 
depends only on (<code class="docutils literal notranslate"><span class="pre">threadIdx</span></code> / <code class="docutils literal notranslate"><span class="pre">WSIZE</span></code>) where <code class="docutils literal notranslate"><span class="pre">WSIZE</span></code> is the warp size.</p>
<p>In this case, no warp diverges because the controlling condition is perfectly aligned with the warps.</p>
<p>For branches including just a few instructions, warp divergence 
generally results in marginal performance losses. For example, the 
compiler may use predication to avoid an actual branch. Instead, all 
instructions are scheduled, but a per-thread condition code or predicate
 controls which threads execute the instructions. Threads with a false 
predicate do not write results, and also do not evaluate addresses or 
read operands.</p>
<p>Starting with the Volta architecture, Independent Thread Scheduling 
allows a warp to remain diverged outside of the data-dependent 
conditional block. An explicit <code class="docutils literal notranslate"><span class="pre">__syncwarp()</span></code> can be used to guarantee that the warp has reconverged for subsequent instructions.</p>
</section>
<section id="branch-predication">
<h2>
<span class="section-number">12.2. </span>Branch Predication<a class="headerlink" href="#branch-predication" title="Permalink to this headline"></a>
</h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Low Priority:</strong> Make it easy for the compiler to use branch predication in lieu of loops or control statements.</p>
</div>
<p>Sometimes, the compiler may unroll loops or optimize out <code class="docutils literal notranslate"><span class="pre">if</span></code> or <code class="docutils literal notranslate"><span class="pre">switch</span></code>
 statements by using branch predication instead. In these cases, no warp
 can ever diverge. The programmer can also control loop unrolling using</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell25"><span></span><span class="cp">#pragma unroll</span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell25">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>For more information on this pragma, refer to the CUDA C++ Programming Guide.</p>
<p>When using branch predication, none of the instructions whose 
execution depends on the controlling condition is skipped. Instead, each
 such instruction is associated with a per-thread condition code or 
predicate that is set to true or false according to the controlling 
condition. Although each of these instructions is scheduled for 
execution, only the instructions with a true predicate are actually 
executed. Instructions with a false predicate do not write results, and 
they also do not evaluate addresses or read operands.</p>
<p>The compiler replaces a branch instruction with predicated 
instructions only if the number of instructions controlled by the branch
 condition is less than or equal to a certain threshold.</p>
</section>
</section>
<section id="deploying-cuda-applications">
<h1>
<span class="section-number">13. </span>Deploying CUDA Applications<a class="headerlink" href="#deploying-cuda-applications" title="Permalink to this headline"></a>
</h1>
<p>Having completed the GPU acceleration of one or more components of 
the application it is possible to compare the outcome with the original 
expectation. Recall that the initial <em>assess</em> step allowed the developer to determine an upper bound for the potential speedup attainable by accelerating given hotspots.</p>
<p>Before tackling other hotspots to improve the total speedup, the 
developer should consider taking the partially parallelized 
implementation and carry it through to production. This is important for
 a number of reasons; for example, it allows the user to profit from 
their investment as early as possible (the speedup may be partial but is
 still valuable), and it minimizes risk for the developer and the user 
by providing an evolutionary rather than revolutionary set of changes to
 the application.</p>
</section>
<section id="understanding-the-programming-environment">
<span id="understanding-programming-environment"></span><h1>
<span class="section-number">14. </span>Understanding the Programming Environment<a class="headerlink" href="#understanding-the-programming-environment" title="Permalink to this headline"></a>
</h1>
<p>With each generation of NVIDIA processors, new features are added to 
the GPU that CUDA can leverage. Consequently, it’s important to 
understand the characteristics of the architecture.</p>
<p>Programmers should be aware of two version numbers. The first is the <a class="reference internal" href="#cuda-compute-capability"><span class="std std-ref">compute capability</span></a>, and the second is the version number of the CUDA Runtime and CUDA Driver APIs.</p>
<section id="cuda-compute-capability">
<span id="id64"></span><h2>
<span class="section-number">14.1. </span>CUDA Compute Capability<a class="headerlink" href="#cuda-compute-capability" title="Permalink to this headline"></a>
</h2>
<p>The <em>compute capability</em> describes the features of the 
hardware and reflects the set of instructions supported by the device as
 well as other specifications, such as the maximum number of threads per
 block and the number of registers per multiprocessor. Higher compute 
capability versions are supersets of lower (that is, earlier) versions, 
so they are backward compatible.</p>
<p>The compute capability of the GPU in the device can be queried programmatically as illustrated in the <code class="docutils literal notranslate"><span class="pre">deviceQuery</span></code> CUDA Sample. The output for that program is shown in <a class="reference internal" href="#sample-cuda-configuration-data-reported-devicequery-figure"><span class="std std-ref">Figure 16</span></a>. This information is obtained by calling <code class="docutils literal notranslate"><span class="pre">cudaGetDeviceProperties()</span></code> and accessing the information in the structure it returns.</p>
<figure class="align-center" id="sample-cuda-configuration-data-reported-devicequery-figure">
<a class="reference internal image-reference" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/_images/sample-cuda-configuration-data.png"><img alt="Sample CUDA configuration data reported by deviceQuery" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/sample-cuda-configuration-data.png" style="width: 800px;"></a>
<figcaption>
<p><span class="caption-number">Figure 16 </span><span class="caption-text">Sample CUDA configuration data reported by deviceQuery</span><a class="headerlink" href="#sample-cuda-configuration-data-reported-devicequery-figure" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>The major and minor revision numbers of the compute capability are shown on the seventh line of <a class="reference internal" href="#sample-cuda-configuration-data-reported-devicequery-figure"><span class="std std-ref">Figure 16</span></a>. Device 0 of this system has compute capability 7.0.</p>
<p>More details about the compute capabilities of various GPUs are in 
CUDA-Enabled GPUs and Compute Capabilities of the CUDA C++ Programming 
Guide. In particular, developers should note the number of 
multiprocessors on the device, the number of registers and the amount of
 memory available, and any special capabilities of the device.</p>
</section>
<section id="additional-hardware-data">
<span id="id65"></span><h2>
<span class="section-number">14.2. </span>Additional Hardware Data<a class="headerlink" href="#additional-hardware-data" title="Permalink to this headline"></a>
</h2>
<p>Certain hardware features are not described by the compute 
capability. For example, the ability to overlap kernel execution with 
asynchronous data transfers between the host and the device is available
 on most but not all GPUs irrespective of the compute capability. In 
such cases, call <code class="docutils literal notranslate"><span class="pre">cudaGetDeviceProperties()</span></code> to determine whether the device is capable of a certain feature. For example, the <code class="docutils literal notranslate"><span class="pre">asyncEngineCount</span></code>
 field of the device property structure indicates whether overlapping 
kernel execution and data transfers is possible (and, if so, how many 
concurrent transfers are possible); likewise, the <code class="docutils literal notranslate"><span class="pre">canMapHostMemory</span></code> field indicates whether zero-copy data transfers can be performed.</p>
</section>
<section id="which-compute-capability-target">
<span id="which-compute-capability-to-target"></span><h2>
<span class="section-number">14.3. </span>Which Compute Capability Target<a class="headerlink" href="#which-compute-capability-target" title="Permalink to this headline"></a>
</h2>
<p>To target specific versions of NVIDIA hardware and CUDA software, use the <code class="docutils literal notranslate"><span class="pre">-arch</span></code>, <code class="docutils literal notranslate"><span class="pre">-code</span></code>, and <code class="docutils literal notranslate"><span class="pre">-gencode</span></code> options of <code class="docutils literal notranslate"><span class="pre">nvcc</span></code>. Code that uses the warp shuffle operation, for example, must be compiled with <code class="docutils literal notranslate"><span class="pre">-arch=sm_30</span></code> (or higher compute capability).</p>
<p>See <a class="reference internal" href="#building-for-maximum-compatibility"><span class="std std-ref">Building for Maximum Compatibility</span></a> for further discussion of the flags used for building code for multiple generations of CUDA-capable device simultaneously.</p>
</section>
<section id="cuda-runtime">
<span id="id66"></span><h2>
<span class="section-number">14.4. </span>CUDA Runtime<a class="headerlink" href="#cuda-runtime" title="Permalink to this headline"></a>
</h2>
<p>The host runtime component of the CUDA software environment can be 
used only by host functions. It provides functions to handle the 
following:</p>
<ul class="simple">
<li><p>Device management</p></li>
<li><p>Context management</p></li>
<li><p>Memory management</p></li>
<li><p>Code module management</p></li>
<li><p>Execution control</p></li>
<li><p>Texture reference management</p></li>
<li><p>Interoperability with OpenGL and Direct3D</p></li>
</ul>
<p>As compared to the lower-level CUDA Driver API, the CUDA Runtime 
greatly eases device management by providing implicit initialization, 
context management, and device code module management. The C++ host code
 generated by <code class="docutils literal notranslate"><span class="pre">nvcc</span></code>
 utilizes the CUDA Runtime, so applications that link to this code will 
depend on the CUDA Runtime; similarly, any code that uses the <code class="docutils literal notranslate"><span class="pre">cuBLAS</span></code>, <code class="docutils literal notranslate"><span class="pre">cuFFT</span></code>, and other CUDA Toolkit libraries will also depend on the CUDA Runtime, which is used internally by these libraries.</p>
<p>The functions that make up the CUDA Runtime API are explained in the CUDA Toolkit Reference Manual.</p>
<p>The CUDA Runtime handles kernel loading and setting up kernel 
parameters and launch configuration before the kernel is launched. The 
implicit driver version checking, code initialization, CUDA context 
management, CUDA module management (cubin to function mapping), kernel 
configuration, and parameter passing are all performed by the CUDA 
Runtime.</p>
<p>It comprises two principal parts:</p>
<ul class="simple">
<li><p>A C-style function interface (<code class="docutils literal notranslate"><span class="pre">cuda_runtime_api.h</span></code>).</p></li>
<li><p>C++-style convenience wrappers (<code class="docutils literal notranslate"><span class="pre">cuda_runtime.h</span></code>) built on top of the C-style functions.</p></li>
</ul>
<p>For more information on the Runtime API, refer to CUDA Runtime of the CUDA C++ Programming Guide.</p>
</section>
</section>
<section id="cuda-compatibility-developer-s-guide">
<span id="cuda-compatibility-and-upgrades"></span><h1>
<span class="section-number">15. </span>CUDA Compatibility Developer’s Guide<a class="headerlink" href="#cuda-compatibility-developer-s-guide" title="Permalink to this headline"></a>
</h1>
<p>CUDA Toolkit is released on a monthly release cadence to deliver new 
features, performance improvements, and critical bug fixes. CUDA 
compatibility allows users to update the latest CUDA Toolkit software 
(including the compiler, libraries, and tools) without requiring update 
to the entire driver stack.</p>
<p>The CUDA software environment consists of three parts:</p>
<ul class="simple">
<li><p>CUDA Toolkit (libraries, CUDA runtime and developer tools) - SDK for developers to build CUDA applications.</p></li>
<li><p>CUDA driver - User-mode driver component used to run CUDA applications (e.g. libcuda.so on Linux systems).</p></li>
<li><p>NVIDIA GPU device driver - Kernel-mode driver component for NVIDIA GPUs.</p></li>
</ul>
<p>On Linux systems, the CUDA driver and kernel mode components are 
delivered together in the NVIDIA display driver package. This is shown 
in Figure 1.</p>
<figure class="align-center" id="id86">
<img alt="Components of CUDA" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/CUDA-components.png">
<figcaption>
<p><span class="caption-number">Figure 17 </span><span class="caption-text">Components of CUDA</span><a class="headerlink" href="#id86" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>The CUDA compiler (nvcc), provides a way to handle CUDA and non-CUDA 
code (by splitting and steering compilation), along with the CUDA 
runtime, is part of the CUDA compiler toolchain. The CUDA Runtime API 
provides developers with high-level C++ interface for simplified 
management of devices, kernel executions etc., While the CUDA driver API
 provides (<a class="reference external" href="https://docs.nvidia.com/cuda/cuda-driver-api/index.html">CUDA Driver API</a>) a low-level programming interface for applications to target NVIDIA hardware.</p>
<p>Built on top of these technologies are CUDA libraries, some of which 
are included in the CUDA Toolkit, while others such as cuDNN may be 
released independently of the CUDA Toolkit.</p>
<section id="cuda-toolkit-versioning">
<span id="id67"></span><h2>
<span class="section-number">15.1. </span>CUDA Toolkit Versioning<a class="headerlink" href="#cuda-toolkit-versioning" title="Permalink to this headline"></a>
</h2>
<p>Starting with CUDA 11, the toolkit versions are based on an industry-standard semantic versioning scheme: .X.Y.Z, where:</p>
<ul class="simple">
<li><p>.X stands for the major version - APIs have changed and binary compatibility is broken.</p></li>
<li><p>.Y stands for the minor version - Introduction of new APIs, 
deprecation of old APIs, and source compatibility might be broken but 
binary compatibility is maintained.</p></li>
<li><p>.Z stands for the release/patch version - new updates and patches will increment this.</p></li>
</ul>
<p>Each component in the toolkit is recommended to be semantically 
versioned. From CUDA 11.3 NVRTC is also semantically versioned. We will 
note some of them later on in the document. The versions of the 
components in the toolkit are available in this <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cuda-major-component-versions">table</a>.</p>
<p>Compatibility of the CUDA platform is thus intended to address a few scenarios:</p>
<ol class="arabic simple">
<li><p>NVIDIA driver upgrades to systems with GPUs running in production
 for enterprises or datacenters can be complex and may need advance 
planning. Delays in rolling out new NVIDIA drivers could mean that users
 of such systems may not have access to new features available in CUDA 
releases. Not requiring driver updates for new CUDA releases can mean 
that new versions of the software can be made available faster to users.</p></li>
<li><p>Many software libraries and applications built on top of CUDA 
(e.g. math libraries or deep learning frameworks) do not have a direct 
dependency on the CUDA runtime, compiler or driver. In such cases, users
 or developers can still benefit from not having to upgrade the entire 
CUDA Toolkit or driver to use these libraries or frameworks.</p></li>
<li><p>Upgrading dependencies is error-prone and time consuming, and in 
some corner cases, can even change the semantics of a program. 
Constantly recompiling with the latest CUDA Toolkit means forcing 
upgrades on the end-customers of an application product. Package 
managers facilitate this process but unexpected issues can still arise 
and if a bug is found, it necessitates a repeat of the above upgrade 
process.</p></li>
</ol>
<p>CUDA supports several compatibility choices:</p>
<ol class="arabic simple">
<li><p>First introduced in CUDA 10, the <strong>CUDA Forward Compatible Upgrade</strong>
 is designed to allow users to get access to new CUDA features and run 
applications built with new CUDA releases on systems with older 
installations of the NVIDIA datacenter driver.</p></li>
<li>
<p>First introduced in CUDA 11.1, <strong>CUDA Enhanced Compatibility</strong> provides two benefits:</p>
<ul class="simple">
<li><p>By leveraging semantic versioning across components in the CUDA 
Toolkit, an application can be built for one CUDA minor release (for 
example 11.1) and work across all future minor releases within the major
 family (i.e. 11.x).</p></li>
<li><p>The CUDA runtime has relaxed the minimum driver version check and
 thus no longer requires a driver upgrade when moving to a new minor 
release.</p></li>
</ul>
</li>
<li><p>The CUDA driver ensures backward Binary Compatibility is 
maintained for compiled CUDA applications. Applications compiled with 
CUDA toolkit versions as old as 3.2 will run on newer drivers.</p></li>
</ol>
</section>
<section id="source-compatibility">
<span id="id68"></span><h2>
<span class="section-number">15.2. </span>Source Compatibility<a class="headerlink" href="#source-compatibility" title="Permalink to this headline"></a>
</h2>
<p>We define source compatibility as a set of guarantees provided by the
 library, where a well-formed application built against a specific 
version of the library (using the SDK) will continue to build and run 
without errors when a newer version of the SDK is installed.</p>
<p>Both the CUDA driver and the CUDA runtime are not source compatible 
across the different SDK releases. APIs can be deprecated and removed. 
Therefore, an application that compiled successfully on an older version
 of the toolkit may require changes in order to compile against a newer 
version of the toolkit.</p>
<p>Developers are notified through deprecation and documentation 
mechanisms of any current or upcoming changes. This does not mean that 
application binaries compiled using an older toolkit will not be 
supported anymore. Application binaries rely on CUDA Driver API 
interface and even though the CUDA Driver API itself may also have 
changed across toolkit versions, CUDA guarantees Binary Compatibility of
 the CUDA Driver API interface.</p>
</section>
<section id="binary-compatibility">
<span id="id69"></span><h2>
<span class="section-number">15.3. </span>Binary Compatibility<a class="headerlink" href="#binary-compatibility" title="Permalink to this headline"></a>
</h2>
<p>We define binary compatibility as a set of guarantees provided by the
 library, where an application targeting the said library will continue 
to work when dynamically linked against a different version of the 
library.</p>
<p>The CUDA Driver API has a versioned C-style ABI, which guarantees 
that applications that were running against an older driver (for example
 CUDA 3.2) will still run and function correctly against a modern driver
 (for example one shipped with CUDA 11.0). This means that even though 
an application source might need to be changed if it has to be 
recompiled against a newer CUDA Toolkit in order to use the newer 
features, replacing the driver components installed in a system with a 
newer version will always support existing applications and its 
functions.</p>
<p>The CUDA Driver API thus is binary-compatible (the OS loader can pick
 up a newer version and the application continues to work) but not 
source-compatible (rebuilding your application against a newer SDK might
 require source changes).</p>
<figure class="align-center" id="id87">
<img alt="CUDA Toolkit and Minimum Driver Versions" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/CTK-and-min-driver-versions.png">
<figcaption>
<p><span class="caption-number">Figure 18 </span><span class="caption-text">CUDA Toolkit and Minimum Driver Versions</span><a class="headerlink" href="#id87" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>Before we proceed further on this topic, it’s important for 
developers to understand the concept of Minimum Driver Version and how 
that may affect them.</p>
<p>Each version of the CUDA Toolkit (and runtime) requires a minimum 
version of the NVIDIA driver. Applications compiled against a CUDA 
Toolkit version will only run on systems with the specified minimum 
driver version for that toolkit version. Prior to CUDA 11.0, the minimum
 driver version for a toolkit was the same as the driver shipped with 
that version of the CUDA Toolkit.</p>
<p>So, when an application is built with CUDA 11.0, it can only run on a
 system with an R450 or later driver. If such an application is run on a
 system with the R418 driver installed, CUDA initialization will return 
an error as can be seen in the example below.</p>
<p>In this example, the deviceQuery sample is compiled with CUDA 11.1 
and is run on a system with R418. In this scenario, CUDA initialization 
returns an error due to the minimum driver requirement.</p>
<div class="highlight-text notranslate">
<div class="highlight"><pre id="codecell26"><span></span>ubuntu@:~/samples/1_Utilities/deviceQuery
$ make
/usr/local/cuda-11.1/bin/nvcc -ccbin g++ -I../../common/inc -m64 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -o deviceQuery.o -c deviceQuery.cpp

/usr/local/cuda-11.1/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -o deviceQuery deviceQuery.o

$ nvidia-smi

+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.165.02   Driver Version: 418.165.02   CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |
| N/A   42C    P0    28W /  70W |      0MiB / 15079MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+


$ samples/bin/x86_64/linux/release/deviceQuery
samples/bin/x86_64/linux/release/deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

cudaGetDeviceCount returned 3
-&gt; initialization error
Result = FAIL
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell26">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>Refer to the <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html">CUDA Toolkit Release Notes</a> for details for the minimum driver version and the version of the driver shipped with the toolkit.</p>
<section id="cuda-binary-cubin-compatibility">
<span id="cuda-cubin-compatibility"></span><h3>
<span class="section-number">15.3.1. </span>CUDA Binary (cubin) Compatibility<a class="headerlink" href="#cuda-binary-cubin-compatibility" title="Permalink to this headline"></a>
</h3>
<p>A slightly related but important topic is one of application binary compatibility across GPU architectures in CUDA.</p>
<p>CUDA C++ provides a simple path for users familiar with the C++ 
programming language to easily write programs for execution by the 
device. Kernels can be written using the CUDA instruction set 
architecture, called PTX, which is described in the PTX reference 
manual. It is however usually more effective to use a high-level 
programming language such as C++. In both cases, kernels must be 
compiled into binary code by nvcc (called cubins) to execute on the 
device.</p>
<p>The cubins are architecture-specific. Binary compatibility for cubins
 is guaranteed from one compute capability minor revision to the next 
one, but not from one compute capability minor revision to the previous 
one or across major compute capability revisions. In other words, a 
cubin object generated for compute capability <em>X.y</em> will only execute on devices of compute capability <em>X.z</em> where <em>z≥y</em>.</p>
<p>To execute code on devices of specific compute capability, an 
application must load binary or PTX code that is compatible with this 
compute capability. For portability, that is, to be able to execute code
 on future GPU architectures with higher compute capability (for which 
no binary code can be generated yet), an application must load PTX code 
that will be just-in-time compiled by the NVIDIA driver for these future
 devices.</p>
<p>More information on cubins, PTX and application compatibility can be found in the <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#binary-compatibility">CUDA C++ Programming Guide</a>.</p>
</section>
</section>
<section id="cuda-compatibility-across-minor-releases">
<span id="id70"></span><h2>
<span class="section-number">15.4. </span>CUDA Compatibility Across Minor Releases<a class="headerlink" href="#cuda-compatibility-across-minor-releases" title="Permalink to this headline"></a>
</h2>
<p>By leveraging the semantic versioning, starting with CUDA 11, 
components in the CUDA Toolkit will remain binary compatible across the 
minor versions of the toolkit. In order to maintain binary compatibility
 across minor versions, the CUDA runtime no longer bumps up the minimum 
driver version required for every minor release - this only happens when
 a major release is shipped.</p>
<p>One of the main reasons a new toolchain requires a new minimum driver
 is to handle the JIT compilation of PTX code and the JIT linking of 
binary code.</p>
<p>In this section, we will review the usage patterns that may require 
new user workflows when taking advantage of the compatibility features 
of the CUDA platform.</p>
<section id="existing-cuda-applications-within-minor-versions-of-cuda">
<span id="extisting-applications-within-minor-versions"></span><h3>
<span class="section-number">15.4.1. </span>Existing CUDA Applications within Minor Versions of CUDA<a class="headerlink" href="#existing-cuda-applications-within-minor-versions-of-cuda" title="Permalink to this headline"></a>
</h3>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell27"><span></span><span class="n">$</span><span class="w"> </span><span class="n">nvidia</span><span class="o">-</span><span class="n">smi</span><span class="w"></span>

<span class="o">+-----------------------------------------------------------------------------+</span><span class="w"></span>
<span class="o">|</span><span class="w"> </span><span class="n">NVIDIA</span><span class="o">-</span><span class="n">SMI</span><span class="w"> </span><span class="mf">450.80.02</span><span class="w">    </span><span class="n">Driver</span><span class="w"> </span><span class="n">Version</span><span class="o">:</span><span class="w"> </span><span class="mf">450.80.02</span><span class="w">    </span><span class="n">CUDA</span><span class="w"> </span><span class="n">Version</span><span class="o">:</span><span class="w"> </span><span class="mf">11.0</span><span class="w">     </span><span class="o">|</span><span class="w"></span>
<span class="o">|-------------------------------+----------------------+----------------------+</span><span class="w"></span>
<span class="o">|</span><span class="w"> </span><span class="n">GPU</span><span class="w">  </span><span class="n">Name</span><span class="w">        </span><span class="n">Persistence</span><span class="o">-</span><span class="n">M</span><span class="o">|</span><span class="w"> </span><span class="n">Bus</span><span class="o">-</span><span class="n">Id</span><span class="w">        </span><span class="n">Disp</span><span class="p">.</span><span class="n">A</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Volatile</span><span class="w"> </span><span class="n">Uncorr</span><span class="p">.</span><span class="w"> </span><span class="n">ECC</span><span class="w"> </span><span class="o">|</span><span class="w"></span>
<span class="o">|</span><span class="w"> </span><span class="n">Fan</span><span class="w">  </span><span class="n">Temp</span><span class="w">  </span><span class="n">Perf</span><span class="w">  </span><span class="n">Pwr</span><span class="o">:</span><span class="n">Usage</span><span class="o">/</span><span class="n">Cap</span><span class="o">|</span><span class="w">         </span><span class="n">Memory</span><span class="o">-</span><span class="n">Usage</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">GPU</span><span class="o">-</span><span class="n">Util</span><span class="w">  </span><span class="n">Compute</span><span class="w"> </span><span class="n">M</span><span class="p">.</span><span class="w"> </span><span class="o">|</span><span class="w"></span>
<span class="o">|</span><span class="w">                               </span><span class="o">|</span><span class="w">                      </span><span class="o">|</span><span class="w">               </span><span class="n">MIG</span><span class="w"> </span><span class="n">M</span><span class="p">.</span><span class="w"> </span><span class="o">|</span><span class="w"></span>
<span class="o">|===============================+======================+======================|</span><span class="w"></span>
<span class="o">|</span><span class="w">   </span><span class="mi">0</span><span class="w">  </span><span class="n">Tesla</span><span class="w"> </span><span class="n">T4</span><span class="w">            </span><span class="n">On</span><span class="w">   </span><span class="o">|</span><span class="w"> </span><span class="mo">00000000</span><span class="o">:</span><span class="mo">00</span><span class="o">:</span><span class="mi">1</span><span class="n">E</span><span class="mf">.0</span><span class="w"> </span><span class="n">Off</span><span class="w"> </span><span class="o">|</span><span class="w">                    </span><span class="mi">0</span><span class="w"> </span><span class="o">|</span><span class="w"></span>
<span class="o">|</span><span class="w"> </span><span class="n">N</span><span class="o">/</span><span class="n">A</span><span class="w">   </span><span class="mi">39</span><span class="n">C</span><span class="w">    </span><span class="n">P8</span><span class="w">     </span><span class="mi">9</span><span class="n">W</span><span class="w"> </span><span class="o">/</span><span class="w">  </span><span class="mi">70</span><span class="n">W</span><span class="w"> </span><span class="o">|</span><span class="w">      </span><span class="mi">0</span><span class="n">MiB</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">15109</span><span class="n">MiB</span><span class="w"> </span><span class="o">|</span><span class="w">      </span><span class="mi">0</span><span class="o">%</span><span class="w">      </span><span class="n">Default</span><span class="w"> </span><span class="o">|</span><span class="w"></span>
<span class="o">|</span><span class="w">                               </span><span class="o">|</span><span class="w">                      </span><span class="o">|</span><span class="w">                  </span><span class="n">N</span><span class="o">/</span><span class="n">A</span><span class="w"> </span><span class="o">|</span><span class="w"></span>
<span class="o">+-------------------------------+----------------------+----------------------+</span><span class="w"></span>

<span class="o">+-----------------------------------------------------------------------------+</span><span class="w"></span>
<span class="o">|</span><span class="w"> </span><span class="n">Processes</span><span class="o">:</span><span class="w">                                                                  </span><span class="o">|</span><span class="w"></span>
<span class="o">|</span><span class="w">  </span><span class="n">GPU</span><span class="w">   </span><span class="n">GI</span><span class="w">   </span><span class="n">CI</span><span class="w">        </span><span class="n">PID</span><span class="w">   </span><span class="n">Type</span><span class="w">   </span><span class="n">Process</span><span class="w"> </span><span class="n">name</span><span class="w">                  </span><span class="n">GPU</span><span class="w"> </span><span class="n">Memory</span><span class="w"> </span><span class="o">|</span><span class="w"></span>
<span class="o">|</span><span class="w">        </span><span class="n">ID</span><span class="w">   </span><span class="n">ID</span><span class="w">                                                   </span><span class="n">Usage</span><span class="w">      </span><span class="o">|</span><span class="w"></span>
<span class="o">|=============================================================================|</span><span class="w"></span>
<span class="o">|</span><span class="w">  </span><span class="n">No</span><span class="w"> </span><span class="n">running</span><span class="w"> </span><span class="n">processes</span><span class="w"> </span><span class="n">found</span><span class="w">                                                 </span><span class="o">|</span><span class="w"></span>
<span class="o">+-----------------------------------------------------------------------------+</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell27">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>When our CUDA 11.1 application (i.e. cudart 11.1 is statically 
linked) is run on the system, we see that it runs successfully even when
 the driver reports a 11.0 version - that is, without requiring the 
driver or other toolkit components to be updated on the system.</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell28"><span></span><span class="n">$</span><span class="w"> </span><span class="n">samples</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">x86_64</span><span class="o">/</span><span class="n">linux</span><span class="o">/</span><span class="n">release</span><span class="o">/</span><span class="n">deviceQuery</span><span class="w"></span>
<span class="n">samples</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">x86_64</span><span class="o">/</span><span class="n">linux</span><span class="o">/</span><span class="n">release</span><span class="o">/</span><span class="n">deviceQuery</span><span class="w"> </span><span class="n">Starting</span><span class="p">...</span><span class="w"></span>

<span class="w"> </span><span class="n">CUDA</span><span class="w"> </span><span class="n">Device</span><span class="w"> </span><span class="n">Query</span><span class="w"> </span><span class="p">(</span><span class="n">Runtime</span><span class="w"> </span><span class="n">API</span><span class="p">)</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="p">(</span><span class="n">CUDART</span><span class="w"> </span><span class="k">static</span><span class="w"> </span><span class="n">linking</span><span class="p">)</span><span class="w"></span>

<span class="n">Detected</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">CUDA</span><span class="w"> </span><span class="n">Capable</span><span class="w"> </span><span class="n">device</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="w"></span>

<span class="n">Device</span><span class="w"> </span><span class="mi">0</span><span class="o">:</span><span class="w"> </span><span class="s">"Tesla T4"</span><span class="w"></span>
<span class="w">  </span><span class="n">CUDA</span><span class="w"> </span><span class="n">Driver</span><span class="w"> </span><span class="n">Version</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">Runtime</span><span class="w"> </span><span class="n">Version</span><span class="w">          </span><span class="mf">11.0</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">11.1</span><span class="w"></span>
<span class="w">  </span><span class="n">CUDA</span><span class="w"> </span><span class="n">Capability</span><span class="w"> </span><span class="n">Major</span><span class="o">/</span><span class="n">Minor</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="n">number</span><span class="o">:</span><span class="w">    </span><span class="mf">7.5</span><span class="w"></span>

<span class="w">  </span><span class="p">...</span><span class="o">&lt;</span><span class="n">snip</span><span class="o">&gt;</span><span class="p">...</span><span class="w"></span>

<span class="n">deviceQuery</span><span class="p">,</span><span class="w"> </span><span class="n">CUDA</span><span class="w"> </span><span class="n">Driver</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CUDART</span><span class="p">,</span><span class="w"> </span><span class="n">CUDA</span><span class="w"> </span><span class="n">Driver</span><span class="w"> </span><span class="n">Version</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">11.0</span><span class="p">,</span><span class="w"> </span><span class="n">CUDA</span><span class="w"> </span><span class="n">Runtime</span><span class="w"> </span><span class="n">Version</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">11.1</span><span class="p">,</span><span class="w"> </span><span class="n">NumDevs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="w"></span>
<span class="n">Result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">PASS</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell28">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>By using new CUDA versions, users can benefit from new CUDA 
programming model APIs, compiler optimizations and math library 
features.</p>
<p>The following sections discuss some caveats and considerations.</p>
<section id="handling-new-cuda-features-and-driver-apis">
<span id="handling-new-cuda-cfeatures-and-driver-apis"></span><h4>
<span class="section-number">15.4.1.1. </span>Handling New CUDA Features and Driver APIs<a class="headerlink" href="#handling-new-cuda-features-and-driver-apis" title="Permalink to this headline"></a>
</h4>
<p>A subset of CUDA APIs don’t need a new driver and they can all be used without any driver dependencies. For example, <code class="docutils literal notranslate"><span class="pre">cuMemMap</span></code> APIs or any of APIs introduced prior to CUDA 11.0, such as <code class="docutils literal notranslate"><span class="pre">cudaDeviceSynchronize</span></code>,
 do not require a driver upgrade. To use other CUDA APIs introduced in a
 minor release (that require a new driver), one would have to implement 
fallbacks or fail gracefully. This situation is not different from what 
is available today where developers use macros to compile out features 
based on CUDA versions. Users should refer to the CUDA headers and 
documentation for new CUDA APIs introduced in a release.</p>
<p>When working with a feature exposed in a minor version of the 
toolkit, the feature might not be available at runtime if the 
application is running against an older CUDA driver. Users wishing to 
take advantage of such a feature should query its availability with a 
dynamic check in the code:</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell29"><span></span><span class="k">static</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">hostRegisterFeatureSupported</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"></span>
<span class="k">static</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">hostRegisterIsDeviceAddress</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"></span>

<span class="k">static</span><span class="w"> </span><span class="n">error_t</span><span class="w"> </span><span class="nf">cuFooFunction</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">ptr</span><span class="p">)</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">dptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">null</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">hostRegisterFeatureSupported</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">         </span><span class="n">cudaHostRegister</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">flags</span><span class="p">);</span><span class="w"></span>
<span class="w">         </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">hostRegisterIsDeviceAddress</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">              </span><span class="n">qptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ptr</span><span class="p">;</span><span class="w"></span>
<span class="w">         </span><span class="p">}</span><span class="w"></span>
<span class="w">       </span><span class="k">else</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">          </span><span class="n">cudaHostGetDevicePointer</span><span class="p">(</span><span class="o">&amp;</span><span class="n">qptr</span><span class="p">,</span><span class="w"> </span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="w">          </span><span class="p">}</span><span class="w"></span>
<span class="w">       </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="k">else</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">            </span><span class="c1">// cudaMalloc();</span>
<span class="w">            </span><span class="c1">// cudaMemcpy();</span>
<span class="w">       </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="n">gemm</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">dptr</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="n">cudaDeviceSynchronize</span><span class="p">();</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="c1">// rest of code here</span>
<span class="w">    </span><span class="n">cudaDeviceGetAttribute</span><span class="p">(</span><span class="w"></span>
<span class="w">           </span><span class="o">&amp;</span><span class="n">hostRegisterFeatureSupported</span><span class="p">,</span><span class="w"></span>
<span class="w">           </span><span class="n">cudaDevAttrHostRegisterSupported</span><span class="p">,</span><span class="w"></span>
<span class="w">           </span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="n">cudaDeviceGetAttribute</span><span class="p">(</span><span class="w"></span>
<span class="w">           </span><span class="o">&amp;</span><span class="n">hostRegisterIsDeviceAddress</span><span class="p">,</span><span class="w"></span>
<span class="w">           </span><span class="n">cudaDevAttrCanUseHostPointerForRegisteredMem</span><span class="p">,</span><span class="w"></span>
<span class="w">           </span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="n">cuFooFunction</span><span class="p">(</span><span class="cm">/* malloced pointer */</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell29">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>Alternatively the application’s interface might not work at all 
without a new CUDA driver and then its best to return an error right 
away:</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell30"><span></span><span class="cp">#define MIN_VERSION 11010</span>
<span class="n">cudaError_t</span><span class="w"> </span><span class="nf">foo</span><span class="p">()</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">cudaGetDriverVersion</span><span class="p">(</span><span class="o">&amp;</span><span class="n">version</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">version</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">MIN_VERSION</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">CUDA_ERROR_INSUFFICIENT_DRIVER</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="c1">// proceed as normal</span>
<span class="p">}</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell30">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>A new error code is added to indicate that the functionality is missing from the driver you are running against: <code class="docutils literal notranslate"><span class="pre">cudaErrorCallRequiresNewerDriver</span></code>.</p>
</section>
<section id="using-ptx">
<span id="id71"></span><h4>
<span class="section-number">15.4.1.2. </span>Using PTX<a class="headerlink" href="#using-ptx" title="Permalink to this headline"></a>
</h4>
<p>PTX defines a virtual machine and ISA for general purpose parallel 
thread execution. PTX programs are translated at load time to the target
 hardware instruction set via the JIT Compiler which is part of the CUDA
 driver. As PTX is compiled by the CUDA driver, new toolchains will 
generate PTX that is not compatible with the older CUDA driver. This is 
not a problem when PTX is used for future device compatibility (the most
 common case), but can lead to issues when used for runtime compilation.</p>
<p>For codes continuing to make use of PTX, in order to support 
compiling on an older driver, your code must be first transformed into 
device code via the static ptxjitcompiler library or NVRTC with the 
option of generating code for a specific architecture (e.g. sm_80) 
rather than a virtual architecture (e.g. compute_80). For this workflow,
 a new nvptxcompiler_static library is shipped with the CUDA Toolkit.</p>
<p>We can see this usage in the following example:</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell31"><span></span><span class="kt">char</span><span class="o">*</span><span class="w"> </span><span class="nf">compilePTXToNVElf</span><span class="p">()</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">nvPTXCompilerHandle</span><span class="w"> </span><span class="n">compiler</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">NULL</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">nvPTXCompileResult</span><span class="w"> </span><span class="n">status</span><span class="p">;</span><span class="w"></span>

<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">elfSize</span><span class="p">,</span><span class="w"> </span><span class="n">infoSize</span><span class="p">,</span><span class="w"> </span><span class="n">errorSize</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">elf</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">infoLog</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">errorLog</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">minorVer</span><span class="p">,</span><span class="w"> </span><span class="n">majorVer</span><span class="p">;</span><span class="w"></span>

<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">*</span><span class="w"> </span><span class="n">compile_options</span><span class="p">[]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="s">"--gpu-name=sm_80"</span><span class="p">,</span><span class="w"></span>
<span class="w">                                      </span><span class="s">"--device-debug"</span><span class="w"></span>
<span class="w">    </span><span class="p">};</span><span class="w"></span>

<span class="w">    </span><span class="n">nvPTXCompilerGetVersion</span><span class="p">(</span><span class="o">&amp;</span><span class="n">majorVer</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">minorVer</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="n">nvPTXCompilerCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">compiler</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="p">)</span><span class="n">strlen</span><span class="p">(</span><span class="n">ptxCode</span><span class="p">),</span><span class="w"> </span><span class="n">ptxCode</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="n">status</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nvPTXCompilerCompile</span><span class="p">(</span><span class="n">compiler</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">compile_options</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">status</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">NVPTXCOMPILE_SUCCESS</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">nvPTXCompilerGetErrorLogSize</span><span class="p">(</span><span class="n">compiler</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">errorSize</span><span class="p">);</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">errorSize</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">            </span><span class="n">errorLog</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">errorSize</span><span class="o">+</span><span class="mi">1</span><span class="p">);</span><span class="w"></span>
<span class="w">            </span><span class="n">nvPTXCompilerGetErrorLog</span><span class="p">(</span><span class="n">compiler</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="n">errorLog</span><span class="p">);</span><span class="w"></span>
<span class="w">            </span><span class="n">printf</span><span class="p">(</span><span class="s">"Error log: %s</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="w"> </span><span class="n">errorLog</span><span class="p">);</span><span class="w"></span>
<span class="w">            </span><span class="n">free</span><span class="p">(</span><span class="n">errorLog</span><span class="p">);</span><span class="w"></span>
<span class="w">        </span><span class="p">}</span><span class="w"></span>
<span class="w">        </span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>

<span class="w">    </span><span class="n">nvPTXCompilerGetCompiledProgramSize</span><span class="p">(</span><span class="n">compiler</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">elfSize</span><span class="p">));</span><span class="w"></span>
<span class="w">    </span><span class="n">elf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">elfSize</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="n">nvPTXCompilerGetCompiledProgram</span><span class="p">(</span><span class="n">compiler</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="n">elf</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="n">nvPTXCompilerGetInfoLogSize</span><span class="p">(</span><span class="n">compiler</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">infoSize</span><span class="p">);</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">infoSize</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">infoLog</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">infoSize</span><span class="o">+</span><span class="mi">1</span><span class="p">);</span><span class="w"></span>
<span class="w">        </span><span class="n">nvPTXCompilerGetInfoLog</span><span class="p">(</span><span class="n">compiler</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="n">infoLog</span><span class="p">);</span><span class="w"></span>
<span class="w">        </span><span class="n">printf</span><span class="p">(</span><span class="s">"Info log: %s</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="w"> </span><span class="n">infoLog</span><span class="p">);</span><span class="w"></span>
<span class="w">        </span><span class="n">free</span><span class="p">(</span><span class="n">infoLog</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>

<span class="w">    </span><span class="n">nvPTXCompilerDestroy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">compiler</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">elf</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell31">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
</section>
<section id="dynamic-code-generation">
<span id="id72"></span><h4>
<span class="section-number">15.4.1.3. </span>Dynamic Code Generation<a class="headerlink" href="#dynamic-code-generation" title="Permalink to this headline"></a>
</h4>
<p>NVRTC is a runtime compilation library for CUDA C++. It accepts CUDA 
C++ source code in character string form and creates handles that can be
 used to obtain the PTX. The PTX string generated by NVRTC can be loaded
 by cuModuleLoadData and cuModuleLoadDataEx.</p>
<p>Dealing with relocatable objects is not yet supported, therefore the <code class="docutils literal notranslate"><span class="pre">cuLink</span></code>*
 set of APIs in the CUDA driver will not work with enhanced 
compatibility. An upgraded driver matching the CUDA runtime version is 
currently required for those APIs.</p>
<p>As mentioned in the PTX section, the compilation of PTX to device 
code lives along with the CUDA driver, hence the generated PTX might be 
newer than what is supported by the driver on the deployment system. 
When using NVRTC, it is recommended that the resulting PTX code is first
 transformed to the final device code via the steps outlined by the PTX 
user workflow. This ensures your code is compatible. Alternatively, 
NVRTC can generate cubins directly starting with CUDA 11.1. Applications
 using the new API can load the final device code directly using driver 
APIs <code class="docutils literal notranslate"><span class="pre">cuModuleLoadData</span></code> and <code class="docutils literal notranslate"><span class="pre">cuModuleLoadDataEx</span></code>.</p>
<p>NVRTC used to support only virtual architectures through the option 
-arch, since it was only emitting PTX. It will now support actual 
architectures as well to emit SASS. The interface is augmented to 
retrieve either the PTX or cubin if an actual architecture is specified.</p>
<p>The example below shows how an existing example can be adapted to use the new features, guarded by the <code class="docutils literal notranslate"><span class="pre">USE_CUBIN</span></code> macro in this case:</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre id="codecell32"><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;nvrtc.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cuda.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;iostream&gt;</span><span class="cp"></span>

<span class="kt">void</span><span class="w"> </span><span class="nf">NVRTC_SAFE_CALL</span><span class="p">(</span><span class="n">nvrtcResult</span><span class="w"> </span><span class="n">result</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">result</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">NVRTC_SUCCESS</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">"</span><span class="se">\n</span><span class="s">nvrtc error: "</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">nvrtcGetErrorString</span><span class="p">(</span><span class="n">result</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="sc">'\n'</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="kt">void</span><span class="w"> </span><span class="nf">CUDA_SAFE_CALL</span><span class="p">(</span><span class="n">CUresult</span><span class="w"> </span><span class="n">result</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">result</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">CUDA_SUCCESS</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">msg</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">cuGetErrorName</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">msg</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">"</span><span class="se">\n</span><span class="s">cuda error: "</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">msg</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="sc">'\n'</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">hello</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">"                                           </span><span class="se">\n</span><span class="s">\</span>
<span class="s">extern </span><span class="se">\"</span><span class="s">C</span><span class="se">\"</span><span class="s"> __global__ void hello() {                          </span><span class="se">\n</span><span class="s">\</span>
<span class="s">  printf(</span><span class="se">\"</span><span class="s">hello world</span><span class="se">\\</span><span class="s">n</span><span class="se">\"</span><span class="s">);                                   </span><span class="se">\n</span><span class="s">\</span>
<span class="s">}                                                               </span><span class="se">\n</span><span class="s">"</span><span class="p">;</span><span class="w"></span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="n">nvrtcProgram</span><span class="w"> </span><span class="n">prog</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="n">NVRTC_SAFE_CALL</span><span class="p">(</span><span class="n">nvrtcCreateProgram</span><span class="p">(</span><span class="o">&amp;</span><span class="n">prog</span><span class="p">,</span><span class="w"> </span><span class="n">hello</span><span class="p">,</span><span class="w"> </span><span class="s">"hello.cu"</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="nb">NULL</span><span class="p">,</span><span class="w"> </span><span class="nb">NULL</span><span class="p">));</span><span class="w"></span>
<span class="cp">#ifdef USE_CUBIN</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">opts</span><span class="p">[]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="s">"-arch=sm_70"</span><span class="p">};</span><span class="w"></span>
<span class="cp">#else</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">opts</span><span class="p">[]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="s">"-arch=compute_70"</span><span class="p">};</span><span class="w"></span>
<span class="cp">#endif</span>
<span class="w">  </span><span class="n">nvrtcResult</span><span class="w"> </span><span class="n">compileResult</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nvrtcCompileProgram</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">opts</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">logSize</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="n">NVRTC_SAFE_CALL</span><span class="p">(</span><span class="n">nvrtcGetProgramLogSize</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">logSize</span><span class="p">));</span><span class="w"></span>
<span class="w">  </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">log</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="kt">char</span><span class="p">[</span><span class="n">logSize</span><span class="p">];</span><span class="w"></span>
<span class="w">  </span><span class="n">NVRTC_SAFE_CALL</span><span class="p">(</span><span class="n">nvrtcGetProgramLog</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="w"> </span><span class="n">log</span><span class="p">));</span><span class="w"></span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">log</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="sc">'\n'</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="n">delete</span><span class="p">[]</span><span class="w"> </span><span class="n">log</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">compileResult</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">NVRTC_SUCCESS</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">codeSize</span><span class="p">;</span><span class="w"></span>
<span class="cp">#ifdef USE_CUBIN</span>
<span class="w">  </span><span class="n">NVRTC_SAFE_CALL</span><span class="p">(</span><span class="n">nvrtcGetCUBINSize</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">codeSize</span><span class="p">));</span><span class="w"></span>
<span class="w">  </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">code</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="kt">char</span><span class="p">[</span><span class="n">codeSize</span><span class="p">];</span><span class="w"></span>
<span class="w">  </span><span class="n">NVRTC_SAFE_CALL</span><span class="p">(</span><span class="n">nvrtcGetCUBIN</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="w"> </span><span class="n">code</span><span class="p">));</span><span class="w"></span>
<span class="cp">#else</span>
<span class="w">  </span><span class="n">NVRTC_SAFE_CALL</span><span class="p">(</span><span class="n">nvrtcGetPTXSize</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">codeSize</span><span class="p">));</span><span class="w"></span>
<span class="w">  </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">code</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="kt">char</span><span class="p">[</span><span class="n">codeSize</span><span class="p">];</span><span class="w"></span>
<span class="w">  </span><span class="n">NVRTC_SAFE_CALL</span><span class="p">(</span><span class="n">nvrtcGetPTX</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="w"> </span><span class="n">code</span><span class="p">));</span><span class="w"></span>
<span class="cp">#endif</span>
<span class="w">  </span><span class="n">NVRTC_SAFE_CALL</span><span class="p">(</span><span class="n">nvrtcDestroyProgram</span><span class="p">(</span><span class="o">&amp;</span><span class="n">prog</span><span class="p">));</span><span class="w"></span>
<span class="w">  </span><span class="n">CUdevice</span><span class="w"> </span><span class="n">cuDevice</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="n">CUcontext</span><span class="w"> </span><span class="n">context</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="n">CUmodule</span><span class="w"> </span><span class="n">module</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="n">CUfunction</span><span class="w"> </span><span class="n">kernel</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="n">CUDA_SAFE_CALL</span><span class="p">(</span><span class="n">cuInit</span><span class="p">(</span><span class="mi">0</span><span class="p">));</span><span class="w"></span>
<span class="w">  </span><span class="n">CUDA_SAFE_CALL</span><span class="p">(</span><span class="n">cuDeviceGet</span><span class="p">(</span><span class="o">&amp;</span><span class="n">cuDevice</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">));</span><span class="w"></span>
<span class="w">  </span><span class="n">CUDA_SAFE_CALL</span><span class="p">(</span><span class="n">cuCtxCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">context</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">cuDevice</span><span class="p">));</span><span class="w"></span>
<span class="w">  </span><span class="n">CUDA_SAFE_CALL</span><span class="p">(</span><span class="n">cuModuleLoadDataEx</span><span class="p">(</span><span class="o">&amp;</span><span class="n">module</span><span class="p">,</span><span class="w"> </span><span class="n">code</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">));</span><span class="w"></span>
<span class="w">  </span><span class="n">CUDA_SAFE_CALL</span><span class="p">(</span><span class="n">cuModuleGetFunction</span><span class="p">(</span><span class="o">&amp;</span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="n">module</span><span class="p">,</span><span class="w"> </span><span class="s">"hello"</span><span class="p">));</span><span class="w"></span>
<span class="w">  </span><span class="n">CUDA_SAFE_CALL</span><span class="p">(</span><span class="n">cuLaunchKernel</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="nb">NULL</span><span class="p">,</span><span class="w"> </span><span class="nb">NULL</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">));</span><span class="w"></span>
<span class="w">  </span><span class="n">CUDA_SAFE_CALL</span><span class="p">(</span><span class="n">cuCtxSynchronize</span><span class="p">());</span><span class="w"></span>
<span class="w">  </span><span class="n">CUDA_SAFE_CALL</span><span class="p">(</span><span class="n">cuModuleUnload</span><span class="p">(</span><span class="n">module</span><span class="p">));</span><span class="w"></span>
<span class="w">  </span><span class="n">CUDA_SAFE_CALL</span><span class="p">(</span><span class="n">cuCtxDestroy</span><span class="p">(</span><span class="n">context</span><span class="p">));</span><span class="w"></span>
<span class="w">  </span><span class="n">delete</span><span class="p">[]</span><span class="w"> </span><span class="n">code</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell32">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
</section>
<section id="recommendations-for-building-a-minor-version-compatible-library">
<span id="how-to-make-library-minver-compatible"></span><h4>
<span class="section-number">15.4.1.4. </span>Recommendations for building a minor-version compatible library<a class="headerlink" href="#recommendations-for-building-a-minor-version-compatible-library" title="Permalink to this headline"></a>
</h4>
<p>We recommend that the CUDA runtime be statically linked to minimize 
dependencies. Verify that your library doesn’t leak dependencies, 
breakages, namespaces, etc. outside your established ABI contract.</p>
<p>Follow semantic versioning for your library’s soname. Having a 
semantically versioned ABI means the interfaces need to be maintained 
and versioned. The library should follow semantic rules and increment 
the version number when a change is made that affects this ABI contract.
 Missing dependencies is also a binary compatibility break, hence you 
should provide fallbacks or guards for functionality that depends on 
those interfaces. Increment major versions when there are ABI breaking 
changes such as API deprecation and modifications. New APIs can be added
 in minor versions.</p>
<p>Conditionally use features to remain compatible against older 
drivers. If no new features are used (or if they are used conditionally 
with fallbacks provided) you’ll be able to remain compatible.</p>
<p>Don’t expose ABI structures that can change. A pointer to a structure with a size embedded is a better solution.</p>
<p>When linking with dynamic libraries from the toolkit, the library 
must be equal to or newer than what is needed by any one of the 
components involved in the linking of your application. For example, if 
you link against the CUDA 11.1 dynamic runtime, and use functionality 
from 11.1, as well as a separate shared library that was linked against 
the CUDA 11.2 dynamic runtime that requires 11.2 functionality, the 
final link step must include a CUDA 11.2 or newer dynamic runtime.</p>
</section>
<section id="recommendations-for-taking-advantage-of-minor-version-compatibility-in-your-application">
<span id="taking-advantage-of-minver-compat"></span><h4>
<span class="section-number">15.4.1.5. </span>Recommendations for taking advantage of minor version compatibility in your application<a class="headerlink" href="#recommendations-for-taking-advantage-of-minor-version-compatibility-in-your-application" title="Permalink to this headline"></a>
</h4>
<p>Certain functionality might not be available so you should query 
where applicable. This is common for building applications that are GPU 
architecture, platform and compiler agnostic. However we now add “the 
underlying driver” to that mix.</p>
<p>As with the previous section on library building recommendations, if 
using the CUDA runtime, we recommend linking to the CUDA runtime 
statically when building your application. When using the driver APIs 
directly, we recommend using the new driver entry point access API (<code class="docutils literal notranslate"><span class="pre">cuGetProcAddress</span></code>) documented here: <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__DRIVER__ENTRY__POINT.html#group__CUDA__DRIVER__ENTRY__POINT">CUDA Driver API :: CUDA Toolkit Documentation</a>.</p>
<p>When using a shared or static library, follow the release notes of 
said library to determine if the library supports minor version 
compatibility.</p>
</section>
</section>
</section>
</section>
<section id="preparing-for-deployment">
<span id="id73"></span><h1>
<span class="section-number">16. </span>Preparing for Deployment<a class="headerlink" href="#preparing-for-deployment" title="Permalink to this headline"></a>
</h1>
<section id="testing-for-cuda-availability">
<span id="id74"></span><h2>
<span class="section-number">16.1. </span>Testing for CUDA Availability<a class="headerlink" href="#testing-for-cuda-availability" title="Permalink to this headline"></a>
</h2>
<p>When deploying a CUDA application, it is often desirable to ensure 
that the application will continue to function properly even if the 
target machine does not have a CUDA-capable GPU and/or a sufficient 
version of the NVIDIA Driver installed. (Developers targeting a single 
machine with known configuration may choose to skip this section.)</p>
<p><strong>Detecting a CUDA-Capable GPU</strong></p>
<p>When an application will be deployed to target machines of 
arbitrary/unknown configuration, the application should explicitly test 
for the existence of a CUDA-capable GPU in order to take appropriate 
action when no such device is available. The <code class="docutils literal notranslate"><span class="pre">cudaGetDeviceCount()</span></code>
 function can be used to query for the number of available devices. Like
 all CUDA Runtime API functions, this function will fail gracefully and 
return <code class="docutils literal notranslate"><span class="pre">cudaErrorNoDevice</span></code> to the application if there is no CUDA-capable GPU or <code class="docutils literal notranslate"><span class="pre">cudaErrorInsufficientDriver</span></code> if there is not an appropriate version of the NVIDIA Driver installed. If <code class="docutils literal notranslate"><span class="pre">cudaGetDeviceCount()</span></code> reports an error, the application should fall back to an alternative code path.</p>
<p>A system with multiple GPUs may contain GPUs of different hardware 
versions and capabilities. When using multiple GPUs from the same 
application, it is recommended to use GPUs of the same type, rather than
 mixing hardware generations. The <code class="docutils literal notranslate"><span class="pre">cudaChooseDevice()</span></code> function can be used to select the device that most closely matches a desired set of features.</p>
<p><strong>Detecting Hardware and Software Configuration</strong></p>
<p>When an application depends on the availability of certain hardware 
or software capabilities to enable certain functionality, the CUDA API 
can be queried for details about the configuration of the available 
device and for the installed software versions.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">cudaGetDeviceProperties()</span></code> function reports various features of the available devices, including the <a class="reference internal" href="#cuda-compute-capability"><span class="std std-ref">CUDA Compute Capability</span></a> of the device (see also the Compute Capabilities section of the CUDA C++ Programming Guide). See <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART____VERSION.html#group__CUDART____VERSION">Version Management</a> for details on how to query the available CUDA software API versions.</p>
</section>
<section id="error-handling">
<span id="id75"></span><h2>
<span class="section-number">16.2. </span>Error Handling<a class="headerlink" href="#error-handling" title="Permalink to this headline"></a>
</h2>
<p>All CUDA Runtime API calls return an error code of type <code class="docutils literal notranslate"><span class="pre">cudaError_t</span></code>; the return value will be equal to <code class="docutils literal notranslate"><span class="pre">cudaSuccess</span></code> if no errors have occurred. (The exceptions to this are kernel launches, which return void, and <code class="docutils literal notranslate"><span class="pre">cudaGetErrorString()</span></code>, which returns a character string describing the <code class="docutils literal notranslate"><span class="pre">cudaError_t</span></code> code that was passed into it.) The CUDA Toolkit libraries (<code class="docutils literal notranslate"><span class="pre">cuBLAS</span></code>, <code class="docutils literal notranslate"><span class="pre">cuFFT</span></code>, etc.) likewise return their own sets of error codes.</p>
<p>Since some CUDA API calls and all kernel launches are asynchronous 
with respect to the host code, errors may be reported to the host 
asynchronously as well; often this occurs the next time the host and 
device synchronize with each other, such as during a call to <code class="docutils literal notranslate"><span class="pre">cudaMemcpy()</span></code> or to <code class="docutils literal notranslate"><span class="pre">cudaDeviceSynchronize()</span></code>.</p>
<p>Always check the error return values on all CUDA API functions, even 
for functions that are not expected to fail, as this will allow the 
application to detect and recover from errors as soon as possible should
 they occur. To check for errors occurring during kernel launches using 
the <code class="docutils literal notranslate"><span class="pre">&lt;&lt;&lt;...&gt;&gt;&gt;</span></code> syntax, which does not return any error code, the return code of <code class="docutils literal notranslate"><span class="pre">cudaGetLastError()</span></code>
 should be checked immediately after the kernel launch. Applications 
that do not check for CUDA API errors could at times run to completion 
without having noticed that the data calculated by the GPU is 
incomplete, invalid, or uninitialized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The CUDA Toolkit Samples provide several helper functions for error 
checking with the various CUDA APIs; these helper functions are located 
in the <code class="docutils literal notranslate"><span class="pre">samples/common/inc/helper_cuda.h</span></code> file in the CUDA Toolkit.</p>
</div>
</section>
<section id="building-for-maximum-compatibility">
<span id="id76"></span><h2>
<span class="section-number">16.3. </span>Building for Maximum Compatibility<a class="headerlink" href="#building-for-maximum-compatibility" title="Permalink to this headline"></a>
</h2>
<p>Each generation of CUDA-capable device has an associated <em>compute capability</em> version that indicates the feature set supported by the device (see <a class="reference internal" href="#cuda-compute-capability"><span class="std std-ref">CUDA Compute Capability</span></a>).
 One or more compute capability versions can be specified to the nvcc 
compiler while building a file; compiling for the native compute 
capability for the target GPU(s) of the application is important to 
ensure that application kernels achieve the best possible performance 
and are able to use the features that are available on a given 
generation of GPU.</p>
<p>When an application is built for multiple compute capabilities simultaneously (using several instances of the <code class="docutils literal notranslate"><span class="pre">-gencode</span></code>
 flag to
nvcc), the binaries for the specified compute capabilities are combined 
into the executable, and the CUDA Driver selects the most
appropriate binary at runtime according to the compute capability of the
 present device. If an appropriate native binary (<em>cubin</em>)
is not available, but the intermediate <em>PTX</em> code (which targets an abstract virtual instruction set and is used for forward-compatibility)
is available, then the kernel will be compiled <em>Just In Time</em> (JIT) (see <a class="reference internal" href="#compiler-jit-cache-management"><span class="std std-ref">Compiler JIT Cache Management Tools</span></a>)
from the PTX to the native cubin for the device. If the PTX is also not available, then the kernel launch will fail.</p>
<p><strong>Windows</strong></p>
<div class="highlight-text notranslate">
<div class="highlight"><pre id="codecell33"><span></span>nvcc.exe -ccbin "C:\vs2008\VC\bin"
  -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT"
  -gencode=arch=compute_30,code=sm_30
  -gencode=arch=compute_35,code=sm_35
  -gencode=arch=compute_50,code=sm_50
  -gencode=arch=compute_60,code=sm_60
  -gencode=arch=compute_70,code=sm_70
  -gencode=arch=compute_75,code=sm_75
  -gencode=arch=compute_75,code=compute_75
  --compile -o "Release\mykernel.cu.obj" "mykernel.cu"
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell33">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p><strong>Mac/Linux</strong></p>
<div class="highlight-text notranslate">
<div class="highlight"><pre id="codecell34"><span></span>/usr/local/cuda/bin/nvcc
  -gencode=arch=compute_30,code=sm_30
  -gencode=arch=compute_35,code=sm_35
  -gencode=arch=compute_50,code=sm_50
  -gencode=arch=compute_60,code=sm_60
  -gencode=arch=compute_70,code=sm_70
  -gencode=arch=compute_75,code=sm_75
  -gencode=arch=compute_75,code=compute_75
  -O2 -o mykernel.o -c mykernel.cu
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell34">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>Alternatively, the <code class="docutils literal notranslate"><span class="pre">nvcc</span></code> command-line option <code class="docutils literal notranslate"><span class="pre">-arch=sm_XX</span></code> can be used as a shorthand equivalent to the following more explicit <code class="docutils literal notranslate"><span class="pre">-gencode=</span></code> command-line options described above:</p>
<div class="highlight-text notranslate">
<div class="highlight"><pre id="codecell35"><span></span>-gencode=arch=compute_XX,code=sm_XX
-gencode=arch=compute_XX,code=compute_XX
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell35">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>However, while the <code class="docutils literal notranslate"><span class="pre">-arch=sm_XX</span></code> command-line option does result in inclusion of a PTX back-end target by default (due to the <code class="docutils literal notranslate"><span class="pre">code=compute_XX</span></code> target it implies), it can only specify a single target <code class="docutils literal notranslate"><span class="pre">cubin</span></code> architecture at a time, and it is not possible to use multiple <code class="docutils literal notranslate"><span class="pre">-arch=</span></code> options on the same <code class="docutils literal notranslate"><span class="pre">nvcc</span></code> command line, which is why the examples above use <code class="docutils literal notranslate"><span class="pre">-gencode=</span></code> explicitly.</p>
</section>
<section id="distributing-the-cuda-runtime-and-libraries">
<span id="distributing-cuda-runtime-and-libraries"></span><h2>
<span class="section-number">16.4. </span>Distributing the CUDA Runtime and Libraries<a class="headerlink" href="#distributing-the-cuda-runtime-and-libraries" title="Permalink to this headline"></a>
</h2>
<p>CUDA applications are built against the CUDA Runtime library, which 
handles device, memory, and kernel management. Unlike the CUDA Driver, 
the CUDA Runtime guarantees neither forward nor backward binary 
compatibility across versions. It is therefore best to <a class="reference internal" href="#redistribution"><span class="std std-ref">redistribute</span></a>
 the CUDA Runtime library with the application when using dynamic 
linking or else to statically link against the CUDA Runtime. This will 
ensure that the executable will be able to run even if the user does not
 have the same CUDA Toolkit installed that the application was built 
against.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When statically linking to the CUDA Runtime, multiple versions of the
 runtime can peacably coexist in the same application process 
simultaneously; for example, if an application uses one version of the 
CUDA Runtime, and a plugin to that application is statically linked to a
 different version, that is perfectly acceptable, as long as the 
installed NVIDIA Driver is sufficient for both.</p>
</div>
<p class="title sectiontitle rubric" id="statically-linked-cuda-runtime">Statically-linked CUDA Runtime</p>
<p>The easiest option is to statically link against the CUDA Runtime. This is the default if using <code class="docutils literal notranslate"><span class="pre">nvcc</span></code>
 to link in CUDA 5.5 and later. Static linking makes the executable 
slightly larger, but it ensures that the correct version of runtime 
library functions are included in the application binary without 
requiring separate redistribution of the CUDA Runtime library.</p>
<p class="title sectiontitle rubric" id="dynamically-linked-cuda-runtime">Dynamically-linked CUDA Runtime</p>
<p>If static linking against the CUDA Runtime is impractical for some 
reason, then a dynamically-linked version of the CUDA Runtime library is
 also available. (This was the default and only option provided in CUDA 
versions 5.0 and earlier.)</p>
<p>To use dynamic linking with the CUDA Runtime when using the <code class="docutils literal notranslate"><span class="pre">nvcc</span></code> from CUDA 5.5 or later to link the application, add
the <code class="docutils literal notranslate"><span class="pre">--cudart=shared</span></code> flag to the link command line; otherwise the <a class="reference internal" href="#statically-linked-cuda-runtime"><span class="std std-ref">statically-linked CUDA Runtime library</span></a> is used by default.</p>
<p>After the application is dynamically linked against the CUDA Runtime, this version of the runtime library should be <a class="reference internal" href="#redistribution"><span class="std std-ref">bundled with</span></a>
 the application. It can be copied into the same directory as the 
application executable or into a subdirectory of that installation path.</p>
<p class="title sectiontitle rubric" id="other-cuda-libraries">Other CUDA Libraries</p>
<p>Although the CUDA Runtime provides the option of static linking, some
 libraries included in the CUDA Toolkit are available only in 
dynamically-linked form. As with
the <a class="reference internal" href="#dynamically-linked-cuda-runtime"><span class="std std-ref">dynamically-linked version of the CUDA Runtime library</span></a>, these libraries should
be <a class="reference internal" href="#redistribution"><span class="std std-ref">bundled with</span></a> the application executable when distributing that application.</p>
<section id="cuda-toolkit-library-redistribution">
<span id="redistribution"></span><h3>
<span class="section-number">16.4.1. </span>CUDA Toolkit Library Redistribution<a class="headerlink" href="#cuda-toolkit-library-redistribution" title="Permalink to this headline"></a>
</h3>
<p>The CUDA Toolkit’s End-User License Agreement (EULA) allows for 
redistribution of many of the CUDA libraries under certain terms and 
conditions. This allows applications that depend on these libraries <a class="reference internal" href="#redistribution-which-files"><span class="std std-ref">to redistribute the exact versions</span></a>
 of the libraries against which they were built and tested, thereby 
avoiding any trouble for end users who might have a different version of
 the CUDA Toolkit (or perhaps none at all) installed on their machines. 
Please refer to the EULA for details.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This does <em>not</em> apply to the NVIDIA Driver; the end user must 
still download and install an NVIDIA Driver appropriate to their GPU(s) 
and operating system.</p>
</div>
<section id="which-files-to-redistribute">
<span id="redistribution-which-files"></span><h4>
<span class="section-number">16.4.1.1. </span>Which Files to Redistribute<a class="headerlink" href="#which-files-to-redistribute" title="Permalink to this headline"></a>
</h4>
<p>When redistributing the dynamically-linked versions of one or more 
CUDA libraries, it is important to identify the exact files that need to
 be redistributed. The following examples use the cuBLAS library from 
CUDA Toolkit 5.5 as an illustration:</p>
<p><strong>Linux</strong></p>
<p>In a shared library on Linux, there is a string field called the <code class="docutils literal notranslate"><span class="pre">SONAME</span></code> that indicates the binary compatibility level of the library. The <code class="docutils literal notranslate"><span class="pre">SONAME</span></code>
 of the library against which the application was built must match the 
filename of the library that is redistributed with the application.</p>
<p>For example, in the standard CUDA Toolkit installation, the files <code class="docutils literal notranslate"><span class="pre">libcublas.so</span></code> and <code class="docutils literal notranslate"><span class="pre">libcublas.so.5.5</span></code> are both symlinks pointing to a specific build of cuBLAS, which is named like <code class="docutils literal notranslate"><span class="pre">libcublas.so.5.5.x</span></code>, where <em>x</em> is the build number (e.g., <code class="docutils literal notranslate"><span class="pre">libcublas.so.5.5.17</span></code>). However, the <code class="docutils literal notranslate"><span class="pre">SONAME</span></code> of this library is given as “<code class="docutils literal notranslate"><span class="pre">libcublas.so.5.5</span></code>”:</p>
<div class="highlight-text notranslate">
<div class="highlight"><pre id="codecell36"><span></span>$ objdump -p /usr/local/cuda/lib64/libcublas.so | grep SONAME
   SONAME               libcublas.so.5.5
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell36">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>Because of this, even if <code class="docutils literal notranslate"><span class="pre">-lcublas</span></code> (with no version number specified) is used when linking the application, the <code class="docutils literal notranslate"><span class="pre">SONAME</span></code> found at link time implies that “<code class="docutils literal notranslate"><span class="pre">libcublas.so.5.5</span></code>”
 is the name of the file that the dynamic loader will look for when 
loading the application and therefore must be the name of the file (or a
 symlink to the same) that is redistributed with the application.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">ldd</span></code>
 tool is useful for identifying the exact filenames of the libraries 
that the application expects to find at runtime as well as the path, if 
any, of the copy of that library that the dynamic loader would select 
when loading the application given the current library search path:</p>
<div class="highlight-text notranslate">
<div class="highlight"><pre id="codecell37"><span></span>$ ldd a.out | grep libcublas
   libcublas.so.5.5 =&gt; /usr/local/cuda/lib64/libcublas.so.5.5
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell37">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p><strong>Mac</strong></p>
<p>In a shared library on Mac OS X, there is a field called the <code class="docutils literal notranslate"><span class="pre">install</span> <span class="pre">name</span></code>
 that indicates the expected installation path and filename the library;
 the CUDA libraries also use this filename to indicate binary 
compatibility. The value of this field is propagated into an application
 built against the library and is used to locate the library of the 
correct version at runtime.</p>
<p>For example, if the install name of the cuBLAS library is given as <code class="docutils literal notranslate"><span class="pre">@rpath/libcublas.5.5.dylib</span></code>, then the library is version 5.5 and the copy of this library
redistributed with the application must be named <code class="docutils literal notranslate"><span class="pre">libcublas.5.5.dylib</span></code>, even though only <code class="docutils literal notranslate"><span class="pre">-lcublas</span></code> (with no version number specified) is used at link time.
Furthermore, this file should be installed into the <code class="docutils literal notranslate"><span class="pre">@rpath</span></code> of the application; see <a class="reference internal" href="#redistribution-where-to-install"><span class="std std-ref">Where to Install Redistributed CUDA Libraries</span></a>.</p>
<p>To view a library’s install name, use the <code class="docutils literal notranslate"><span class="pre">otool</span> <span class="pre">-L</span></code> command:</p>
<div class="highlight-text notranslate">
<div class="highlight"><pre id="codecell38"><span></span>$ otool -L a.out
a.out:
        @rpath/libcublas.5.5.dylib (...)
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell38">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p><strong>Windows</strong></p>
<p>The binary compatibility version of the CUDA libraries on Windows is indicated as part of the filename.</p>
<p>For example, a 64-bit application linked to cuBLAS 5.5 will look for <code class="docutils literal notranslate"><span class="pre">cublas64_55.dll</span></code> at runtime, so this is the file that should be redistributed with that application, even though <code class="docutils literal notranslate"><span class="pre">cublas.lib</span></code> is the file that the application is linked against. For 32-bit applications, the file would be <code class="docutils literal notranslate"><span class="pre">cublas32_55.dll</span></code>.</p>
<p>To verify the exact DLL filename that the application expects to find at runtime, use the <code class="docutils literal notranslate"><span class="pre">dumpbin</span></code> tool from the Visual Studio command prompt:</p>
<div class="highlight-text notranslate">
<div class="highlight"><pre id="codecell39"><span></span>$ dumpbin /IMPORTS a.exe
Microsoft (R) COFF/PE Dumper Version 10.00.40219.01
Copyright (C) Microsoft Corporation.  All rights reserved.


Dump of file a.exe

File Type: EXECUTABLE IMAGE

  Section contains the following imports:

    ...
    cublas64_55.dll
    ...
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell39">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
</section>
<section id="where-to-install-redistributed-cuda-libraries">
<span id="redistribution-where-to-install"></span><h4>
<span class="section-number">16.4.1.2. </span>Where to Install Redistributed CUDA Libraries<a class="headerlink" href="#where-to-install-redistributed-cuda-libraries" title="Permalink to this headline"></a>
</h4>
<p>Once the correct library files are identified for redistribution, 
they must be configured for installation into a location where the 
application will be able to find them.</p>
<p>On Windows, if the CUDA Runtime or other dynamically-linked CUDA 
Toolkit library is placed in the same directory as the executable, 
Windows will locate it automatically. On Linux and Mac, the <code class="docutils literal notranslate"><span class="pre">-rpath</span></code>
 linker option should be used to instruct the executable to search its 
local path for these libraries before searching the system paths:</p>
<p><strong>Linux/Mac</strong></p>
<div class="highlight-text notranslate">
<div class="highlight"><pre id="codecell40"><span></span>nvcc -I $(CUDA_HOME)/include
  -Xlinker "-rpath '$ORIGIN'" --cudart=shared
  -o myprogram myprogram.cu
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell40">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p><strong>Windows</strong></p>
<div class="highlight-text notranslate">
<div class="highlight"><pre id="codecell41"><span></span>nvcc.exe -ccbin "C:\vs2008\VC\bin"
  -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT" --cudart=shared
  -o "Release\myprogram.exe" "myprogram.cu"
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell41">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It may be necessary to adjust the value of <code class="docutils literal notranslate"><span class="pre">-ccbin</span></code> to reflect the location of your Visual Studio installation.</p>
</div>
<p>To specify an alternate path where the libraries will be distributed, use linker options similar to those below:</p>
<p><strong>Linux/Mac</strong></p>
<div class="highlight-text notranslate">
<div class="highlight"><pre id="codecell42"><span></span>nvcc -I $(CUDA_HOME)/include
  -Xlinker "-rpath '$ORIGIN/lib'" --cudart=shared
  -o myprogram myprogram.cu
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell42">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p><strong>Windows</strong></p>
<div class="highlight-text notranslate">
<div class="highlight"><pre id="codecell43"><span></span>nvcc.exe -ccbin "C:\vs2008\VC\bin"
  -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT /DELAY" --cudart=shared
  -o "Release\myprogram.exe" "myprogram.cu"
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell43">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>For Linux and Mac, the <code class="docutils literal notranslate"><span class="pre">-rpath</span></code> option is used as before. For Windows, the <code class="docutils literal notranslate"><span class="pre">/DELAY</span></code> option is used; this requires that the application call <code class="docutils literal notranslate"><span class="pre">SetDllDirectory()</span></code> before the first call to any CUDA API function in order to specify the directory containing the CUDA DLLs.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For Windows 8, <code class="docutils literal notranslate"><span class="pre">SetDefaultDLLDirectories()</span></code> and <code class="docutils literal notranslate"><span class="pre">AddDllDirectory()</span></code> should be used instead of <code class="docutils literal notranslate"><span class="pre">SetDllDirectory()</span></code>. Please see the MSDN documentation for these routines for more information.</p>
</div>
</section>
</section>
</section>
</section>
<section id="deployment-infrastructure-tools">
<span id="id77"></span><h1>
<span class="section-number">17. </span>Deployment Infrastructure Tools<a class="headerlink" href="#deployment-infrastructure-tools" title="Permalink to this headline"></a>
</h1>
<section id="nvidia-smi">
<span id="id78"></span><h2>
<span class="section-number">17.1. </span>Nvidia-SMI<a class="headerlink" href="#nvidia-smi" title="Permalink to this headline"></a>
</h2>
<p>The NVIDIA System Management Interface (<code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code>)
 is a command line utility that aids in the management and monitoring of
 NVIDIA GPU devices. This utility allows administrators to query GPU 
device state and, with the appropriate privileges, permits 
administrators to modify GPU device state. <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> is targeted at Tesla and certain Quadro GPUs, though limited support is also available on other NVIDIA GPUs. <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> ships with NVIDIA GPU display drivers on Linux, and with 64-bit Windows Server 2008 R2 and Windows 7. <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code>
 can output queried information as XML or as human-readable plain text 
either to standard output or to a file. See the nvidia-smi documenation 
for details. Please note that new versions of nvidia-smi are not 
guaranteed to be backward-compatible with previous versions.</p>
<section id="queryable-state">
<span id="id79"></span><h3>
<span class="section-number">17.1.1. </span>Queryable state<a class="headerlink" href="#queryable-state" title="Permalink to this headline"></a>
</h3>
<dl class="simple">
<dt>ECC error counts</dt>
<dd>
<p>Both correctable single-bit and detectable double-bit errors are 
reported. Error counts are provided for both the current boot cycle and 
the lifetime of the GPU.</p>
</dd>
<dt>GPU utilization</dt>
<dd>
<p>Current utilization rates are reported for both the compute resources of the GPU and the memory interface.</p>
</dd>
<dt>Active compute process</dt>
<dd>
<p>The list of active processes running on the GPU is reported, along 
with the corresponding process name/ID and allocated GPU memory.</p>
</dd>
<dt>Clocks and performance state</dt>
<dd>
<p>Max and current clock rates are reported for several important clock domains, as well as the current GPU performance state (<em>pstate</em>).</p>
</dd>
<dt>Temperature and fan speed</dt>
<dd>
<p>The current GPU core temperature is reported, along with fan speeds for products with active cooling.</p>
</dd>
<dt>Power management</dt>
<dd>
<p>The current board power draw and power limits are reported for products that report these measurements.</p>
</dd>
<dt>Identification</dt>
<dd>
<p>Various dynamic and static information is reported, including board 
serial numbers, PCI device IDs, VBIOS/Inforom version numbers and 
product names.</p>
</dd>
</dl>
</section>
<section id="modifiable-state">
<span id="id80"></span><h3>
<span class="section-number">17.1.2. </span>Modifiable state<a class="headerlink" href="#modifiable-state" title="Permalink to this headline"></a>
</h3>
<dl class="simple">
<dt>ECC mode</dt>
<dd>
<p>Enable and disable ECC reporting.</p>
</dd>
<dt>ECC reset</dt>
<dd>
<p>Clear single-bit and double-bit ECC error counts.</p>
</dd>
<dt>Compute mode</dt>
<dd>
<p>Indicate whether compute processes can run on the GPU and whether 
they run exclusively or concurrently with other compute processes.</p>
</dd>
<dt>Persistence mode</dt>
<dd>
<p>Indicate whether the NVIDIA driver stays loaded when no applications 
are connected to the GPU. It is best to enable this option in most 
circumstances.</p>
</dd>
<dt>GPU reset</dt>
<dd>
<p>Reinitialize the GPU hardware and software state via a secondary bus reset.</p>
</dd>
</dl>
</section>
</section>
<section id="nvml">
<span id="id81"></span><h2>
<span class="section-number">17.2. </span>NVML<a class="headerlink" href="#nvml" title="Permalink to this headline"></a>
</h2>
<p>The NVIDIA Management Library (NVML) is a C-based interface that provides direct access to the queries and commands exposed via <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code>
 intended as a platform for building 3rd-party system management 
applications. The NVML API is shipped with the CUDA Toolkit (since 
version 8.0) and is also available standalone on the NVIDIA developer 
website as part of the GPU Deployment Kit through a single header file 
accompanied by PDF documentation, stub libraries, and sample 
applications; see <a class="reference external" href="https://developer.nvidia.com/gpu-deployment-kit">https://developer.nvidia.com/gpu-deployment-kit</a>. Each new version of NVML is backward-compatible.</p>
<p>An additional set of Perl and Python bindings are provided for the 
NVML API. These bindings expose the same features as the C-based 
interface and also provide backwards compatibility. The Perl bindings 
are provided via CPAN and the Python bindings via PyPI.</p>
<p>All of these products (<code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code>, NVML, and the NVML language bindings) are updated with each new CUDA release and provide roughly the same functionality.</p>
<p>See <a class="reference external" href="https://developer.nvidia.com/nvidia-management-library-nvml">https://developer.nvidia.com/nvidia-management-library-nvml</a> for additional information.</p>
</section>
<section id="cluster-management-tools">
<span id="id82"></span><h2>
<span class="section-number">17.3. </span>Cluster Management Tools<a class="headerlink" href="#cluster-management-tools" title="Permalink to this headline"></a>
</h2>
<p>Managing your GPU cluster will help achieve maximum GPU utilization 
and help you and your users extract the best possible performance. Many 
of the industry’s most popular cluster management tools support CUDA 
GPUs via NVML. For a listing of some of these tools, see <a class="reference external" href="https://developer.nvidia.com/cluster-management">https://developer.nvidia.com/cluster-management</a>.</p>
</section>
<section id="compiler-jit-cache-management-tools">
<span id="compiler-jit-cache-management"></span><h2>
<span class="section-number">17.4. </span>Compiler JIT Cache Management Tools<a class="headerlink" href="#compiler-jit-cache-management-tools" title="Permalink to this headline"></a>
</h2>
<p>Any PTX device code loaded by an application at runtime is compiled further to binary code by the device driver. This is called <em>just-in-time compilation</em> (<em>JIT</em>).
 Just-in-time compilation increases application load time but allows 
applications to benefit from latest compiler improvements. It is also 
the only way for applications to run on devices that did not exist at 
the time the application was compiled.</p>
<p>When JIT compilation of PTX device code is used, the NVIDIA driver 
caches the resulting binary code on disk. Some aspects of this behavior 
such as cache location and maximum cache size can be controlled via the 
use of environment variables; see Just in Time Compilation of the CUDA 
C++ Programming Guide.</p>
</section>
<section id="cuda-visible-devices">
<span id="id83"></span><h2>
<span class="section-number">17.5. </span>CUDA_VISIBLE_DEVICES<a class="headerlink" href="#cuda-visible-devices" title="Permalink to this headline"></a>
</h2>
<p>It is possible to rearrange the collection of installed CUDA devices 
that will be visible to and enumerated by a CUDA application prior to 
the start of that application by way of the <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> environment variable.</p>
<p>Devices to be made visible to the application should be included as a
 comma-separated list in terms of the system-wide list of enumerable 
devices. For example, to use only devices 0 and 2 from the system-wide 
list of devices, set <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES=0,2</span></code> before launching the application. The application will then enumerate these devices as device 0 and device 1, respectively.</p>
</section>
</section>
<section id="id84">
<h1>
<span class="section-number">18. </span>Recommendations and Best Practices<a class="headerlink" href="#id84" title="Permalink to this headline"></a>
</h1>
<p>This chapter contains a summary of the recommendations for optimization that are explained in this document.</p>
<section id="overall-performance-optimization-strategies">
<h2>
<span class="section-number">18.1. </span>Overall Performance Optimization Strategies<a class="headerlink" href="#overall-performance-optimization-strategies" title="Permalink to this headline"></a>
</h2>
<p>Performance optimization revolves around three basic strategies:</p>
<ul class="simple">
<li><p>Maximizing parallel execution</p></li>
<li><p>Optimizing memory usage to achieve maximum memory bandwidth</p></li>
<li><p>Optimizing instruction usage to achieve maximum instruction throughput</p></li>
</ul>
<p>Maximizing parallel execution starts with structuring the algorithm 
in a way that exposes as much parallelism as possible. Once the 
parallelism of the algorithm has been exposed, it needs to be mapped to 
the hardware as efficiently as possible. This is done by carefully 
choosing the execution configuration of each kernel launch. The 
application should also maximize parallel execution at a higher level by
 explicitly exposing concurrent execution on the device through streams,
 as well as maximizing concurrent execution between the host and the 
device.</p>
<p>Optimizing memory usage starts with minimizing data transfers between
 the host and the device because those transfers have much lower 
bandwidth than internal device data transfers. Kernel access to global 
memory also should be minimized by maximizing the use of shared memory 
on the device. Sometimes, the best optimization might even be to avoid 
any data transfer in the first place by simply recomputing the data 
whenever it is needed.</p>
<p>The effective bandwidth can vary by an order of magnitude depending 
on the access pattern for each type of memory. The next step in 
optimizing memory usage is therefore to organize memory accesses 
according to the optimal memory access patterns. This optimization is 
especially important for global memory accesses, because latency of 
access costs hundreds of clock cycles. Shared memory accesses, in 
counterpoint, are usually worth optimizing only when there exists a high
 degree of bank conflicts.</p>
<p>As for optimizing instruction usage, the use of arithmetic 
instructions that have low throughput should be avoided. This suggests 
trading precision for speed when it does not affect the end result, such
 as using intrinsics instead of regular functions or single precision 
instead of double precision. Finally, particular attention must be paid 
to control flow instructions due to the SIMT (single instruction 
multiple thread) nature of the device.</p>
</section>
</section>
<section id="nvcc-compiler-switches">
<h1>
<span class="section-number">19. </span>nvcc Compiler Switches<a class="headerlink" href="#nvcc-compiler-switches" title="Permalink to this headline"></a>
</h1>
<section id="nvcc">
<h2>
<span class="section-number">19.1. </span>nvcc<a class="headerlink" href="#nvcc" title="Permalink to this headline"></a>
</h2>
<p>The NVIDIA <code class="docutils literal notranslate"><span class="pre">nvcc</span></code> compiler driver converts <code class="docutils literal notranslate"><span class="pre">.cu</span></code>
 files into C++ for the host system and CUDA assembly or binary 
instructions for the device. It supports a number of command-line 
parameters, of which the following are especially useful for 
optimization and related best practices:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-maxrregcount=N</span></code> specifies the maximum number of registers kernels can use at a per-file level. See <a class="reference internal" href="#register-pressure"><span class="std std-ref">Register Pressure</span></a>. (See also the<code class="docutils literal notranslate"><span class="pre">__launch_bounds__</span></code>
 qualifier discussed in Execution Configuration of the CUDA C++ 
Programming Guide to control the number of registers used on a 
per-kernel basis.)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--ptxas-options=-v</span></code> or <code class="docutils literal notranslate"><span class="pre">-Xptxas=-v</span></code> lists per-kernel register, shared, and constant memory usage.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-ftz=true</span></code> (denormalized numbers are flushed to zero)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-prec-div=false</span></code> (less precise division)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-prec-sqrt=false</span></code> (less precise square root)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-use_fast_math</span></code> compiler option of <code class="docutils literal notranslate"><span class="pre">nvcc</span></code> coerces every <code class="docutils literal notranslate"><span class="pre">functionName()</span></code> call to the equivalent <code class="docutils literal notranslate"><span class="pre">__functionName()</span></code> call. This makes the code run faster at the cost of diminished precision and accuracy. See <a class="reference internal" href="#math-libraries"><span class="std std-ref">Math Libraries</span></a>.</p></li>
</ul>
</section>
</section>
<section id="notices">
<h1>
<span class="section-number">20. </span>Notices<a class="headerlink" href="#notices" title="Permalink to this headline"></a>
</h1>
<section id="notice">
<h2>
<span class="section-number">20.1. </span>Notice<a class="headerlink" href="#notice" title="Permalink to this headline"></a>
</h2>
<p>This document is provided for information purposes only and shall not
 be regarded as a warranty of a certain functionality, condition, or 
quality of a product. NVIDIA Corporation (“NVIDIA”) makes no 
representations or warranties, expressed or implied, as to the accuracy 
or completeness of the information contained in this document and 
assumes no responsibility for any errors contained herein. NVIDIA shall 
have no liability for the consequences or use of such information or for
 any infringement of patents or other rights of third parties that may 
result from its use. This document is not a commitment to develop, 
release, or deliver any Material (defined below), code, or 
functionality.</p>
<p>NVIDIA reserves the right to make corrections, modifications, 
enhancements, improvements, and any other changes to this document, at 
any time without notice.</p>
<p>Customer should obtain the latest relevant information before placing
 orders and should verify that such information is current and complete.</p>
<p>NVIDIA products are sold subject to the NVIDIA standard terms and 
conditions of sale supplied at the time of order acknowledgement, unless
 otherwise agreed in an individual sales agreement signed by authorized 
representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby 
expressly objects to applying any customer general terms and conditions 
with regards to the purchase of the NVIDIA product referenced in this 
document. No contractual obligations are formed either directly or 
indirectly by this document.</p>
<p>NVIDIA products are not designed, authorized, or warranted to be 
suitable for use in medical, military, aircraft, space, or life support 
equipment, nor in applications where failure or malfunction of the 
NVIDIA product can reasonably be expected to result in personal injury, 
death, or property or environmental damage. NVIDIA accepts no liability 
for inclusion and/or use of NVIDIA products in such equipment or 
applications and therefore such inclusion and/or use is at customer’s 
own risk.</p>
<p>NVIDIA makes no representation or warranty that products based on 
this document will be suitable for any specified use. Testing of all 
parameters of each product is not necessarily performed by NVIDIA. It is
 customer’s sole responsibility to evaluate and determine the 
applicability of any information contained in this document, ensure the 
product is suitable and fit for the application planned by customer, and
 perform the necessary testing for the application in order to avoid a 
default of the application or the product. Weaknesses in customer’s 
product designs may affect the quality and reliability of the NVIDIA 
product and may result in additional or different conditions and/or 
requirements beyond those contained in this document. NVIDIA accepts no 
liability related to any default, damage, costs, or problem which may be
 based on or attributable to: (i) the use of the NVIDIA product in any 
manner that is contrary to this document or (ii) customer product 
designs.</p>
<p>No license, either expressed or implied, is granted under any NVIDIA 
patent right, copyright, or other NVIDIA intellectual property right 
under this document. Information published by NVIDIA regarding 
third-party products or services does not constitute a license from 
NVIDIA to use such products or services or a warranty or endorsement 
thereof. Use of such information may require a license from a third 
party under the patents or other intellectual property rights of the 
third party, or a license from NVIDIA under the patents or other 
intellectual property rights of NVIDIA.</p>
<p>Reproduction of information in this document is permissible only if 
approved in advance by NVIDIA in writing, reproduced without alteration 
and in full compliance with all applicable export laws and regulations, 
and accompanied by all associated conditions, limitations, and notices.</p>
<p>THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS,
 FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND 
SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO 
WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO 
THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF 
NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. 
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE 
FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, 
SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED 
AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF 
THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF 
SUCH DAMAGES. Notwithstanding any damages that customer might incur for 
any reason whatsoever, NVIDIA’s aggregate and cumulative liability 
towards customer for the products described herein shall be limited in 
accordance with the Terms of Sale for the product.</p>
</section>
<section id="opencl">
<h2>
<span class="section-number">20.2. </span>OpenCL<a class="headerlink" href="#opencl" title="Permalink to this headline"></a>
</h2>
<p>OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.</p>
</section>
<section id="trademarks">
<h2>
<span class="section-number">20.3. </span>Trademarks<a class="headerlink" href="#trademarks" title="Permalink to this headline"></a>
</h2>
<p>NVIDIA and the NVIDIA logo are trademarks or registered trademarks of
 NVIDIA Corporation in the U.S. and other countries. Other company and 
product names may be trademarks of the respective companies with which 
they are associated.</p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr>

  <div role="contentinfo">
<img src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/NVIDIA-LogoBlack.svg" class="only-light">
<img src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/NVIDIA-LogoWhite.svg" class="only-dark">

<p class="notices">
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" target="_blank">Privacy Policy</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/" target="_blank">Manage My Privacy</a>
|
<a href="https://www.nvidia.com/en-us/preferences/start/" target="_blank">Do Not Sell or Share My Data</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" target="_blank">Terms of Service</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" target="_blank">Accessibility</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" target="_blank">Corporate Policies</a>
|
<a href="https://www.nvidia.com/en-us/product-security/" target="_blank">Product Security</a>
|
<a href="https://www.nvidia.com/en-us/contact/" target="_blank">Contact</a>
</p>

<p>
  Copyright © 2007-2025, NVIDIA Corporation &amp; affiliates. All rights reserved.
</p>

    <p>
      <span class="lastupdated">Last updated on May 31, 2025.
      </span></p>

  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(false);
      });
  </script>
 
<script type="text/javascript">if (typeof _satellite !== "undefined"){_satellite.pageBottom();}</script><script>_satellite["_runScript4"](function(event, target, Promise) {
//if(typeof s!='undefined'){
 try{
  
  window.ClickDownTrack=function(obj,eventVal,prefix){  
    try{
      if(typeof s!='undefined'){
    secName = prefix+":"+s.pageName;
    secName=secName.toLowerCase();
    s.linkTrackVars='events';
    if(prefix!=='download'){s.eVar4=prefix.toLowerCase();s.linkTrackVars='events,eVar4';}
    s.events=eventVal;s.linkTrackEvents=eventVal;
    if(prefix!=='download'){s.tl(obj,'d',prefix);}else{s.tl(obj,'d',secName);}
	  s.linkTrackVars="None"
	  s.linkTrackEvents="None" }
    }catch(e){console.log("Bt downTrack"+e)}
}
  }catch(e){console.log("Bt downTrack"+e)}
try{
setTimeout(function(){
if(typeof jQuery!='undefined'){
jQuery(".col-md-8,col-md-4").on("click","a[href*='driveworks/files/driveinstall-latest']",function(){ClickDownTrack(this,'event4','download');
});
jQuery(".col-md-4").on("click","a[href*='driveworks/files/driveinstall-latest']",function(){ClickDownTrack(this,'event4','download');
});
jQuery("#panelTargetInstaller").on("click","button.cudaDownloadButton", function() {
var getTitle=jQuery(this).parent('a').attr('title');ClickDownTrack(this,'event4',getTitle);
});

jQuery('#agree').on('click', function() {
   var getIframeVal=jQuery(this).parents('#agreement').children('iframe').attr('src').substring(jQuery('#agree').parents('#agreement').children('iframe').attr('src').lastIndexOf('/')+1).replace('.html','');
     ClickDownTrack(this,'event10,event4',getIframeVal)})
if(document.URL.indexOf('embedded/downloads')<0){
jQuery("a[href*=jetpack-l4t]").click(function(){ClickDownTrack(this,'event10','download')});  
}
if(document.URL.indexOf('gameworksdownload')>-1){
jQuery("a[href*=aftermath-12-windows],a[href*=vrworks-25-sdk]").click(function(){ClickDownTrack(this,'event10','download')}); 
}
jQuery('.downloads ul li a.ng-scope.ng-binding').click(function(){
if(jQuery(this).attr('href')!=='#'){
  if(jQuery(this).attr('href').indexOf('index.html')<0){
    if(jQuery(this).attr('href').indexOf('asset')>-1||jQuery(this).attr('href').indexOf('-sdk')>-1){
      var getVal='download';
      var lnk=jQuery(this).attr('href');
      var parts= lnk.split("/");
      if(parts[parts.length-1]==""){
      	getVal=parts[parts.length-2];
      }else{
      getVal=parts[parts.length-1];
      }
      
      ClickDownTrack(this,'event10',getVal)
    }
  }
}
})
  
if(document.URL.indexOf('/vrworks/vrworks-360video/download')>-1){
jQuery('.field .field-items .btn').on('click',function(){var getVal=jQuery(this).attr('href').replace('https://developer.nvidia.com/','').replace(/\//g,'-');ClickDownTrack(this,'event10',getVal)})
}
if(document.URL.indexOf('rdp/cudnn-download')>-1){
jQuery('.panel-body p a').on('click',function(){ClickDownTrack(this,'event10','download')})
}
if(document.URL.indexOf('nvidia-grid-software-management-sdk-downloads')>-1){
  jQuery('#block-system-main a[href*=NVIDIA-GRID-Software-Management-SDK],#block-system-main a[href*=gridsdk-userguide]').click(function(){ClickDownTrack(this,'event10','download')})
}
if(document.URL.indexOf('nvwmi-sdk')>-1){
jQuery('.panel-success a[href*=standalone],.panel-success a[href*=sdk],a[href*=api-reference]').click(function(){ClickDownTrack(this,'event10','download')})
}
if(document.URL.indexOf('user')>-1){
jQuery('#console .container .downloadable-file-link').click(function(){ClickDownTrack(this,'event10','download')})
}
if(document.URL.indexOf('rdp/digits-download')>-1){
  jQuery('a[href*=nv-deep-learning-repo],a[href*=digits-2-ubuntu-1404-prod]').click(function(){ClickDownTrack(this,'event10','download')})
}
if(document.URL.indexOf('video-codec-sdk-archive')>-1){
  jQuery('.col-md-8 a[href*=video-sdk-601],.col-md-8 a[href*=video_codec_sdk]').click(function(){ClickDownTrack(this,'event10','download')})
} 
if(document.URL.indexOf('ffmpeg')>-1){
  jQuery('a[href*=Using_FFmpeg_with_NVIDIA_GPU_Hardware_Acceleration-pdf]').click(function(){ClickDownTrack(this,'event10','download')})
}
if(document.URL.indexOf('designworks/video_codec_sdk/downloads/v8.0')>-1){
jQuery('a[href*=accept_eula]').click(function(){ClickDownTrack(this,'event10','download')})
}
if(document.URL.indexOf('embedded/downloads')>-1){
if(_satellite.getVar('authStage')=='logged-in'){jQuery('.downloads ul li a.ng-scope.ng-binding').click(function(){
if(jQuery(this).attr('href')!=='#'){
  if(jQuery(this).attr('href').indexOf('index.html')<0){
    if(jQuery(this).attr('href').indexOf('embedded')>-1){
      ClickDownTrack(this,'event10','download')
    }
  }
}
}) }else{jQuery('.downloads li a').click(function(){var $this=jQuery(this); var getText=$this.text();ClickOmniTracki(this,'event10','button:section:',getText);})}
}



}},2000);
//}
}catch(e){console.log("Bt downTrack"+e)}
//}
});</script><script>_satellite["_runScript5"](function(event, target, Promise) {
if(typeof jQuery!=='undefined'){
jQuery('#navbar-collapse a').click(function() {
	var $this = jQuery(this);
  var getVal = $this.text();

  if(getVal=='Join'){_satellite.cookie.remove('_dtfmch');}if(!jQuery(this).hasClass("dropdown-toggle")){
     if(getVal=='Log out'){
    ClickOmniTrackNavi(this, 'event10,event11,event13', 'link:nav:header:', getVal);
   }else{ClickOmniTrackNavi(this, 'event10,event11', 'link:nav:header:', getVal);}  
  }

});

  
  
  
jQuery('.footer-boilerplate a').click(function() {
	var $this = jQuery(this);
	var getVal = $this.text();
	ClickOmniTracki(this, 'event10', 'link:footer:', getVal);
});}
});</script><script>_satellite["_runScript6"](function(event, target, Promise) {

//if(typeof s!='undefined'){
   
  window.ClickOmniTracki=function(obj,eventVal,prefix,secName){  
  try
   {
    if(typeof s!='undefined'){
      secName = prefix+secName+":"+s.pageName ;
     secName=secName.toLowerCase();
      s.events=eventVal;s.linkTrackEvents=eventVal;
      s.eVar10=s.prop10=secName;
      s.linkTrackVars='events,eVar10,prop10';
      s.tl(obj,'o',secName); 
	    s.linkTrackVars="None"
	    s.linkTrackEvents="None"}
	 }catch(error){console.log("Bt DownTrack"+error)}}
window.ClickOmniTrackNavi=function(obj,eventVal,prefix,secName){  
  try
   {  
      if(typeof s!='undefined'){
     var secN=secName.lastIndexOf('/')+1;
      secName=secName.substring(secN);
      secName=secName.toLowerCase();
      secName=secName.replace(/\//g,':').replace('#','');
      s.eVar11=s.prop11=secName;
      secName = prefix+secName+":"+s.pageName;
      s.events=eventVal;s.linkTrackEvents=eventVal;
      s.eVar10=s.prop10=secName; 
      s.linkTrackVars='events,eVar10,prop10,eVar11';
      s.tl(obj,'o',secName); 
	    s.linkTrackVars="None"
	    s.linkTrackEvents="None" }
	 }catch(error){console.log("Bt DownTrack"+error)}}


window.ClickOmniFilter=function(obj,eventVal,prefix,secName,filter){  
  try
   {  
      if(typeof s!='undefined'){
     var secN=secName.lastIndexOf('/')+1;
      secName=secName.substring(secN);
      secName=secName.toLowerCase();
      secName=secName.replace(/\//g,':').replace('#','');
     s.eVar35=filter.toLowerCase().replace(/[".]/g,"").replace(/, /g,"|").trim().replace(/: /g,":");
      secName = prefix+secName+":"+s.pageName;
      s.events=eventVal;s.linkTrackEvents=eventVal;
      s.eVar10=s.prop10=secName; 
      s.linkTrackVars='events,eVar10,prop10,eVar35';
      s.tl(obj,'o',secName); 
	    s.linkTrackVars="None"
	    s.linkTrackEvents="None"}
	 }catch(error){console.log("Bt DownTrack"+error)}}

//}

window.ClickOmniTrackRec=function(obj,eventVal,prefix,secName,recType){  
  try
   {
    if(typeof s!='undefined'){
      s.list3=secName;
      secName = prefix+secName+":"+s.pageName ;
      secName=secName.toLowerCase();
      s.events=eventVal;s.linkTrackEvents=eventVal;
      s.eVar10=s.prop10=secName;
	  s.eVar130=recType;
	  s.linkTrackVars='events,eVar10,prop10,eVar130,list3';
      s.tl(obj,'o',secName); 
	    s.linkTrackVars="None"
	    s.linkTrackEvents="None"}
	 }catch(error){console.log("Bt DownTrack"+error)}}

window.ClickOmniSearch=function(obj,eventVal,prefix,secName){  
  try
   { 
   if(typeof s!='undefined'){
   
       	var srchKeyword=secName;
      secName = prefix+secName+":"+s.pageName ;
     secName=secName.toLowerCase();
      s.events=eventVal;s.linkTrackEvents=eventVal;
      s.eVar10=s.prop10=secName;
    	s.eVar20=s.prop20=srchKeyword;
      s.linkTrackVars='events,eVar10,prop10,eVar20,prop20';
    	
      s.tl(obj,'o',secName); 
	    s.linkTrackVars="None"
	    s.linkTrackEvents="None"}
	 }catch(error){console.log("Bt DownTrack"+error)}}
});</script>
 




<script>_satellite["_runScript7"](function(event, target, Promise) {
var cjs=document.createElement("script");cjs.type="text/javascript",cjs.src="https://images.nvidia.com/content/websidestory/aa/cdtm.js",document.getElementsByTagName("head")[0].appendChild(cjs);
});</script><script>_satellite["_runScript8"](function(event, target, Promise) {
!function(){window._uxa=window._uxa||[];try{"undefined"!=typeof s&&(void 0!==s.eVar1&&window._uxa.push(["setCustomVariable",1,"Page_Name",_satellite.getVar("pageName"),3]),void 0!==s.eVar9&&window._uxa.push(["setCustomVariable",2,"Country_Language_Code",s.eVar9,3]),void 0!==s.eVar13&&window._uxa.push(["setCustomVariable",3,"Nvid",s.eVar13,3]),void 0!==s.eVar19&&window._uxa.push(["setCustomVariable",4,"New_vs_Returned",s.eVar19,3]),void 0!==s.eVar20&&window._uxa.push(["setCustomVariable",5,"Search_Term",s.eVar20,3]),void 0!==s.eVar32&&window._uxa.push(["setCustomVariable",6,"Company_Name",s.eVar32,3]),void 0!==s.eVar33&&window._uxa.push(["setCustomVariable",7,"Industry_Name",s.eVar33,3]),void 0!==s.eVar41&&window._uxa.push(["setCustomVariable",8,"Ncid",s.eVar41,3]),void 0!==s.referrer&&window._uxa.push(["setCustomVariable",9,"Referrer",s.referrer,3]))}catch(e){}if("undefined"==typeof CS_CONF){window._uxa.push(["setPath",window.location.pathname+window.location.hash.replace("#","?__")]);var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//t.contentsquare.net/uxa/c6af8848c2687.js",document.getElementsByTagName("head")[0].appendChild(e)}else window._uxa.push(["trackPageview",window.location.pathname+window.location.hash.replace("#","?__")])}();
});</script><script>_satellite["_runScript9"](function(event, target, Promise) {
document.addEventListener("DOMContentLoaded",(function(){const e=`https://surveys.hotjar.com/4904bf71-6484-47a7-83ff-4715cceabdb5?utm_source=${window.location.href}`,t=document.getElementById("openPopupButton");t&&(t.setAttribute("href",e),t.addEventListener("click",(function(t){t.preventDefault();const n=780,o=780,c=t.screenX-n/2,s=t.screenY+10;window.open(e,"popupWindow",`width=${n},height=${o},left=${c},top=${s},scrollbars=yes,resizable=no,status=no,location=no,menubar=no,toolbar=no`)})))}));
});</script><script>_satellite["_runScript10"](function(event, target, Promise) {
!function(t,h,e,j,s,n){t.hj=t.hj||function(){(t.hj.q=t.hj.q||[]).push(arguments)},t._hjSettings={hjid:3655182,hjsv:6},s=h.getElementsByTagName("head")[0],(n=h.createElement("script")).async=1,n.src=e+t._hjSettings.hjid+j+t._hjSettings.hjsv,s.appendChild(n)}(window,document,"https://static.hotjar.com/c/hotjar-",".js?sv=");
});</script><script>
if("undefined"!=typeof jQuery||"undefined"!=typeof $){var runOnce=!0;jQuery(document).on("click","input#FirstName",(function(){if(runOnce){var e=jQuery(this).parents("form").attr("id"),t="form:"+e+":start:"+_satellite.getVar("pageName");e&&(s.linkTrackVars="eVar10,eVar29,prop29,events",s.linkTrackEvents="event10,event29",s.events="event10,event29",s.eVar29=e+":"+_satellite.getVar("pageName"),s.prop29=s.eVar29,s.eVar10=t,s.tl(this,"o",t)),runOnce=!1}})),window.MktoForms2&&MktoForms2.whenReady((function(e){var t=_satellite.cookie.get("sc_cmp"),n=document.querySelector('form input[name*="ncid"]');void 0!==t&&""!=t?n.setAttribute("value",t):document.location.href.indexOf("ncid=")<0&&n.setAttribute("value","no-ncid"),e.onSuccess((function(e){var t="mktoform_"+e.formid;ClickOmniTracki(!0,"event10,event30","form:",t+":success:")}))}))}
</script><script>
function gtag(){dataLayer.push(arguments)}var script=document.createElement("script");script.type="text/javascript",script.src="https://www.googletagmanager.com/gtag/js?id=AW-1041695361",document.getElementsByTagName("head")[0].appendChild(script),window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","AW-1041695361");
</script><script>
gtag("event","page_view",{send_to:"AW-1041695361",user_id:"replace with value"});
</script><iframe height="0" width="0" style="display: none; visibility: hidden;"></iframe><img style="display: none;" alt="" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/ipv.gif"><cs-native-frame-holder hidden=""></cs-native-frame-holder><iframe id="_hjSafeContext_16325316" title="_hjSafeContext" tabindex="-1" aria-hidden="true" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/a_003.html" style="display: none !important; width: 1px !important; height: 1px !important; opacity: 0 !important; pointer-events: none !important;"></iframe><iframe id="_hjSafeContext_9042905" title="_hjSafeContext" tabindex="-1" aria-hidden="true" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/a_003.html" style="display: none !important; width: 1px !important; height: 1px !important; opacity: 0 !important; pointer-events: none !important;"></iframe><div class="_hj-widget-container _hj-widget-theme-light" id="survey_1021547"><div class="_hj-s3UIi__styles__globalStyles _hj-Pbej5__styles__resetStyles "><div lang="en" id="hotjar-survey-f7aed7e8-40d2-4349-b275-398385394f04" dir="ltr" style="--hjFeedbackAccentColor: #cccccc; --hjFeedbackAccentHoverColor: #b3b3b3; --hjFeedbackAccentActiveColor: #a6a6a6; --hjFeedbackAccentTextColor: #000; --hjFeedbackDisabledAccentColor: #F1F2F6; --hjFeedbackDisabledAccentTextColor: rgba(0, 0, 0, 0.43); --hjFeedbackBorderColor: #E4E6EB; --hjFeedbackOptionButtonBackgroundColor: #E0E2E8; --hjFeedbackInputPlaceholderColor: #707070; font-size: clamp(16px, 1rem, 32px) !important; --hjFeedbackPageTextColor: rgba(255, 255, 255, 0.94);"><div data-testid="survey-root"><div class="css-sfbnep"><div aria-hidden="true" class="css-u2agci"></div><div class="css-112xj8j"><button aria-label="Feedback - Show survey" id="hj-survey-toggle-1" class="css-sffkpe"><div class="css-1xi4v5y"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="22" fill="none" viewBox="0 0 24 22" data-testid="icon-buddy"><path fill="#020202" fill-rule="evenodd" d="M22.362.827C21.72.5 20.88.5 19.2.5H3.3C1.755.5.983.5.575.819A1.5 1.5 0 000 1.969c-.01.517.453 1.135 1.38 2.371l1.14 1.52c.178.237.267.356.33.487a1.5 1.5 0 01.122.365C3 6.855 3 7.003 3 7.3v9.4c0 1.68 0 2.52.327 3.162a3 3 0 001.311 1.311c.642.327 1.482.327 3.162.327h11.4c1.68 0 2.52 0 3.162-.327a3 3 0 001.311-1.311C24 19.22 24 18.38 24 16.7V5.3c0-1.68 0-2.52-.327-3.162a3 3 0 00-1.31-1.311zM9.277 11.937a1.125 1.125 0 00-1.948 1.126 7.123 7.123 0 006.171 3.562 7.123 7.123 0 006.171-3.562 1.125 1.125 0 10-1.947-1.126 4.872 4.872 0 01-4.224 2.438 4.873 4.873 0 01-4.223-2.438zM10.5 8a1.5 1.5 0 11-3 0 1.5 1.5 0 013 0zM18 9.5a1.5 1.5 0 100-3 1.5 1.5 0 000 3z" clip-rule="evenodd"></path></svg></div><span class="css-njnxsn">Feedback</span></button><div role="dialog" aria-labelledby="hj-survey-lbl-1" aria-hidden="true"><div id="innerLayer"><div class="css-1vii6z0"><div class="css-desq4a"><button class="css-zzqb0h"><svg aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" data-testid="icon-collapse"><path d="M17.8059 9.87186L13.2207 16.2912C12.6225 17.1287 11.3778 17.1287 10.7795 16.2912L6.19431 9.87186C5.48516 8.87905 6.19485 7.5 7.41491 7.5L16.5853 7.5C17.8054 7.5 18.5151 8.87906 17.8059 9.87186Z" fill="#8f8f8f"></path></svg></button></div><form inert="" class="css-dptyls" __bizdiag="260154992" __biza="WJ__"><div class="css-mioz4j"><h2 id="hj-survey-lbl-1" aria-label="How would you rate your overall website experience? Select an option from 1 to 7, with 1 being Very dissatisfied and 7 being Very satisfied" class="css-8ecr85">How would you rate your overall website experience?</h2><div aria-labelledby="hj-survey-lbl-1" role="radiogroup" class="css-5yh89e"><div class="css-cc85er"><label class="css-eivoke"><input aria-describedby="hj-surveys-scale-option-label-0-19e4997a-e92c-4616-abb3-90b649e07f47" type="radio" aria-label="1" name="f0dc52ad-d22e-45ec-8dda-e75a4d142146" class="css-13vcunm" value="1"><span aria-hidden="true" class="css-94a8hh">1</span></label><label class="css-eivoke"><input type="radio" aria-label="2" name="f0dc52ad-d22e-45ec-8dda-e75a4d142146" class="css-13vcunm" value="2"><span aria-hidden="true" class="css-94a8hh">2</span></label><label class="css-eivoke"><input type="radio" aria-label="3" name="f0dc52ad-d22e-45ec-8dda-e75a4d142146" class="css-13vcunm" value="3"><span aria-hidden="true" class="css-94a8hh">3</span></label><label class="css-eivoke"><input type="radio" aria-label="4" name="f0dc52ad-d22e-45ec-8dda-e75a4d142146" class="css-13vcunm" value="4"><span aria-hidden="true" class="css-94a8hh">4</span></label><label class="css-eivoke"><input type="radio" aria-label="5" name="f0dc52ad-d22e-45ec-8dda-e75a4d142146" class="css-13vcunm" value="5"><span aria-hidden="true" class="css-94a8hh">5</span></label><label class="css-eivoke"><input type="radio" aria-label="6" name="f0dc52ad-d22e-45ec-8dda-e75a4d142146" class="css-13vcunm" value="6"><span aria-hidden="true" class="css-94a8hh">6</span></label><label class="css-eivoke"><input aria-describedby="hj-surveys-scale-option-label-1-b7a5c721-8c05-4a4f-ab8d-5661abe83604" type="radio" aria-label="7" name="f0dc52ad-d22e-45ec-8dda-e75a4d142146" class="css-13vcunm" value="7"><span aria-hidden="true" class="css-94a8hh">7</span></label></div><div class="css-m6ndoo"><span id="hj-surveys-scale-option-label-0-19e4997a-e92c-4616-abb3-90b649e07f47" title="Very dissatisfied" aria-label="Very dissatisfied" class="css-ksupk8">Very dissatisfied</span><span id="hj-surveys-scale-option-label-1-b7a5c721-8c05-4a4f-ab8d-5661abe83604" title="Very satisfied" aria-label="Very satisfied" class="css-ouk37a">Very satisfied</span></div></div></div><div class="css-14zyqle"><div class="css-v4tztt"><button type="button" kind="primary" disabled="disabled" aria-label="Next question" size="s" class="css-eaf3z1">Next</button></div><div class="css-12kbg3u"><img data-testid="custom-footer-logo" alt="" src="1.%20Preface%20%E2%80%94%20CUDA%20C++%20Best%20Practices%20Guide%2012.9%20documentation_files/a82491d08da94cbdb13d07b60bbcd60e.png" class="css-c733a"></div></div></form></div></div></div></div></div></div></div></div></div></body></html>